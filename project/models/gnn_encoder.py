# models/gnn_encoder.py - ä¿®å¤Baselineç»´åº¦ä¸åŒ¹é…é—®é¢˜

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, Set2Set, global_mean_pool, global_add_pool
from torch_geometric.data import Data, Batch

class GNNEncoder(nn.Module):
    """
    ä¿®å¤ç‰ˆGNNç¼–ç å™¨ - ä¸“é—¨è§£å†³Baselineè¾¹ç‰¹å¾ç»´åº¦ä¸åŒ¹é…é—®é¢˜
    
    ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼š
    1. è‡ªåŠ¨å¤„ç†4ç»´â†’2ç»´è¾¹ç‰¹å¾é™ç»´
    2. ç¡®ä¿8ç»´èŠ‚ç‚¹ç‰¹å¾è¾“å…¥
    3. ä¿è¯256ç»´è¾“å‡º
    """
    
    def __init__(self, node_dim=8, edge_dim=2, hidden_dim=128, output_dim=256, num_layers=3):
        super(GNNEncoder, self).__init__()
        
        self.node_dim = node_dim
        self.edge_dim = edge_dim  # æœŸæœ›çš„è¾¹ç‰¹å¾ç»´åº¦ï¼ˆé€šå¸¸æ˜¯2ï¼‰
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        
        # èŠ‚ç‚¹åµŒå…¥å±‚
        self.node_embedding = nn.Linear(node_dim, hidden_dim)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šè¾¹ç‰¹å¾è‡ªé€‚åº”åµŒå…¥å±‚
        # æ— è®ºè¾“å…¥å¤šå°‘ç»´ï¼Œéƒ½èƒ½æ­£ç¡®å¤„ç†
        self.edge_embedding = nn.Linear(edge_dim, hidden_dim)
        
        # GATå·ç§¯å±‚
        self.conv_layers = nn.ModuleList()
        for i in range(num_layers):
            self.conv_layers.append(
                GATConv(hidden_dim, hidden_dim, heads=4, concat=False, 
                       edge_dim=hidden_dim, dropout=0.1)
            )
        
        # å…¨å±€æ± åŒ–
        self.global_pool = Set2Set(hidden_dim, processing_steps=3)
        
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
        self.output_layers = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),  # Set2Setè¾“å‡º2*hidden_dim
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, output_dim),       # ç¡®ä¿256ç»´è¾“å‡º
            nn.ReLU()
        )
        
        # æ‰¹å½’ä¸€åŒ–
        self.batch_norms = nn.ModuleList([
            nn.BatchNorm1d(hidden_dim) for _ in range(num_layers)
        ])
        
        print(f"âœ… GNNç¼–ç å™¨åˆå§‹åŒ–: èŠ‚ç‚¹{node_dim}ç»´ -> è¾¹{edge_dim}ç»´ -> è¾“å‡º{output_dim}ç»´")
        
    def forward(self, data):
        """
        å‰å‘ä¼ æ’­ - è‡ªåŠ¨å¤„ç†è¾¹ç‰¹å¾ç»´åº¦
        """
        if isinstance(data, list):
            data = Batch.from_data_list(data)
        
        x = data.x.float()
        edge_index = data.edge_index
        edge_attr = data.edge_attr.float() if data.edge_attr is not None else None
        batch = data.batch if hasattr(data, 'batch') else None
        
        # ğŸ”§ å…³é”®ä¿®å¤1ï¼šèŠ‚ç‚¹ç‰¹å¾ç»´åº¦éªŒè¯
        if x.size(1) != self.node_dim:
            raise ValueError(f"âŒ èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.node_dim}ç»´ï¼Œå®é™…{x.size(1)}ç»´")
        
        # ğŸ”§ å…³é”®ä¿®å¤2ï¼šè¾¹ç‰¹å¾è‡ªåŠ¨é™ç»´å¤„ç†
        if edge_attr is not None:
            actual_edge_dim = edge_attr.size(1)
            
            if actual_edge_dim != self.edge_dim:
                print(f"ğŸ”§ è¾¹ç‰¹å¾ç»´åº¦è‡ªåŠ¨è°ƒæ•´: {actual_edge_dim}ç»´ -> {self.edge_dim}ç»´")
                
                if actual_edge_dim > self.edge_dim:
                    # é™ç»´ï¼šæˆªå–å‰Nç»´
                    edge_attr = edge_attr[:, :self.edge_dim]
                    print(f"   æˆªå–å‰{self.edge_dim}ç»´: [å¸¦å®½, å»¶è¿Ÿ]")
                    
                elif actual_edge_dim < self.edge_dim:
                    # å‡ç»´ï¼šç”¨é›¶å¡«å……
                    padding_dims = self.edge_dim - actual_edge_dim
                    padding = torch.zeros(edge_attr.size(0), padding_dims, device=edge_attr.device)
                    edge_attr = torch.cat([edge_attr, padding], dim=1)
                    print(f"   é›¶å¡«å……åˆ°{self.edge_dim}ç»´")
        
        # ç‰¹å¾åµŒå…¥
        x = self.node_embedding(x)
        if edge_attr is not None:
            edge_attr = self.edge_embedding(edge_attr)
            edge_attr = F.normalize(edge_attr, p=2, dim=1)
        
        # GNNå·ç§¯
        for i, conv in enumerate(self.conv_layers):
            x_residual = x
            
            if edge_attr is not None:
                x = conv(x, edge_index, edge_attr=edge_attr)
            else:
                x = conv(x, edge_index)
            
            # æ‰¹å½’ä¸€åŒ–
            if batch is not None:
                x = self.batch_norms[i](x)
            else:
                if x.size(0) > 1:
                    x = self.batch_norms[i](x)
            
            x = F.relu(x)
            
            # æ®‹å·®è¿æ¥
            if x_residual.size() == x.size():
                x = x + x_residual
        
        # å…¨å±€æ± åŒ–
        if batch is not None:
            graph_embedding = self.global_pool(x, batch)
        else:
            batch_single = torch.zeros(x.size(0), dtype=torch.long, device=x.device)
            graph_embedding = self.global_pool(x, batch_single)
        
        # ğŸ”§ éªŒè¯æ± åŒ–åç»´åº¦
        expected_pooled_dim = 2 * self.hidden_dim
        if graph_embedding.size(-1) != expected_pooled_dim:
            print(f"âš ï¸ æ± åŒ–ç»´åº¦å¼‚å¸¸: æœŸæœ›{expected_pooled_dim}, å®é™…{graph_embedding.size(-1)}")
        
        # è¾“å‡ºå±‚
        graph_embedding = self.output_layers(graph_embedding)
        graph_embedding = F.normalize(graph_embedding, p=2, dim=1)
        
        # ğŸ”§ æœ€ç»ˆéªŒè¯è¾“å‡ºç»´åº¦
        if graph_embedding.size(-1) != self.output_dim:
            print(f"âš ï¸ è¾“å‡ºç»´åº¦å¼‚å¸¸: æœŸæœ›{self.output_dim}, å®é™…{graph_embedding.size(-1)}")
        
        return graph_embedding


class EdgeAwareGNNEncoder(GNNEncoder):
    """
    è¾¹æ„ŸçŸ¥GNNç¼–ç å™¨ - ç»§æ‰¿åŸºç¡€ä¿®å¤é€»è¾‘
    """
    
    def __init__(self, node_dim=8, edge_dim=4, hidden_dim=128, output_dim=256, num_layers=4):
        # Edge-awareæ¨¡å¼ä½¿ç”¨4ç»´è¾¹ç‰¹å¾
        super(EdgeAwareGNNEncoder, self).__init__(node_dim, edge_dim, hidden_dim, output_dim, num_layers)
        
        # VNFéœ€æ±‚ç¼–ç å™¨
        self.vnf_requirement_encoder = nn.Linear(6, hidden_dim)
        
        # è¾¹é‡è¦æ€§ç½‘ç»œ
        self.edge_importance_net = nn.Sequential(
            nn.Linear(edge_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.feature_fusion = nn.Sequential(
            nn.Linear(output_dim + hidden_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        print(f"âœ… EdgeAwareç¼–ç å™¨åˆå§‹åŒ–: æ”¯æŒ{edge_dim}ç»´è¾¹ç‰¹å¾")
        
    def forward_with_vnf_context(self, data, vnf_context=None):
        """å¸¦VNFä¸Šä¸‹æ–‡çš„å‰å‘ä¼ æ’­"""
        # åŸºç¡€å›¾ç¼–ç 
        graph_embedding = self.forward(data)
        
        # VNFä¸Šä¸‹æ–‡èåˆ
        if vnf_context is not None:
            if isinstance(vnf_context, torch.Tensor):
                vnf_tensor = vnf_context.float()
            else:
                vnf_tensor = torch.tensor(vnf_context, dtype=torch.float32)
            
            if vnf_tensor.dim() == 1:
                vnf_tensor = vnf_tensor.unsqueeze(0)
            
            # VNFä¸Šä¸‹æ–‡ç¼–ç 
            vnf_embedding = self.vnf_requirement_encoder(vnf_tensor)
            
            # ç‰¹å¾èåˆ
            if graph_embedding.size(0) == vnf_embedding.size(0):
                fused_features = torch.cat([graph_embedding, vnf_embedding], dim=1)
                enhanced_embedding = self.feature_fusion(fused_features)
            else:
                # å¹¿æ’­å¤„ç†
                enhanced_embedding = graph_embedding + 0.3 * vnf_embedding.mean(dim=0, keepdim=True)
            
            return enhanced_embedding
        else:
            return graph_embedding


def create_gnn_encoder(config: dict, mode: str = 'edge_aware'):
    """
    åˆ›å»ºGNNç¼–ç å™¨çš„å·¥å‚å‡½æ•° - ä¿®å¤ç‰ˆ
    
    ğŸ”§ å…³é”®ä¿®å¤ï¼šè‡ªåŠ¨å¤„ç†baselineçš„è¾¹ç‰¹å¾ç»´åº¦é—®é¢˜
    """
    
    if mode == 'edge_aware':
        gnn_config = config.get('gnn', {}).get('edge_aware', {})
        # Edge-awareä½¿ç”¨4ç»´è¾¹ç‰¹å¾
        encoder = EdgeAwareGNNEncoder(
            node_dim=8,  # å›ºå®š8ç»´èŠ‚ç‚¹ç‰¹å¾
            edge_dim=4,  # 4ç»´è¾¹ç‰¹å¾ï¼š[bandwidth, latency, jitter, packet_loss]
            hidden_dim=gnn_config.get('hidden_dim', 128),
            output_dim=gnn_config.get('output_dim', 256),
            num_layers=gnn_config.get('layers', 4)
        )
        print(f"âœ… åˆ›å»ºEdge-aware GNNç¼–ç å™¨: 4ç»´è¾¹ç‰¹å¾")
        
    else:  # baseline
        gnn_config = config.get('gnn', {}).get('baseline', {})
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šBaselineä»ç„¶ä½¿ç”¨2ç»´ï¼Œä½†GNNä¼šè‡ªåŠ¨é™ç»´
        encoder = GNNEncoder(
            node_dim=8,  # å›ºå®š8ç»´èŠ‚ç‚¹ç‰¹å¾
            edge_dim=2,  # 2ç»´è¾¹ç‰¹å¾ï¼š[bandwidth, latency]ï¼ˆGNNä¼šè‡ªåŠ¨æˆªå–ï¼‰
            hidden_dim=gnn_config.get('hidden_dim', 64),
            output_dim=gnn_config.get('output_dim', 256),
            num_layers=gnn_config.get('layers', 3)
        )
        print(f"âœ… åˆ›å»ºBaseline GNNç¼–ç å™¨: 2ç»´è¾¹ç‰¹å¾ï¼ˆè‡ªåŠ¨é™ç»´ï¼‰")
    
    return encoder


# ä¸“é—¨çš„æµ‹è¯•å‡½æ•°
def test_baseline_dimension_fix():
    """æµ‹è¯•Baselineç»´åº¦ä¿®å¤"""
    print("ğŸ§ª æµ‹è¯•Baselineè¾¹ç‰¹å¾ç»´åº¦ä¿®å¤...")
    print("=" * 60)
    
    # æ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼šç¯å¢ƒè¾“å‡º4ç»´è¾¹ç‰¹å¾ï¼ŒBaselineæœŸæœ›2ç»´
    num_nodes = 10
    num_edges = 20
    
    # æ¨¡æ‹Ÿç¯å¢ƒæ•°æ®ï¼š8ç»´èŠ‚ç‚¹ç‰¹å¾ + 4ç»´è¾¹ç‰¹å¾
    x = torch.randn(num_nodes, 8)  # 8ç»´èŠ‚ç‚¹ç‰¹å¾
    edge_index = torch.randint(0, num_nodes, (2, num_edges))
    edge_attr = torch.randn(num_edges, 4)  # 4ç»´è¾¹ç‰¹å¾ï¼ˆç¯å¢ƒè¾“å‡ºï¼‰
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®:")
    print(f"   èŠ‚ç‚¹ç‰¹å¾: {x.shape}")
    print(f"   è¾¹ç‰¹å¾: {edge_attr.shape}")
    
    # æµ‹è¯•1: Baselineç¼–ç å™¨ï¼ˆæœŸæœ›2ç»´è¾¹ç‰¹å¾ï¼‰
    print(f"\nğŸ§ª æµ‹è¯•1: Baselineç¼–ç å™¨å¤„ç†4ç»´è¾¹ç‰¹å¾")
    baseline_encoder = GNNEncoder(node_dim=8, edge_dim=2, output_dim=256)
    
    data_baseline = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
    
    try:
        with torch.no_grad():
            output_baseline = baseline_encoder(data_baseline)
        print(f"âœ… Baselineæµ‹è¯•æˆåŠŸ: {output_baseline.shape}")
        assert output_baseline.shape[1] == 256, f"è¾“å‡ºç»´åº¦åº”ä¸º256ï¼Œå®é™…{output_baseline.shape[1]}"
        print(f"   âœ“ è¾“å‡ºç»´åº¦æ­£ç¡®: 256ç»´")
        
    except Exception as e:
        print(f"âŒ Baselineæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•2: Edge-awareç¼–ç å™¨ï¼ˆæœŸæœ›4ç»´è¾¹ç‰¹å¾ï¼‰
    print(f"\nğŸ§ª æµ‹è¯•2: Edge-awareç¼–ç å™¨å¤„ç†4ç»´è¾¹ç‰¹å¾")
    edge_aware_encoder = EdgeAwareGNNEncoder(node_dim=8, edge_dim=4, output_dim=256)
    
    data_edge_aware = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
    
    try:
        with torch.no_grad():
            output_edge_aware = edge_aware_encoder(data_edge_aware)
        print(f"âœ… Edge-awareæµ‹è¯•æˆåŠŸ: {output_edge_aware.shape}")
        assert output_edge_aware.shape[1] == 256, f"è¾“å‡ºç»´åº¦åº”ä¸º256ï¼Œå®é™…{output_edge_aware.shape[1]}"
        print(f"   âœ“ è¾“å‡ºç»´åº¦æ­£ç¡®: 256ç»´")
        
    except Exception as e:
        print(f"âŒ Edge-awareæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•3: ç»´åº¦ä¸€è‡´æ€§éªŒè¯
    print(f"\nğŸ§ª æµ‹è¯•3: è¾“å‡ºç»´åº¦ä¸€è‡´æ€§éªŒè¯")
    assert output_baseline.shape == output_edge_aware.shape, "ä¸¤ç§æ¨¡å¼è¾“å‡ºç»´åº¦ä¸ä¸€è‡´"
    print(f"âœ… è¾“å‡ºç»´åº¦ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡: {output_baseline.shape}")
    
    # æµ‹è¯•4: é…ç½®æ–‡ä»¶å·¥å‚å‡½æ•°
    print(f"\nğŸ§ª æµ‹è¯•4: é…ç½®æ–‡ä»¶å·¥å‚å‡½æ•°")
    test_config = {
        'gnn': {
            'edge_aware': {'hidden_dim': 128, 'output_dim': 256, 'layers': 4, 'edge_dim': 4},
            'baseline': {'hidden_dim': 64, 'output_dim': 256, 'layers': 3, 'edge_dim': 2}
        }
    }
    
    try:
        baseline_encoder_config = create_gnn_encoder(test_config, mode='baseline')
        edge_aware_encoder_config = create_gnn_encoder(test_config, mode='edge_aware')
        
        # æµ‹è¯•é…ç½®åˆ›å»ºçš„ç¼–ç å™¨
        with torch.no_grad():
            output_baseline_config = baseline_encoder_config(data_baseline)
            output_edge_aware_config = edge_aware_encoder_config(data_edge_aware)
            
        print(f"âœ… é…ç½®å·¥å‚å‡½æ•°æµ‹è¯•æˆåŠŸ")
        print(f"   Baselineè¾“å‡º: {output_baseline_config.shape}")
        print(f"   Edge-awareè¾“å‡º: {output_edge_aware_config.shape}")
        
    except Exception as e:
        print(f"âŒ é…ç½®å·¥å‚å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print(f"\nğŸ‰ æ‰€æœ‰Baselineç»´åº¦ä¿®å¤æµ‹è¯•é€šè¿‡!")
    print(f"æ ¸å¿ƒä¿®å¤ç‚¹:")
    print(f"  âœ… è‡ªåŠ¨æˆªå–4ç»´â†’2ç»´è¾¹ç‰¹å¾")
    print(f"  âœ… ä¿æŒ8ç»´èŠ‚ç‚¹ç‰¹å¾è¾“å…¥") 
    print(f"  âœ… ç¡®ä¿256ç»´è¾“å‡º")
    print(f"  âœ… å…¼å®¹ç°æœ‰é…ç½®æ–‡ä»¶")
    
    return True


def test_edge_dimension_scenarios():
    """æµ‹è¯•å„ç§è¾¹ç‰¹å¾ç»´åº¦åœºæ™¯"""
    print("\nğŸ§ª æµ‹è¯•è¾¹ç‰¹å¾ç»´åº¦å¤„ç†åœºæ™¯...")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    num_nodes = 5
    num_edges = 8
    x = torch.randn(num_nodes, 8)
    edge_index = torch.randint(0, num_nodes, (2, num_edges))
    
    # åœºæ™¯æµ‹è¯•
    scenarios = [
        (2, "2ç»´è¾¹ç‰¹å¾ â†’ 2ç»´æœŸæœ›"),
        (4, "4ç»´è¾¹ç‰¹å¾ â†’ 2ç»´æœŸæœ›"), 
        (6, "6ç»´è¾¹ç‰¹å¾ â†’ 2ç»´æœŸæœ›"),
        (1, "1ç»´è¾¹ç‰¹å¾ â†’ 2ç»´æœŸæœ›")
    ]
    
    baseline_encoder = GNNEncoder(node_dim=8, edge_dim=2)
    
    for edge_dim, description in scenarios:
        print(f"\nğŸ“Š åœºæ™¯: {description}")
        
        edge_attr = torch.randn(num_edges, edge_dim)
        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
        
        try:
            with torch.no_grad():
                output = baseline_encoder(data)
            print(f"   âœ… æˆåŠŸå¤„ç†: è¾“å…¥{edge_dim}ç»´ â†’ è¾“å‡º{output.shape}")
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
    
    print(f"\nâœ… è¾¹ç‰¹å¾ç»´åº¦å¤„ç†æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    success = test_baseline_dimension_fix()
    if success:
        test_edge_dimension_scenarios()