# models/gnn_encoder.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, Set2Set, global_mean_pool, global_add_pool
from torch_geometric.data import Data, Batch

class GNNEncoder(nn.Module):
    """
    å›¾ç¥ç»ç½‘ç»œç¼–ç å™¨ - è§£å†³VNFåµŒå…¥ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜
    
    åŠŸèƒ½ï¼š
    1. å¤„ç†å¯å˜å¤§å°çš„ç½‘ç»œå›¾ï¼ˆèŠ‚ç‚¹+è¾¹ç‰¹å¾ï¼‰
    2. ç¼–ç è¾¹ç¼˜ä¿¡æ¯ï¼ˆå¸¦å®½ã€å»¶è¿Ÿã€æŠ–åŠ¨ã€ä¸¢åŒ…ï¼‰
    3. è¾“å‡ºå›ºå®šå¤§å°çš„çŠ¶æ€è¡¨ç¤º
    """
    
    def __init__(self, node_dim=8, edge_dim=4, hidden_dim=128, output_dim=256, num_layers=3):
        super(GNNEncoder, self).__init__()
        
        self.node_dim = node_dim
        self.edge_dim = edge_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        
        # è¾“å…¥ç‰¹å¾é¢„å¤„ç†
        self.node_embedding = nn.Linear(node_dim, hidden_dim)
        self.edge_embedding = nn.Linear(edge_dim, hidden_dim)
        
        # å›¾å·ç§¯å±‚ - ä½¿ç”¨GATæ¥å¤„ç†è¾¹ç‰¹å¾
        self.conv_layers = nn.ModuleList()
        for i in range(num_layers):
            if i == 0:
                # ç¬¬ä¸€å±‚ï¼šå¤„ç†åŸå§‹ç‰¹å¾
                self.conv_layers.append(
                    GATConv(hidden_dim, hidden_dim, heads=4, concat=False, 
                           edge_dim=hidden_dim, dropout=0.1)
                )
            else:
                # åç»­å±‚ï¼šç‰¹å¾ä¼ æ’­å’Œèšåˆ
                self.conv_layers.append(
                    GATConv(hidden_dim, hidden_dim, heads=4, concat=False, 
                           edge_dim=hidden_dim, dropout=0.1)
                )
        
        # å…¨å±€æ± åŒ– - å…³é”®ï¼šå°†å¯å˜å¤§å°å›¾è½¬æ¢ä¸ºå›ºå®šå¤§å°
        self.global_pool = Set2Set(hidden_dim, processing_steps=3)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_layers = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),  # Set2Setè¾“å‡ºæ˜¯2*hidden_dim
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, output_dim),
            nn.ReLU()
        )
        
        # æ‰¹å½’ä¸€åŒ–
        self.batch_norms = nn.ModuleList([
            nn.BatchNorm1d(hidden_dim) for _ in range(num_layers)
        ])
        
    def forward(self, data):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            data: PyTorch Geometric Dataå¯¹è±¡æˆ–Batch
                - data.x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, node_dim]
                - data.edge_index: è¾¹ç´¢å¼• [2, num_edges] 
                - data.edge_attr: è¾¹ç‰¹å¾ [num_edges, edge_dim]
                - data.batch: æ‰¹æ¬¡ä¿¡æ¯ï¼ˆå¦‚æœæ˜¯batchï¼‰
        
        Returns:
            graph_embedding: å›ºå®šå¤§å°çš„å›¾è¡¨ç¤º [batch_size, output_dim]
        """
        
        # å¤„ç†å•ä¸ªå›¾å’Œæ‰¹é‡å›¾
        if isinstance(data, list):
            data = Batch.from_data_list(data)
        
        x = data.x.float()  # èŠ‚ç‚¹ç‰¹å¾
        edge_index = data.edge_index  # è¾¹ç´¢å¼•
        edge_attr = data.edge_attr.float() if data.edge_attr is not None else None  # è¾¹ç‰¹å¾
        batch = data.batch if hasattr(data, 'batch') else None
        
        # ç‰¹å¾é¢„å¤„ç†å’Œå½’ä¸€åŒ–
        x = self.node_embedding(x)
        if edge_attr is not None:
            edge_attr = self.edge_embedding(edge_attr)
            # è¾¹ç‰¹å¾å½’ä¸€åŒ– - é‡è¦ï¼šç¡®ä¿è®­ç»ƒç¨³å®šæ€§
            edge_attr = F.normalize(edge_attr, p=2, dim=1)
        
        # å¤šå±‚å›¾å·ç§¯ + æ®‹å·®è¿æ¥
        for i, conv in enumerate(self.conv_layers):
            x_residual = x
            
            # å›¾å·ç§¯
            if edge_attr is not None:
                x = conv(x, edge_index, edge_attr=edge_attr)
            else:
                x = conv(x, edge_index)
            
            # æ‰¹å½’ä¸€åŒ–å’Œæ¿€æ´»
            if batch is not None:
                # å¤„ç†æ‰¹é‡æ•°æ®çš„å½’ä¸€åŒ–
                x = self.batch_norms[i](x)
            else:
                # å•å›¾æ•°æ®
                if x.size(0) > 1:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡Œæ‰¹å½’ä¸€åŒ–
                    x = self.batch_norms[i](x)
            
            x = F.relu(x)
            
            # æ®‹å·®è¿æ¥ï¼ˆå¦‚æœç»´åº¦åŒ¹é…ï¼‰
            if x_residual.size() == x.size():
                x = x + x_residual
        
        # å…¨å±€æ± åŒ– - å…³é”®æ­¥éª¤ï¼šå›¾â†’å‘é‡
        if batch is not None:
            # æ‰¹é‡å¤„ç†
            graph_embedding = self.global_pool(x, batch)
        else:
            # å•å›¾å¤„ç† - åˆ›å»ºè™šæ‹Ÿbatch
            batch_single = torch.zeros(x.size(0), dtype=torch.long, device=x.device)
            graph_embedding = self.global_pool(x, batch_single)
        
        # è¾“å‡ºæŠ•å½±
        graph_embedding = self.output_layers(graph_embedding)
        
        # æœ€ç»ˆå½’ä¸€åŒ– - ç¡®ä¿è¾“å‡ºç¨³å®šæ€§
        graph_embedding = F.normalize(graph_embedding, p=2, dim=1)
        
        return graph_embedding
    
    def encode_network_state(self, graph, node_features, edge_features):
        """
        ä¾¿æ·æ–¹æ³•ï¼šç›´æ¥ä»ç½‘ç»œçŠ¶æ€ç”Ÿæˆç¼–ç 
        
        Args:
            graph: NetworkXå›¾å¯¹è±¡
            node_features: èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ
            edge_features: è¾¹ç‰¹å¾çŸ©é˜µ
            
        Returns:
            encoded_state: ç¼–ç åçš„ç½‘ç»œçŠ¶æ€
        """
        # è½¬æ¢ä¸ºPyG Dataæ ¼å¼
        edge_list = list(graph.edges())
        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
        
        x = torch.tensor(node_features, dtype=torch.float32)
        edge_attr = torch.tensor(edge_features, dtype=torch.float32)
        
        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
        
        # ç¼–ç 
        with torch.no_grad():
            encoded_state = self.forward(data)
        
        return encoded_state.squeeze(0)  # ç§»é™¤batchç»´åº¦
    
    def get_edge_importance(self, data):
        """
        è·å–è¾¹çš„é‡è¦æ€§æƒé‡ - ç”¨äºåˆ†æå“ªäº›ç½‘ç»œé“¾è·¯æœ€é‡è¦
        
        Returns:
            edge_weights: æ¯æ¡è¾¹çš„é‡è¦æ€§åˆ†æ•°
        """
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ³¨æ„åŠ›æƒé‡çš„æå–é€»è¾‘
        # æš‚æ—¶è¿”å›å¹³å‡å€¼
        if hasattr(data, 'edge_attr') and data.edge_attr is not None:
            return torch.ones(data.edge_attr.size(0))
        else:
            return torch.ones(data.edge_index.size(1))


class EdgeAwareGNNEncoder(GNNEncoder):
    """
    è¾¹ç¼˜æ„ŸçŸ¥çš„GNNç¼–ç å™¨ - ä¸“é—¨ä¸ºVNFåµŒå…¥ä¼˜åŒ–
    
    ç‰¹æ®ŠåŠŸèƒ½ï¼š
    1. å¼ºè°ƒè¾¹ç¼˜ç‰¹å¾ï¼ˆå¸¦å®½ã€å»¶è¿Ÿç­‰ï¼‰çš„é‡è¦æ€§
    2. æ·»åŠ VNFéœ€æ±‚æ„ŸçŸ¥æœºåˆ¶
    3. æ”¯æŒå¤šå°ºåº¦ç½‘ç»œè¡¨ç¤º
    """
    
    def __init__(self, node_dim=8, edge_dim=4, hidden_dim=128, output_dim=256, num_layers=3):
        super(EdgeAwareGNNEncoder, self).__init__(node_dim, edge_dim, hidden_dim, output_dim, num_layers)
        
        # VNFéœ€æ±‚ç¼–ç å™¨
        self.vnf_requirement_encoder = nn.Linear(4, hidden_dim)  # CPU, Memory, Bandwidth, Priority
        
        # è¾¹ç¼˜é‡è¦æ€§è¯„ä¼°å™¨
        self.edge_importance_net = nn.Sequential(
            nn.Linear(edge_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
    def forward_with_vnf_context(self, data, vnf_requirements=None):
        """
        å¸¦VNFä¸Šä¸‹æ–‡çš„å‰å‘ä¼ æ’­
        
        Args:
            data: ç½‘ç»œå›¾æ•°æ®
            vnf_requirements: VNFéœ€æ±‚ [batch_size, 4] æˆ– [4]
            
        Returns:
            context_aware_embedding: ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›¾åµŒå…¥
        """
        # æ ‡å‡†å›¾ç¼–ç 
        graph_embedding = self.forward(data)
        
        # å¦‚æœæœ‰VNFéœ€æ±‚ï¼Œèåˆä¸Šä¸‹æ–‡ä¿¡æ¯
        if vnf_requirements is not None:
            vnf_requirements = torch.tensor(vnf_requirements, dtype=torch.float32)
            if vnf_requirements.dim() == 1:
                vnf_requirements = vnf_requirements.unsqueeze(0)
            
            vnf_context = self.vnf_requirement_encoder(vnf_requirements)
            
            # ä¸Šä¸‹æ–‡èåˆ
            graph_embedding = graph_embedding + 0.3 * vnf_context
        
        return graph_embedding


# æµ‹è¯•å’ŒéªŒè¯å‡½æ•°
def test_gnn_encoder():
    """æµ‹è¯•GNNç¼–ç å™¨çš„åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•GNNç¼–ç å™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    num_nodes = 10
    num_edges = 20
    node_dim = 8
    edge_dim = 4
    
    # æ¨¡æ‹Ÿç½‘ç»œæ•°æ®
    x = torch.randn(num_nodes, node_dim)
    edge_index = torch.randint(0, num_nodes, (2, num_edges))
    edge_attr = torch.randn(num_edges, edge_dim)
    
    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = GNNEncoder(node_dim=node_dim, edge_dim=edge_dim, output_dim=256)
    
    # æµ‹è¯•ç¼–ç 
    with torch.no_grad():
        output = encoder(data)
    
    print(f"âœ… è¾“å…¥å›¾: {num_nodes}èŠ‚ç‚¹, {num_edges}è¾¹")
    print(f"âœ… è¾“å‡ºå‘é‡: {output.shape}")
    print(f"âœ… å›ºå®šå¤§å°ç¼–ç æˆåŠŸ!")
    
    # æµ‹è¯•ä¸åŒå¤§å°çš„å›¾
    data2 = Data(x=torch.randn(15, node_dim), 
                 edge_index=torch.randint(0, 15, (2, 30)),
                 edge_attr=torch.randn(30, edge_dim))
    
    with torch.no_grad():
        output2 = encoder(data2)
    
    print(f"âœ… ä¸åŒå¤§å°å›¾æµ‹è¯•: {output2.shape}")
    print(f"âœ… è¾“å‡ºç»´åº¦ä¸€è‡´: {output.shape == output2.shape}")

if __name__ == "__main__":
    test_gnn_encoder()