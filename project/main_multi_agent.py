# main_multi_agent.py

import os
import yaml
import torch
import numpy as np
import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Any
import argparse

# å¯¼å…¥ä¿®å¤åçš„ç»„ä»¶
from env.vnf_env_multi import MultiVNFEmbeddingEnv
from env.topology_loader import generate_topology
from agents.base_agent import create_agent
from utils.logger import Logger
from utils.metrics import calculate_sar, calculate_splat
from utils.visualization import plot_training_curves

class MultiAgentTrainer:
    """
    å¤šæ™ºèƒ½ä½“VNFåµŒå…¥è®­ç»ƒå™¨
    
    æ”¯æŒï¼š
    1. å¤šç§ç®—æ³•åŒæ—¶è®­ç»ƒå’Œå¯¹æ¯”ï¼ˆDDQN, DQN, PPOï¼‰
    2. Edge-aware vs Baselineå¯¹æ¯”
    3. å®Œæ•´çš„å®éªŒè®°å½•å’Œå¯è§†åŒ–
    """
    
    def __init__(self, config_path: str = "config/config.yaml"):
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # å®éªŒé…ç½®
        self.episodes = self.config['train']['episodes']
        self.save_interval = 50
        self.eval_interval = 25
        
        # æ™ºèƒ½ä½“ç±»å‹
        self.agent_types = ['ddqn', 'dqn', 'ppo']
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç½‘ç»œæ‹“æ‰‘
        self._setup_network_topology()
        
        # åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“
        self._setup_environments()
        self._setup_agents()
        
        # åˆå§‹åŒ–æ—¥å¿—è®°å½•
        self._setup_logging()
        
        print(f"âœ… å¤šæ™ºèƒ½ä½“è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ™ºèƒ½ä½“ç±»å‹: {self.agent_types}")
        print(f"   - è®­ç»ƒè½®æ•°: {self.episodes}")
        print(f"   - ç½‘ç»œèŠ‚ç‚¹: {len(self.graph.nodes())}")
    
    def _setup_network_topology(self):
        """è®¾ç½®ç½‘ç»œæ‹“æ‰‘"""
        topology_config = self.config.get('topology', {})
        
        if topology_config.get('type') == 'fat-tree':
            # Fat-treeæ‹“æ‰‘
            k = topology_config.get('k', 4)
            self.graph = self._create_fat_tree(k)
        else:
            # éšæœºæ‹“æ‰‘
            num_nodes = topology_config.get('num_nodes', 20)
            prob = topology_config.get('prob', 0.3)
            self.graph, self.node_features, self.edge_features = generate_topology(num_nodes, prob)
            return
        
        # ä¸ºFat-treeç”Ÿæˆç‰¹å¾
        num_nodes = len(self.graph.nodes())
        num_edges = len(self.graph.edges())
        
        # èŠ‚ç‚¹ç‰¹å¾ï¼š[CPU, Memory, Storage, Available_Bandwidth]
        self.node_features = np.random.rand(num_nodes, 4)
        self.node_features[:, 0] = self.node_features[:, 0] * 0.8 + 0.2  # CPU: 0.2-1.0
        self.node_features[:, 1] = self.node_features[:, 1] * 0.8 + 0.2  # Memory: 0.2-1.0
        self.node_features[:, 2] = self.node_features[:, 2] * 0.6 + 0.4  # Storage: 0.4-1.0
        self.node_features[:, 3] = self.node_features[:, 3] * 50 + 50    # Bandwidth: 50-100
        
        # è¾¹ç‰¹å¾ï¼š[Available_Bandwidth, Delay, Jitter, Loss_Rate]
        self.edge_features = np.random.rand(num_edges, 4)
        self.edge_features[:, 0] = self.edge_features[:, 0] * 80 + 20    # Bandwidth: 20-100
        self.edge_features[:, 1] = self.edge_features[:, 1] * 5 + 1      # Delay: 1-6 ms
        self.edge_features[:, 2] = self.edge_features[:, 2] * 0.5        # Jitter: 0-0.5 ms
        self.edge_features[:, 3] = self.edge_features[:, 3] * 0.02       # Loss: 0-2%
        
        print(f"ğŸŒ ç½‘ç»œæ‹“æ‰‘ç”Ÿæˆå®Œæˆ:")
        print(f"   - èŠ‚ç‚¹æ•°: {num_nodes}")
        print(f"   - è¾¹æ•°: {num_edges}")
        print(f"   - è¿é€šæ€§: {nx.is_connected(self.graph)}")
    
    def _create_fat_tree(self, k: int):
        """åˆ›å»ºFat-treeæ‹“æ‰‘"""
        # ç®€åŒ–çš„Fat-treeå®ç°
        G = nx.Graph()
        
        # è®¡ç®—å„å±‚èŠ‚ç‚¹æ•°
        core_switches = (k // 2) ** 2
        agg_switches = k * k // 2
        edge_switches = k * k // 2
        hosts = k ** 3 // 4
        
        total_nodes = core_switches + agg_switches + edge_switches + hosts
        
        # æ·»åŠ èŠ‚ç‚¹
        for i in range(total_nodes):
            G.add_node(i)
        
        # æ·»åŠ è¾¹ï¼ˆç®€åŒ–è¿æ¥ï¼‰
        # Coreåˆ°Aggregation
        for core in range(core_switches):
            for agg in range(core_switches, core_switches + agg_switches):
                if np.random.random() < 0.5:  # éšæœºè¿æ¥æ¨¡æ‹ŸFat-tree
                    G.add_edge(core, agg)
        
        # Aggregationåˆ°Edge
        for agg in range(core_switches, core_switches + agg_switches):
            for edge in range(core_switches + agg_switches, core_switches + agg_switches + edge_switches):
                if np.random.random() < 0.6:
                    G.add_edge(agg, edge)
        
        # Edgeåˆ°Host
        for edge in range(core_switches + agg_switches, core_switches + agg_switches + edge_switches):
            for host in range(core_switches + agg_switches + edge_switches, total_nodes):
                if np.random.random() < 0.3:
                    G.add_edge(edge, host)
        
        # ç¡®ä¿è¿é€šæ€§
        if not nx.is_connected(G):
            components = list(nx.connected_components(G))
            for i in range(len(components) - 1):
                node1 = list(components[i])[0]
                node2 = list(components[i + 1])[0]
                G.add_edge(node1, node2)
        
        return G
    
    def _setup_environments(self):
        """è®¾ç½®è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒ"""
        reward_config = self.config['reward']
        
        # Edge-awareç¯å¢ƒï¼ˆä½¿ç”¨å®Œæ•´çš„è¾¹ç‰¹å¾ï¼‰
        self.env_edge_aware = MultiVNFEmbeddingEnv(
            graph=self.graph,
            node_features=self.node_features,
            edge_features=self.edge_features,  # å®Œæ•´çš„4ç»´è¾¹ç‰¹å¾
            reward_config=reward_config,
            chain_length_range=(2, 5)
        )
        
        # Baselineç¯å¢ƒï¼ˆç®€åŒ–çš„è¾¹ç‰¹å¾ï¼Œæ¨¡æ‹Ÿä¼ ç»Ÿæ–¹æ³•ï¼‰
        baseline_edge_features = self.edge_features[:, :2]  # åªä½¿ç”¨å¸¦å®½å’Œå»¶è¿Ÿ
        baseline_edge_features = np.hstack([
            baseline_edge_features,
            np.zeros((baseline_edge_features.shape[0], 2))  # å¡«å……é›¶å€¼
        ])
        
        self.env_baseline = MultiVNFEmbeddingEnv(
            graph=self.graph,
            node_features=self.node_features,
            edge_features=baseline_edge_features,  # ç®€åŒ–çš„è¾¹ç‰¹å¾
            reward_config=reward_config,
            chain_length_range=(2, 5)
        )
        
        print(f"ğŸŒ ç¯å¢ƒè®¾ç½®å®Œæˆ:")
        print(f"   - Edge-awareç¯å¢ƒ: 4ç»´è¾¹ç‰¹å¾ (å¸¦å®½, å»¶è¿Ÿ, æŠ–åŠ¨, ä¸¢åŒ…)")
        print(f"   - Baselineç¯å¢ƒ: 2ç»´è¾¹ç‰¹å¾ (ä»…å¸¦å®½, å»¶è¿Ÿ)")
    
    def _setup_agents(self):
        """è®¾ç½®æ™ºèƒ½ä½“"""
        # è®¡ç®—å®é™…çš„çŠ¶æ€ç»´åº¦
        # ç¯å¢ƒè¿”å›çš„èŠ‚ç‚¹ç‰¹å¾ = åŸå§‹ç‰¹å¾ + çŠ¶æ€ä¿¡æ¯(4ç»´)
        actual_state_dim = self.node_features.shape[1] + 4  # +4 for [is_used, cpu_util, memory_util, vnf_count]
        action_dim = len(self.graph.nodes())
        edge_dim = self.edge_features.shape[1]
        
        print(f"ğŸ”§ æ™ºèƒ½ä½“å‚æ•°:")
        print(f"   - åŸå§‹èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {self.node_features.shape[1]}")
        print(f"   - å®é™…çŠ¶æ€ç»´åº¦: {actual_state_dim}")
        print(f"   - åŠ¨ä½œç»´åº¦: {action_dim}")
        print(f"   - è¾¹ç‰¹å¾ç»´åº¦: {edge_dim}")
        
        # Edge-awareæ™ºèƒ½ä½“
        self.agents_edge_aware = {}
        for agent_type in self.agent_types:
            agent_id = f"{agent_type}_edge_aware"
            self.agents_edge_aware[agent_type] = create_agent(
                agent_type=agent_type,
                agent_id=agent_id,
                state_dim=actual_state_dim,  # ä½¿ç”¨å®é™…çŠ¶æ€ç»´åº¦
                action_dim=action_dim,
                edge_dim=edge_dim,
                config=self.config
            )
        
        # Baselineæ™ºèƒ½ä½“
        self.agents_baseline = {}
        for agent_type in self.agent_types:
            agent_id = f"{agent_type}_baseline"
            self.agents_baseline[agent_type] = create_agent(
                agent_type=agent_type,
                agent_id=agent_id,
                state_dim=actual_state_dim,  # ä½¿ç”¨å®é™…çŠ¶æ€ç»´åº¦
                action_dim=action_dim,
                edge_dim=edge_dim,
                config=self.config
            )
        
        print(f"ğŸ¤– æ™ºèƒ½ä½“è®¾ç½®å®Œæˆ:")
        print(f"   - Edge-awareæ™ºèƒ½ä½“: {list(self.agents_edge_aware.keys())}")
        print(f"   - Baselineæ™ºèƒ½ä½“: {list(self.agents_baseline.keys())}")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ›å»ºæ—¥å¿—è®°å½•å™¨
        self.loggers = {}
        
        # Edge-awareæ™ºèƒ½ä½“æ—¥å¿—
        for agent_type in self.agent_types:
            logger_id = f"{agent_type}_edge_aware"
            self.loggers[logger_id] = Logger(
                log_dir=os.path.join(self.results_dir, f"{logger_id}_{timestamp}")
            )
        
        # Baselineæ™ºèƒ½ä½“æ—¥å¿—
        for agent_type in self.agent_types:
            logger_id = f"{agent_type}_baseline"
            self.loggers[logger_id] = Logger(
                log_dir=os.path.join(self.results_dir, f"{logger_id}_{timestamp}")
            )
        
        print(f"ğŸ“Š æ—¥å¿—è®°å½•è®¾ç½®å®Œæˆ")
    
    def train_single_episode(self, agent, env, agent_id: str) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªepisode
        
        Args:
            agent: æ™ºèƒ½ä½“
            env: ç¯å¢ƒ
            agent_id: æ™ºèƒ½ä½“ID
            
        Returns:
            episode_stats: Episodeç»Ÿè®¡ä¿¡æ¯
        """
        state = env.reset()
        total_reward = 0.0
        step_count = 0
        success = False
        
        # é‡ç½®episodeç»Ÿè®¡
        agent.reset_episode_stats()
        
        while step_count < env.max_episode_steps:
            # è·å–æœ‰æ•ˆåŠ¨ä½œ
            valid_actions = env.get_valid_actions()
            if not valid_actions:
                break
            
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, valid_actions=valid_actions)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, action, reward, next_state, done, info=info)
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            total_reward += reward
            step_count += 1
            
            # å­¦ä¹ ï¼ˆæ ¹æ®æ™ºèƒ½ä½“ç±»å‹ï¼‰
            if hasattr(agent, 'should_update') and agent.should_update():
                # PPOæ™ºèƒ½ä½“
                learning_info = agent.learn()
            elif len(getattr(agent, 'replay_buffer', [])) >= agent.batch_size:
                # DQNç³»åˆ—æ™ºèƒ½ä½“
                learning_info = agent.learn()
            
            if done:
                success = info.get('success', False)
                break
        
        # PPOæ™ºèƒ½ä½“çš„æœ€ç»ˆå­¦ä¹ 
        if hasattr(agent, 'should_update') and len(agent.experiences) > 0:
            learning_info = agent.learn()
        
        # è®¡ç®—SARå’ŒSPLat
        sar = 1.0 if success else 0.0
        splat = info.get('splat', step_count) if success else float('inf')
        
        episode_stats = {
            'total_reward': total_reward,
            'steps': step_count,
            'success': success,
            'sar': sar,
            'splat': splat,
            'info': info
        }
        
        return episode_stats
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\nğŸš€ å¼€å§‹å¤šæ™ºèƒ½ä½“è®­ç»ƒ...")
        print(f"ç›®æ ‡episodes: {self.episodes}")
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = {
            'edge_aware': {agent_type: {'rewards': [], 'sar': [], 'splat': [], 'success': []} 
                          for agent_type in self.agent_types},
            'baseline': {agent_type: {'rewards': [], 'sar': [], 'splat': [], 'success': []} 
                        for agent_type in self.agent_types}
        }
        
        for episode in range(1, self.episodes + 1):
            print(f"\nğŸ“ Episode {episode}/{self.episodes}")
            
            # è®­ç»ƒEdge-awareæ™ºèƒ½ä½“
            for agent_type in self.agent_types:
                agent = self.agents_edge_aware[agent_type]
                env = self.env_edge_aware
                
                episode_stats = self.train_single_episode(agent, env, f"{agent_type}_edge_aware")
                
                # è®°å½•ç»“æœ
                all_results['edge_aware'][agent_type]['rewards'].append(episode_stats['total_reward'])
                all_results['edge_aware'][agent_type]['sar'].append(episode_stats['sar'])
                all_results['edge_aware'][agent_type]['splat'].append(episode_stats['splat'])
                all_results['edge_aware'][agent_type]['success'].append(episode_stats['success'])
                
                # è®°å½•åˆ°æ—¥å¿—
                logger_id = f"{agent_type}_edge_aware"
                if logger_id in self.loggers:
                    self.loggers[logger_id].log_episode(episode, episode_stats)
            
            # è®­ç»ƒBaselineæ™ºèƒ½ä½“
            for agent_type in self.agent_types:
                agent = self.agents_baseline[agent_type]
                env = self.env_baseline
                
                episode_stats = self.train_single_episode(agent, env, f"{agent_type}_baseline")
                
                # è®°å½•ç»“æœ
                all_results['baseline'][agent_type]['rewards'].append(episode_stats['total_reward'])
                all_results['baseline'][agent_type]['sar'].append(episode_stats['sar'])
                all_results['baseline'][agent_type]['splat'].append(episode_stats['splat'])
                all_results['baseline'][agent_type]['success'].append(episode_stats['success'])
                
                # è®°å½•åˆ°æ—¥å¿—
                logger_id = f"{agent_type}_baseline"
                if logger_id in self.loggers:
                    self.loggers[logger_id].log_episode(episode, episode_stats)
            
            # å®šæœŸæ‰“å°è¿›åº¦
            if episode % 25 == 0:
                self._print_progress(episode, all_results)
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if episode % self.save_interval == 0:
                self._save_models(episode)
            
            # å®šæœŸç”Ÿæˆå¯è§†åŒ–
            if episode % 50 == 0:
                self._generate_visualizations(episode, all_results)
        
        # è®­ç»ƒå®Œæˆåçš„æœ€ç»ˆåˆ†æ
        self._final_analysis(all_results)
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        return all_results
    
    def _print_progress(self, episode: int, results: Dict):
        """æ‰“å°è®­ç»ƒè¿›åº¦"""
        print(f"\nğŸ“Š Episode {episode} æ€§èƒ½ç»Ÿè®¡:")
        
        # è®¡ç®—æœ€è¿‘25ä¸ªepisodeçš„å¹³å‡æ€§èƒ½
        window = 25
        start_idx = max(0, episode - window)
        
        for variant in ['edge_aware', 'baseline']:
            print(f"\n{variant.upper()}:")
            for agent_type in self.agent_types:
                recent_sar = np.mean(results[variant][agent_type]['sar'][start_idx:])
                recent_splat = np.mean([s for s in results[variant][agent_type]['splat'][start_idx:] 
                                      if s != float('inf')])
                recent_reward = np.mean(results[variant][agent_type]['rewards'][start_idx:])
                
                print(f"  {agent_type.upper()}: SAR={recent_sar:.3f}, SPLat={recent_splat:.2f}, Reward={recent_reward:.1f}")
    
    def _save_models(self, episode: int):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(self.results_dir, "checkpoints", f"episode_{episode}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜Edge-awareæ™ºèƒ½ä½“
        for agent_type, agent in self.agents_edge_aware.items():
            filepath = os.path.join(checkpoint_dir, f"{agent_type}_edge_aware.pth")
            agent.save_checkpoint(filepath)
        
        # ä¿å­˜Baselineæ™ºèƒ½ä½“
        for agent_type, agent in self.agents_baseline.items():
            filepath = os.path.join(checkpoint_dir, f"{agent_type}_baseline.pth")
            agent.save_checkpoint(filepath)
    
    def _generate_visualizations(self, episode: int, results: Dict):
        """ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–"""
        # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # SARå¯¹æ¯”
        axes[0, 0].set_title('Service Acceptance Rate (SAR)')
        for agent_type in self.agent_types:
            # Edge-aware
            episodes = list(range(1, len(results['edge_aware'][agent_type]['sar']) + 1))
            axes[0, 0].plot(episodes, results['edge_aware'][agent_type]['sar'], 
                          label=f'{agent_type.upper()} (Edge-aware)', alpha=0.8)
            # Baseline
            axes[0, 0].plot(episodes, results['baseline'][agent_type]['sar'], 
                          label=f'{agent_type.upper()} (Baseline)', alpha=0.8, linestyle='--')
        axes[0, 0].legend()
        axes[0, 0].set_ylabel('SAR')
        axes[0, 0].grid(True)
        
        # SPLatå¯¹æ¯”
        axes[0, 1].set_title('Service Path Latency (SPLat)')
        for agent_type in self.agent_types:
            # è¿‡æ»¤æ— ç©·å¤§å€¼
            edge_splat = [s for s in results['edge_aware'][agent_type]['splat'] if s != float('inf')]
            baseline_splat = [s for s in results['baseline'][agent_type]['splat'] if s != float('inf')]
            
            if edge_splat:
                axes[0, 1].plot(range(1, len(edge_splat) + 1), edge_splat, 
                              label=f'{agent_type.upper()} (Edge-aware)', alpha=0.8)
            if baseline_splat:
                axes[0, 1].plot(range(1, len(baseline_splat) + 1), baseline_splat, 
                              label=f'{agent_type.upper()} (Baseline)', alpha=0.8, linestyle='--')
        axes[0, 1].legend()
        axes[0, 1].set_ylabel('SPLat')
        axes[0, 1].grid(True)
        
        # å¥–åŠ±å¯¹æ¯”
        axes[1, 0].set_title('Training Rewards')
        for agent_type in self.agent_types:
            episodes = list(range(1, len(results['edge_aware'][agent_type]['rewards']) + 1))
            axes[1, 0].plot(episodes, results['edge_aware'][agent_type]['rewards'], 
                          label=f'{agent_type.upper()} (Edge-aware)', alpha=0.8)
            axes[1, 0].plot(episodes, results['baseline'][agent_type]['rewards'], 
                          label=f'{agent_type.upper()} (Baseline)', alpha=0.8, linestyle='--')
        axes[1, 0].legend()
        axes[1, 0].set_ylabel('Reward')
        axes[1, 0].grid(True)
        
        # æˆåŠŸç‡å¯¹æ¯”
        axes[1, 1].set_title('Success Rate')
        for agent_type in self.agent_types:
            # è®¡ç®—æ»‘åŠ¨å¹³å‡æˆåŠŸç‡
            window = 20
            edge_success = self._rolling_average(results['edge_aware'][agent_type]['success'], window)
            baseline_success = self._rolling_average(results['baseline'][agent_type]['success'], window)
            
            episodes = list(range(window, len(edge_success) + window))
            axes[1, 1].plot(episodes, edge_success, 
                          label=f'{agent_type.upper()} (Edge-aware)', alpha=0.8)
            axes[1, 1].plot(episodes, baseline_success, 
                          label=f'{agent_type.upper()} (Baseline)', alpha=0.8, linestyle='--')
        axes[1, 1].legend()
        axes[1, 1].set_ylabel('Success Rate')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, f'training_progress_episode_{episode}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _rolling_average(self, data: List[float], window: int) -> List[float]:
        """è®¡ç®—æ»‘åŠ¨å¹³å‡"""
        if len(data) < window:
            return []
        
        return [np.mean(data[i:i+window]) for i in range(len(data) - window + 1)]
    
    def _final_analysis(self, results: Dict):
        """æœ€ç»ˆæ€§èƒ½åˆ†æ"""
        print(f"\nğŸ¯ æœ€ç»ˆæ€§èƒ½åˆ†æ:")
        print(f"{'='*60}")
        
        # è®¡ç®—æœ€å50ä¸ªepisodeçš„å¹³å‡æ€§èƒ½
        window = 50
        
        summary_data = []
        
        for variant in ['edge_aware', 'baseline']:
            print(f"\n{variant.upper()} ç»“æœ:")
            for agent_type in self.agent_types:
                recent_sar = np.mean(results[variant][agent_type]['sar'][-window:])
                recent_splat = np.mean([s for s in results[variant][agent_type]['splat'][-window:] 
                                      if s != float('inf')])
                recent_reward = np.mean(results[variant][agent_type]['rewards'][-window:])
                recent_success = np.mean(results[variant][agent_type]['success'][-window:])
                
                print(f"  {agent_type.upper()}:")
                print(f"    SAR: {recent_sar:.3f}")
                print(f"    SPLat: {recent_splat:.2f}")
                print(f"    Reward: {recent_reward:.1f}")
                print(f"    Success Rate: {recent_success:.3f}")
                
                summary_data.append({
                    'Variant': variant,
                    'Algorithm': agent_type.upper(),
                    'SAR': recent_sar,
                    'SPLat': recent_splat,
                    'Reward': recent_reward,
                    'Success_Rate': recent_success
                })
        
        # ä¿å­˜CSVç»“æœ
        df = pd.DataFrame(summary_data)
        df.to_csv(os.path.join(self.results_dir, 'final_results_summary.csv'), index=False)
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        print(f"\nğŸ“ˆ Edge-aware vs Baseline æ”¹è¿›å¹…åº¦:")
        for agent_type in self.agent_types:
            edge_sar = np.mean(results['edge_aware'][agent_type]['sar'][-window:])
            baseline_sar = np.mean(results['baseline'][agent_type]['sar'][-window:])
            sar_improvement = ((edge_sar - baseline_sar) / baseline_sar) * 100 if baseline_sar > 0 else 0
            
            edge_splat = np.mean([s for s in results['edge_aware'][agent_type]['splat'][-window:] 
                                if s != float('inf')])
            baseline_splat = np.mean([s for s in results['baseline'][agent_type]['splat'][-window:] 
                                    if s != float('inf')])
            splat_improvement = ((baseline_splat - edge_splat) / baseline_splat) * 100 if baseline_splat > 0 else 0
            
            print(f"  {agent_type.upper()}:")
            print(f"    SARæ”¹è¿›: {sar_improvement:+.1f}%")
            print(f"    SPLatæ”¹è¿›: {splat_improvement:+.1f}%")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='VNFåµŒå…¥å¤šæ™ºèƒ½ä½“è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='config/config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiAgentTrainer(config_path=args.config)
    
    # è¦†ç›–é…ç½®ä¸­çš„episodesæ•°ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.episodes:
        trainer.episodes = args.episodes
        trainer.config['train']['episodes'] = args.episodes
    
    # å¼€å§‹è®­ç»ƒ
    results = trainer.train()
    
    print(f"\nâœ… è®­ç»ƒä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {trainer.results_dir}")


if __name__ == "__main__":
    main()