# main_multi_agent.py - ä¿®å¤ç‰ˆï¼šä½¿ç”¨æ–°çš„é…ç½®åŠ è½½å™¨

import os
import yaml
import torch
import numpy as np
import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Any
import argparse
import random

from env.vnf_env_multi import MultiVNFEmbeddingEnv
from env.topology_loader import generate_topology
from agents.base_agent import create_agent
from utils.logger import Logger
from utils.metrics import calculate_sar, calculate_splat
# âœ… å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ–°çš„é…ç½®åŠ è½½å™¨
from config_loader import get_scenario_config, print_scenario_plan, validate_all_configs, load_config
# åœ¨main_multi_agent.pyä¸­  
from rewards.enhanced_edge_aware_reward import compute_enhanced_edge_aware_reward
from project.enhanced_training_system import SafeEnhancedTrainer

class MultiAgentTrainer:
    """
    å¤šæ™ºèƒ½ä½“VNFåµŒå…¥è®­ç»ƒå™¨ - é…ç½®åŠ è½½å™¨ä¿®å¤ç‰ˆæœ¬
    
    ä¸»è¦ä¿®å¤ï¼š
    1. âœ… ä½¿ç”¨ç»Ÿä¸€çš„config_loaderæ›¿ä»£æ—§çš„åœºæ™¯é…ç½®ç³»ç»Ÿ
    2. âœ… æ­£ç¡®çš„åœºæ™¯é…ç½®ä¼ é€’æœºåˆ¶
    3. âœ… é¿å…é‡å¤èµ„æºä¿®æ”¹é€ æˆçš„å†²çª
    4. âœ… ç¡®ä¿åœºæ™¯åç§°æ­£ç¡®æ˜¾ç¤º
    5. âœ… åœºæ™¯é—´èµ„æºé…ç½®çš„å¹³æ»‘è¿‡æ¸¡
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        # âœ… ä½¿ç”¨æ–°çš„é…ç½®åŠ è½½å™¨
        self.config = load_config(config_path)
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.episodes = self.config['train']['episodes']
        self.save_interval = 50
        self.eval_interval = 25
        self.agent_types = ['ddqn', 'dqn', 'ppo']
        
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # âœ… æ¸è¿›å¼åœºæ™¯ç›¸å…³ - ä½¿ç”¨æ–°é…ç½®ç³»ç»Ÿ
        self.current_scenario = "normal_operation"
        self.scenario_start_episode = 1
        self.last_applied_scenario = None  # é¿å…é‡å¤åº”ç”¨
        
        # âœ… åœºæ™¯ä¿¡æ¯æ˜ å°„
        self.scenario_info = {
            'normal_operation': {
                'name': 'æ­£å¸¸è¿è¥æœŸ',
                'episodes': [1, 25],
                'expected_sar': '80-95%',
                'realism': 'â­â­â­â­â­',
                'focus': 'åŸºç¡€åŠŸèƒ½éªŒè¯'
            },
            'peak_congestion': {
                'name': 'é«˜å³°æ‹¥å¡æœŸ',
                'episodes': [26, 50],
                'expected_sar': '65-80%',
                'realism': 'â­â­â­â­',
                'focus': 'Edge-awareä¼˜åŠ¿ä½“ç°'
            },
            'failure_recovery': {
                'name': 'æ•…éšœæ¢å¤æœŸ',
                'episodes': [51, 75],
                'expected_sar': '50-65%',
                'realism': 'â­â­â­',
                'focus': 'é²æ£’æ€§éªŒè¯'
            },
            'extreme_pressure': {
                'name': 'æé™å‹åŠ›æœŸ',
                'episodes': [76, 100],
                'expected_sar': '35-50%',
                'realism': 'â­â­',
                'focus': 'ç®—æ³•è¾¹ç•Œç ”ç©¶'
            }
        }
        
        # æ‰“å°è®­ç»ƒè®¡åˆ’
        print_scenario_plan()
        
        # éªŒè¯é…ç½®
        print("\nğŸ” éªŒè¯é…ç½®æ–‡ä»¶...")
        validate_all_configs()
        
        self._setup_network_topology()
        self._setup_environments()
        self._setup_agents()
        self._setup_logging()
        
        print(f"âœ… å¤šæ™ºèƒ½ä½“è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ (é…ç½®åŠ è½½å™¨ä¿®å¤ç‰ˆ)")
        print(f"   - æ™ºèƒ½ä½“ç±»å‹: {self.agent_types}")
        print(f"   - è®­ç»ƒè½®æ•°: {self.episodes}")
        print(f"   - ç½‘ç»œèŠ‚ç‚¹: {len(self.graph.nodes())}")
    
    def _setup_network_topology(self):
        """è®¾ç½®ç½‘ç»œæ‹“æ‰‘"""
        # âœ… ä½¿ç”¨å®Œæ•´çš„é…ç½®å­—å…¸ç”Ÿæˆæ‹“æ‰‘
        full_config = {
            'topology': self.config['topology'],
            'vnf_requirements': self.config['vnf_requirements'],
            'dimensions': self.config['dimensions']
        }
        
        self.graph, self.node_features, self.edge_features = generate_topology(config=full_config)
        
        num_nodes = len(self.graph.nodes())
        num_edges = len(self.graph.edges())
        
        # éªŒè¯ç½‘ç»œæ‹“æ‰‘
        if self.graph.edges():
            sample_edge = list(self.graph.edges(data=True))[0]
            edge_attrs = list(sample_edge[2].keys())
            bandwidths = [self.graph.edges[u, v].get('bandwidth', 0) for u, v in self.graph.edges()]
            
            print(f"ğŸŒ ç½‘ç»œæ‹“æ‰‘ç”Ÿæˆå®Œæˆ:")
            print(f"   - èŠ‚ç‚¹æ•°: {num_nodes}")
            print(f"   - è¾¹æ•°: {num_edges}")
            print(f"   - è¿é€šæ€§: {nx.is_connected(self.graph)}")
            print(f"   - èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {self.node_features.shape[1]} (é¢„æœŸ4ç»´)")
            print(f"   - è¾¹ç‰¹å¾ç»´åº¦: {self.edge_features.shape[1]} (é¢„æœŸ4ç»´)")
            print(f"   - è¾¹å±æ€§: {edge_attrs}")
            print(f"   - å¸¦å®½èŒƒå›´: {min(bandwidths):.1f} - {max(bandwidths):.1f}")
            
            # âœ… éªŒè¯ç»´åº¦ä¸€è‡´æ€§
            assert self.node_features.shape[1] == 4, f"èŠ‚ç‚¹ç‰¹å¾åº”ä¸º4ç»´ï¼Œå®é™…{self.node_features.shape[1]}ç»´"
            assert self.edge_features.shape[1] == 4, f"è¾¹ç‰¹å¾åº”ä¸º4ç»´ï¼Œå®é™…{self.edge_features.shape[1]}ç»´"
            print(f"   âœ… ç‰¹å¾ç»´åº¦éªŒè¯é€šè¿‡")
    
    def _setup_environments(self):
        """è®¾ç½®è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒ"""
        reward_config = self.config['reward']
        chain_length_range = tuple(self.config['vnf_requirements']['chain_length_range'])
        
        # âœ… åˆ›å»ºå®Œæ•´çš„é…ç½®å­—å…¸ä¼ é€’ç»™ç¯å¢ƒ
        env_config = {
            'topology': self.config['topology'],
            'vnf_requirements': self.config['vnf_requirements'],
            'reward': self.config['reward'],
            'train': self.config['train'],
            'dimensions': self.config['dimensions']
        }
        
        # Edge-awareç¯å¢ƒï¼ˆä½¿ç”¨å®Œæ•´çš„4ç»´è¾¹ç‰¹å¾ï¼‰
        self.env_edge_aware = MultiVNFEmbeddingEnv(
            graph=self.graph.copy(),
            node_features=self.node_features.copy(),
            edge_features=self.edge_features.copy(),
            reward_config=reward_config,
            chain_length_range=chain_length_range,
            config=env_config.copy()
        )
        
        # Baselineç¯å¢ƒ
        self.env_baseline = MultiVNFEmbeddingEnv(
            graph=self.graph.copy(),
            node_features=self.node_features.copy(),
            edge_features=self.edge_features.copy(),
            reward_config=reward_config,
            chain_length_range=chain_length_range,
            config=env_config.copy()
        )
        # æ ‡è®°Baselineç¯å¢ƒï¼Œè®©æ™ºèƒ½ä½“åªçœ‹åˆ°2ç»´ç‰¹å¾
        self.env_baseline.is_baseline_mode = True
        
        print(f"ğŸŒ ç¯å¢ƒè®¾ç½®å®Œæˆ:")
        print(f"   - Edge-awareç¯å¢ƒ: 4ç»´è¾¹ç‰¹å¾ (å¸¦å®½, å»¶è¿Ÿ, æŠ–åŠ¨, ä¸¢åŒ…)")
        print(f"   - Baselineç¯å¢ƒ: 4ç»´ç¯å¢ƒç‰¹å¾ï¼Œä½†æ™ºèƒ½ä½“åªæ„ŸçŸ¥2ç»´ (å¸¦å®½, å»¶è¿Ÿ)")
    
    def _setup_agents(self):
        """è®¾ç½®æ™ºèƒ½ä½“"""
        # âœ… æ ¹æ®é…ç½®æ–‡ä»¶ç¡®å®šç»´åº¦
        expected_node_dim = self.config['dimensions']['node_feature_dim']  # 8ç»´
        actual_state_dim = expected_node_dim  # ç¯å¢ƒç›´æ¥æä¾›8ç»´
        action_dim = len(self.graph.nodes())
        
        print(f"ğŸ”§ æ™ºèƒ½ä½“å‚æ•°:")
        print(f"   - åŸå§‹èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {self.node_features.shape[1]} (4ç»´åŸºç¡€)")
        print(f"   - ç¯å¢ƒè¾“å‡ºçŠ¶æ€ç»´åº¦: {expected_node_dim} (8ç»´æ‰©å±•)")
        print(f"   - æ™ºèƒ½ä½“è¾“å…¥ç»´åº¦: {actual_state_dim}")
        print(f"   - åŠ¨ä½œç»´åº¦: {action_dim}")
        
        # Edge-awareæ™ºèƒ½ä½“
        self.agents_edge_aware = {}
        for agent_type in self.agent_types:
            agent_id = f"{agent_type}_edge_aware"
            edge_dim = self.config['gnn']['edge_aware']['edge_dim']
            self.agents_edge_aware[agent_type] = create_agent(
                agent_type=agent_type,
                agent_id=agent_id,
                state_dim=actual_state_dim,
                action_dim=action_dim,
                edge_dim=edge_dim,
                config=self.config
            )
            print(f"ğŸ¤– Agent {agent_id} ä½¿ç”¨è¾¹ç‰¹å¾ç»´åº¦: {edge_dim}")
        
        # Baselineæ™ºèƒ½ä½“
        self.agents_baseline = {}
        for agent_type in self.agent_types:
            agent_id = f"{agent_type}_baseline"
            edge_dim = self.config['gnn']['baseline']['edge_dim']
            self.agents_baseline[agent_type] = create_agent(
                agent_type=agent_type,
                agent_id=agent_id,
                state_dim=actual_state_dim,
                action_dim=action_dim,
                edge_dim=edge_dim,
                config=self.config
            )
            print(f"ğŸ¤– Agent {agent_id} ä½¿ç”¨è¾¹ç‰¹å¾ç»´åº¦: {edge_dim}")
        
        print(f"ğŸ¤– æ™ºèƒ½ä½“è®¾ç½®å®Œæˆ:")
        print(f"   - Edge-awareæ™ºèƒ½ä½“: {list(self.agents_edge_aware.keys())}")
        print(f"   - Baselineæ™ºèƒ½ä½“: {list(self.agents_baseline.keys())}")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.loggers = {}
        
        for agent_type in self.agent_types:
            # Edge-awareæ—¥å¿—å™¨
            logger_id = f"{agent_type}_edge_aware"
            self.loggers[logger_id] = Logger(
                log_dir=os.path.join(self.results_dir, f"{logger_id}_{timestamp}")
            )
            # Baselineæ—¥å¿—å™¨
            logger_id = f"{agent_type}_baseline"
            self.loggers[logger_id] = Logger(
                log_dir=os.path.join(self.results_dir, f"{logger_id}_{timestamp}")
            )
        
        print(f"ğŸ“Š æ—¥å¿—è®°å½•è®¾ç½®å®Œæˆ")

    def _update_scenario(self, episode: int):
        """âœ… ä¿®å¤ç‰ˆï¼šæ›´æ–°å½“å‰åœºæ™¯ - ä½¿ç”¨æ–°é…ç½®ç³»ç»Ÿ"""
        new_scenario = None
        
        # åŸºäºepisodeæ•°é‡ç¡®å®šå½“å‰åº”è¯¥æ˜¯å“ªä¸ªåœºæ™¯
        if episode <= 25:
            new_scenario = "normal_operation"
        elif episode <= 50:
            new_scenario = "peak_congestion"
        elif episode <= 75:
            new_scenario = "failure_recovery"
        else:
            new_scenario = "extreme_pressure"
        
        # âœ… å…³é”®ä¿®å¤ï¼šåªåœ¨åœºæ™¯çœŸæ­£æ”¹å˜æ—¶æ‰åº”ç”¨é…ç½®
        if new_scenario and new_scenario != self.current_scenario:
            print(f"\nğŸ¯ åœºæ™¯åˆ‡æ¢: {self.current_scenario} â†’ {new_scenario}")
            
            old_scenario = self.current_scenario
            self.current_scenario = new_scenario
            self.scenario_start_episode = episode
            
            # âœ… ä½¿ç”¨æ–°çš„é…ç½®åŠ è½½å™¨è·å–åœºæ™¯é…ç½®
            scenario_config = get_scenario_config(episode)
            
            # æ˜¾ç¤ºåœºæ™¯ä¿¡æ¯
            current_scenario_info = self.scenario_info[new_scenario]
            print(f"ğŸ“ Episode {episode}: è¿›å…¥ {current_scenario_info['name']}")
            print(f"   ç°å®æ€§ç­‰çº§: {current_scenario_info['realism']}")
            print(f"   é¢„æœŸSARèŒƒå›´: {current_scenario_info['expected_sar']}")
            print(f"   ç ”ç©¶ç„¦ç‚¹: {current_scenario_info['focus']}")
            
            # âœ… ç›´æ¥åº”ç”¨åœºæ™¯é…ç½®åˆ°ç¯å¢ƒå¹¶éªŒè¯
            print(f"ğŸ”§ åº”ç”¨åœºæ™¯é…ç½®åˆ°ç¯å¢ƒ...")
            print(f"   é…ç½®è¯¦æƒ…: {scenario_config['topology']['node_resources']}")
            print(f"   VNFéœ€æ±‚: {scenario_config['vnf_requirements']}")
            self.env_edge_aware.apply_scenario_config(scenario_config)
            self.env_baseline.apply_scenario_config(scenario_config)
            
            # éªŒè¯èµ„æºæ›´æ–°
            bandwidths = [self.env_edge_aware.graph.edges[u, v].get('bandwidth', 0) for u, v in self.env_edge_aware.graph.edges()]
            print(f"   æ›´æ–°åå¸¦å®½èŒƒå›´: {min(bandwidths):.1f} - {max(bandwidths):.1f}")
            print(f"   {'-'*50}")
            self.last_applied_scenario = new_scenario
            return True
        return False
    
    def train_single_episode(self, agent, env, agent_id: str) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªepisode"""
        state = env.reset()
        total_reward = 0.0
        step_count = 0
        success = False
        info = {}
        
        # é‡ç½®æ™ºèƒ½ä½“episodeç»Ÿè®¡
        if hasattr(agent, 'reset_episode_stats'):
            agent.reset_episode_stats()
        
        max_steps = getattr(env, 'max_episode_steps', 20)
        
        while step_count < max_steps:
            # è·å–æœ‰æ•ˆåŠ¨ä½œ
            valid_actions = env.get_valid_actions()
            if not valid_actions:
                info = {'success': False, 'reason': 'no_valid_actions'}
                break
            
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, valid_actions=valid_actions)
            if action not in valid_actions:
                action = random.choice(valid_actions)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, action, reward, next_state, done)
            
            # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
            state = next_state
            total_reward += reward
            step_count += 1
            
            # å­¦ä¹ æ›´æ–°
            try:
                if hasattr(agent, 'should_update') and agent.should_update():
                    learning_info = agent.learn()
                elif hasattr(agent, 'replay_buffer') and len(getattr(agent, 'replay_buffer', [])) >= getattr(agent, 'batch_size', 32):
                    learning_info = agent.learn()
            except Exception as e:
                pass  # å¿½ç•¥å­¦ä¹ é”™è¯¯
            
            if done:
                success = info.get('success', False)
                print(f"Episodeç»“æŸ: agent={agent_id}, success={success}, reason={info.get('reason', 'unknown')}")  # âœ… æ·»åŠ è°ƒè¯•ä¿¡æ¯
                break
        
        # æœ€åä¸€æ¬¡å­¦ä¹ æ›´æ–°
        try:
            if hasattr(agent, 'experiences') and len(getattr(agent, 'experiences', [])) > 0:
                if hasattr(agent, 'should_update') and agent.should_update():
                    learning_info = agent.learn()
        except Exception as e:
            pass
        
        # âœ… å…³é”®ä¿®å¤ï¼šæ­£ç¡®è·å–åœºæ™¯åç§°ç”¨äºç»Ÿè®¡
        current_scenario_name = getattr(env, 'current_scenario_name', self.current_scenario)
        
        # è®¡ç®—episodeç»Ÿè®¡
        sar = 1.0 if success else 0.0
        splat = info.get('splat', info.get('avg_delay', float('inf'))) if success else float('inf')
        jitter = info.get('avg_jitter', 0.0) if success else 0.0
        loss = info.get('avg_loss', 0.0) if success else 0.0
        
        episode_stats = {
            'total_reward': total_reward,
            'steps': step_count,
            'success': success,
            'sar': sar,
            'splat': splat,
            'jitter': jitter,
            'loss': loss,
            'info': info,
            'scenario': current_scenario_name
        }
        
        return episode_stats
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯ - é…ç½®åŠ è½½å™¨ä¿®å¤ç‰ˆæœ¬"""
        print(f"\nğŸš€ å¼€å§‹å¤šæ™ºèƒ½ä½“æ¸è¿›å¼åœºæ™¯è®­ç»ƒ (é…ç½®åŠ è½½å™¨ä¿®å¤ç‰ˆ)...")
        print(f"ç›®æ ‡episodes: {self.episodes}")
        
        all_results = {
            'edge_aware': {agent_type: {
                'rewards': [], 'sar': [], 'splat': [], 'success': [], 
                'jitter': [], 'loss': [], 'scenarios': []
            } for agent_type in self.agent_types},
            'baseline': {agent_type: {
                'rewards': [], 'sar': [], 'splat': [], 'success': [], 
                'jitter': [], 'loss': [], 'scenarios': []
            } for agent_type in self.agent_types}
        }
        
        # âœ… åˆå§‹åŒ–ç¬¬ä¸€ä¸ªåœºæ™¯ - ä½¿ç”¨æ–°é…ç½®ç³»ç»Ÿ
        print(f"ğŸ”§ åˆå§‹åŒ–ç¬¬ä¸€ä¸ªåœºæ™¯...")
        initial_scenario_config = get_scenario_config(1)
        self.env_edge_aware.apply_scenario_config(initial_scenario_config)
        self.env_baseline.apply_scenario_config(initial_scenario_config)
        print(f"ğŸ¯ å¼€å§‹åœºæ™¯: {self.scenario_info[self.current_scenario]['name']}")
        
        for episode in range(1, self.episodes + 1):
            # æ£€æŸ¥å¹¶æ›´æ–°åœºæ™¯
            scenario_changed = self._update_scenario(episode)
            
            if episode % 25 == 0 or scenario_changed:
                current_scenario_info = self.scenario_info.get(self.current_scenario, {})
                scenario_display_name = current_scenario_info.get('name', self.current_scenario)
                print(f"\nğŸ“ Episode {episode}/{self.episodes} - å½“å‰åœºæ™¯: {scenario_display_name}")
            
            # è®­ç»ƒEdge-awareæ™ºèƒ½ä½“
            for agent_type in self.agent_types:
                agent = self.agents_edge_aware[agent_type]
                env = self.env_edge_aware
                episode_stats = self.train_single_episode(agent, env, f"{agent_type}_edge_aware")
                
                # è®°å½•ç»“æœ
                all_results['edge_aware'][agent_type]['rewards'].append(episode_stats['total_reward'])
                all_results['edge_aware'][agent_type]['sar'].append(episode_stats['sar'])
                all_results['edge_aware'][agent_type]['splat'].append(episode_stats['splat'])
                all_results['edge_aware'][agent_type]['success'].append(episode_stats['success'])
                all_results['edge_aware'][agent_type]['jitter'].append(episode_stats['jitter'])
                all_results['edge_aware'][agent_type]['loss'].append(episode_stats['loss'])
                all_results['edge_aware'][agent_type]['scenarios'].append(episode_stats['scenario'])
                
                # è®°å½•æ—¥å¿—
                logger_id = f"{agent_type}_edge_aware"
                if logger_id in self.loggers:
                    self.loggers[logger_id].log_episode(episode, episode_stats)
            
            # è®­ç»ƒBaselineæ™ºèƒ½ä½“
            for agent_type in self.agent_types:
                agent = self.agents_baseline[agent_type]
                env = self.env_baseline
                episode_stats = self.train_single_episode(agent, env, f"{agent_type}_baseline")
                
                # è®°å½•ç»“æœ
                all_results['baseline'][agent_type]['rewards'].append(episode_stats['total_reward'])
                all_results['baseline'][agent_type]['sar'].append(episode_stats['sar'])
                all_results['baseline'][agent_type]['splat'].append(episode_stats['splat'])
                all_results['baseline'][agent_type]['success'].append(episode_stats['success'])
                all_results['baseline'][agent_type]['jitter'].append(episode_stats['jitter'])
                all_results['baseline'][agent_type]['loss'].append(episode_stats['loss'])
                all_results['baseline'][agent_type]['scenarios'].append(episode_stats['scenario'])
                
                # è®°å½•æ—¥å¿—
                logger_id = f"{agent_type}_baseline"
                if logger_id in self.loggers:
                    self.loggers[logger_id].log_episode(episode, episode_stats)
            
            # å®šæœŸæ‰“å°è¿›åº¦
            if episode % 25 == 0:
                self._print_progress(episode, all_results)
            
            # å®šæœŸä¿å­˜æ¨¡å‹ï¼ˆç¡®ä¿åªè°ƒç”¨ä¸€æ¬¡ï¼‰
            if episode % self.save_interval == 0:
                print(f"ä¿å­˜æ£€æŸ¥ç‚¹: episode {episode}")  # âœ… æ·»åŠ è°ƒè¯•ä¿¡æ¯
                self._save_models(episode)
        
        # æœ€ç»ˆåˆ†æ
        self._final_analysis(all_results)
        
        print(f"\nğŸ‰ æ¸è¿›å¼åœºæ™¯è®­ç»ƒå®Œæˆ!")
        return all_results
    
    def _print_progress(self, episode: int, results: Dict):
        """æ‰“å°è®­ç»ƒè¿›åº¦ - åŒ…å«åœºæ™¯ä¿¡æ¯"""
        current_scenario_info = self.scenario_info.get(self.current_scenario, {})
        scenario_display_name = current_scenario_info.get('name', self.current_scenario)
        
        print(f"\nğŸ“Š Episode {episode} æ€§èƒ½ç»Ÿè®¡ (åœºæ™¯: {scenario_display_name}):")
        window = 25
        start_idx = max(0, episode - window)
        
        # è·å–å½“å‰åœºæ™¯çš„é¢„æœŸæ€§èƒ½
        expected_sar = current_scenario_info.get('expected_sar', 'Unknown')
        
        for variant in ['edge_aware', 'baseline']:
            print(f"\n{variant.upper()}:")
            for agent_type in self.agent_types:
                recent_sar = np.mean(results[variant][agent_type]['sar'][start_idx:])
                recent_splat = np.mean([s for s in results[variant][agent_type]['splat'][start_idx:] 
                                      if s != float('inf')])
                recent_reward = np.mean(results[variant][agent_type]['rewards'][start_idx:])
                
                # âœ… ä¿®å¤ç‰ˆï¼šæ›´å‡†ç¡®çš„SARè¯„ä¼°
                if expected_sar == "80-95%":
                    sar_status = "âœ…" if 0.8 <= recent_sar <= 1.0 else "âŒ"
                elif expected_sar == "65-80%":
                    sar_status = "âœ…" if 0.65 <= recent_sar <= 0.8 else ("âš ï¸" if recent_sar > 0.8 else "âŒ")
                elif expected_sar == "50-65%":
                    sar_status = "âœ…" if 0.5 <= recent_sar <= 0.65 else ("âš ï¸" if recent_sar > 0.65 else "âŒ")
                elif expected_sar == "35-50%":
                    sar_status = "âœ…" if 0.35 <= recent_sar <= 0.5 else ("âš ï¸" if recent_sar > 0.5 else "âŒ")
                else:
                    sar_status = "?"
                
                print(f"  {agent_type.upper()}:")
                print(f"    SAR: {recent_sar:.3f} {sar_status} (é¢„æœŸ: {expected_sar})")
                print(f"    SPLat: {recent_splat:.2f}")
                print(f"    Reward: {recent_reward:.1f}")
    
    def _save_models(self, episode: int):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(self.results_dir, "checkpoints", f"episode_{episode}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜Edge-awareæ™ºèƒ½ä½“
        for agent_type, agent in self.agents_edge_aware.items():
            filepath = os.path.join(checkpoint_dir, f"{agent_type}_edge_aware.pth")
            if hasattr(agent, 'save_checkpoint'):
                agent.save_checkpoint(filepath)
                print(f"ğŸ’¾ Agent {agent_type}_edge_aware æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
        
        # ä¿å­˜Baselineæ™ºèƒ½ä½“
        for agent_type, agent in self.agents_baseline.items():
            filepath = os.path.join(checkpoint_dir, f"{agent_type}_baseline.pth")
            if hasattr(agent, 'save_checkpoint'):
                agent.save_checkpoint(filepath)
                print(f"ğŸ’¾ Agent {agent_type}_baseline æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
    
    def _final_analysis(self, results: Dict):
        """æœ€ç»ˆæ€§èƒ½åˆ†æ - æŒ‰åœºæ™¯åˆ†ç»„"""
        print(f"\nğŸ¯ æ¸è¿›å¼åœºæ™¯æœ€ç»ˆæ€§èƒ½åˆ†æ:")
        print(f"{'='*70}")
        
        # æŒ‰åœºæ™¯åˆ†ç»„åˆ†æ
        all_scenario_data = []
        for scenario_name, scenario_config in self.scenario_info.items():
            print(f"\nğŸ“‹ {scenario_config['name']}:")
            print(f"   ç°å®æ€§: {scenario_config['realism']} | é¢„æœŸSAR: {scenario_config['expected_sar']}")
            
            for variant in ['edge_aware', 'baseline']:
                print(f"\n  {variant.upper()}:")
                for agent_type in self.agent_types:
                    # âœ… ä¿®å¤ç‰ˆï¼šæ›´å‡†ç¡®åœ°è·å–åœºæ™¯æ•°æ®
                    scenario_episodes = []
                    for i, ep_scenario in enumerate(results[variant][agent_type]['scenarios']):
                        if ep_scenario == scenario_name:
                            scenario_episodes.append(i)
                    
                    if scenario_episodes:
                        # è®¡ç®—è¯¥åœºæ™¯çš„å¹³å‡æ€§èƒ½ï¼ˆä½¿ç”¨æœ€å10ä¸ªepisodeä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœï¼‰
                        recent_episodes = scenario_episodes[-10:] if len(scenario_episodes) >= 10 else scenario_episodes
                        
                        scenario_sar = np.mean([results[variant][agent_type]['sar'][i] for i in recent_episodes])
                        scenario_splat_values = [results[variant][agent_type]['splat'][i] for i in recent_episodes 
                                               if results[variant][agent_type]['splat'][i] != float('inf')]
                        scenario_splat = np.mean(scenario_splat_values) if scenario_splat_values else float('inf')
                        scenario_reward = np.mean([results[variant][agent_type]['rewards'][i] for i in recent_episodes])
                        
                        print(f"    {agent_type.upper()}:")
                        print(f"      SAR: {scenario_sar:.3f}")
                        print(f"      SPLat: {scenario_splat:.2f}")
                        print(f"      Reward: {scenario_reward:.1f}")
                        
                        all_scenario_data.append({
                            'Scenario': scenario_config['name'],
                            'Variant': variant,
                            'Algorithm': agent_type.upper(),
                            'SAR': scenario_sar,
                            'SPLat': scenario_splat,
                            'Reward': scenario_reward,
                            'Expected_SAR': scenario_config['expected_sar']
                        })
        
        # ä¿å­˜ç»“æœ
        df_scenarios = pd.DataFrame(all_scenario_data)
        df_scenarios.to_csv(os.path.join(self.results_dir, 'scenario_results.csv'), index=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: scenario_results.csv")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='VNFåµŒå…¥å¤šæ™ºèƒ½ä½“æ¸è¿›å¼åœºæ™¯è®­ç»ƒ (é…ç½®åŠ è½½å™¨ä¿®å¤ç‰ˆ)')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    args = parser.parse_args()
    
    trainer = MultiAgentTrainer(config_path=args.config)
    
    if args.episodes:
        trainer.episodes = args.episodes
        trainer.config['train']['episodes'] = args.episodes
    
    results = trainer.train()
    
    print(f"\nâœ… æ¸è¿›å¼åœºæ™¯è®­ç»ƒä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {trainer.results_dir}")
    print(f"ğŸ¯ è®­ç»ƒç»å†äº†4ä¸ªåœºæ™¯ï¼Œä»é«˜ç°å®æ€§åˆ°ç ”ç©¶å¯¼å‘")

if __name__ == "__main__":
    main()