# models/gnn_encoder.py - ä¿®å¤ç‰ˆï¼šè§£å†³ç»´åº¦åŒ¹é…é—®é¢˜

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, Set2Set, global_mean_pool, global_add_pool
from torch_geometric.data import Data, Batch

class GNNEncoder(nn.Module):
    """
    ä¿®å¤ç‰ˆGNNç¼–ç å™¨ - è§£å†³ç»´åº¦åŒ¹é…é—®é¢˜
    """
    
    def __init__(self, node_dim=8, edge_dim=4, hidden_dim=128, output_dim=256, num_layers=3):
        super(GNNEncoder, self).__init__()
        
        self.node_dim = node_dim
        self.edge_dim = edge_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        
        # èŠ‚ç‚¹åµŒå…¥å±‚
        self.node_embedding = nn.Linear(node_dim, hidden_dim)
        
        # è¾¹åµŒå…¥å±‚ï¼šæ”¯æŒå¯å˜ç»´åº¦
        self.edge_embedding = nn.Linear(edge_dim, hidden_dim)
        
        # GATå·ç§¯å±‚
        self.conv_layers = nn.ModuleList()
        for i in range(num_layers):
            self.conv_layers.append(
                GATConv(hidden_dim, hidden_dim, heads=4, concat=False, 
                       edge_dim=hidden_dim, dropout=0.1)
            )
        
        # ğŸ”§ ä¿®å¤ï¼šå…¨å±€æ± åŒ–å±‚ä½¿ç”¨æ­£ç¡®çš„ç»´åº¦
        self.global_pool = Set2Set(hidden_dim, processing_steps=3)
        
        # ğŸ”§ ä¿®å¤ï¼šè¾“å‡ºå±‚è¾“å…¥ç»´åº¦åº”è¯¥æ˜¯ 2 * hidden_dimï¼ˆSet2Setçš„è¾“å‡ºï¼‰
        self.output_layers = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),  # Set2Setè¾“å‡ºæ˜¯2å€hidden_dim
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, output_dim),
            nn.ReLU()
        )
        
        # æ‰¹å½’ä¸€åŒ–
        self.batch_norms = nn.ModuleList([
            nn.BatchNorm1d(hidden_dim) for _ in range(num_layers)
        ])
        
        print(f"âœ… GNNç¼–ç å™¨åˆå§‹åŒ–: èŠ‚ç‚¹{node_dim}ç»´ -> éšè—{hidden_dim}ç»´ -> è¾“å‡º{output_dim}ç»´")
        
    def forward(self, data):
        """å‰å‘ä¼ æ’­"""
        if isinstance(data, list):
            data = Batch.from_data_list(data)
        
        x = data.x.float()
        edge_index = data.edge_index
        edge_attr = data.edge_attr.float() if data.edge_attr is not None else None
        batch = data.batch if hasattr(data, 'batch') else None
        
        # ç»´åº¦éªŒè¯
        if x.size(1) != self.node_dim:
            raise ValueError(f"âŒ èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.node_dim}ç»´ï¼Œå®é™…{x.size(1)}ç»´")
        
        if edge_attr is not None and edge_attr.size(1) != self.edge_dim:
            # æ”¯æŒç»´åº¦è‡ªé€‚åº”
            if self.edge_dim == 4 and edge_attr.size(1) == 2:
                padding = torch.zeros(edge_attr.size(0), 2, device=edge_attr.device)
                edge_attr = torch.cat([edge_attr, padding], dim=1)
                print(f"ğŸ”§ è¾¹ç‰¹å¾è‡ªåŠ¨æ‰©å±•: 2ç»´ -> 4ç»´")
            else:
                raise ValueError(f"âŒ è¾¹ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.edge_dim}ç»´ï¼Œå®é™…{edge_attr.size(1)}ç»´")
        
        # ç‰¹å¾åµŒå…¥
        x = self.node_embedding(x)
        if edge_attr is not None:
            edge_attr = self.edge_embedding(edge_attr)
            edge_attr = F.normalize(edge_attr, p=2, dim=1)
        
        # GNNå·ç§¯
        for i, conv in enumerate(self.conv_layers):
            x_residual = x
            
            if edge_attr is not None:
                x = conv(x, edge_index, edge_attr=edge_attr)
            else:
                x = conv(x, edge_index)
            
            # æ‰¹å½’ä¸€åŒ–
            if batch is not None:
                x = self.batch_norms[i](x)
            else:
                if x.size(0) > 1:
                    x = self.batch_norms[i](x)
            
            x = F.relu(x)
            
            # æ®‹å·®è¿æ¥
            if x_residual.size() == x.size():
                x = x + x_residual
        
        # å…¨å±€æ± åŒ–
        if batch is not None:
            graph_embedding = self.global_pool(x, batch)
        else:
            batch_single = torch.zeros(x.size(0), dtype=torch.long, device=x.device)
            graph_embedding = self.global_pool(x, batch_single)
        
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿graph_embeddingç»´åº¦ä¸º 2*hidden_dim
        print(f"ğŸ” æ± åŒ–åç»´åº¦: {graph_embedding.shape}, æœŸæœ›: [batch_size, {2*self.hidden_dim}]")
        
        # è¾“å‡ºå±‚
        graph_embedding = self.output_layers(graph_embedding)
        graph_embedding = F.normalize(graph_embedding, p=2, dim=1)
        
        return graph_embedding


class EdgeAwareGNNEncoder(GNNEncoder):
    """è¾¹æ„ŸçŸ¥GNNç¼–ç å™¨ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, node_dim=8, edge_dim=4, hidden_dim=128, output_dim=256, num_layers=3):
        super(EdgeAwareGNNEncoder, self).__init__(node_dim, edge_dim, hidden_dim, output_dim, num_layers)
        
        # VNFéœ€æ±‚ç¼–ç å™¨
        self.vnf_requirement_encoder = nn.Linear(6, hidden_dim)
        
        # è¾¹é‡è¦æ€§ç½‘ç»œ
        self.edge_importance_net = nn.Sequential(
            nn.Linear(edge_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # ğŸ”§ ä¿®å¤ï¼šç‰¹å¾èåˆç½‘ç»œè¾“å…¥ç»´åº¦
        self.feature_fusion = nn.Sequential(
            nn.Linear(output_dim + hidden_dim, output_dim),  # output_dim + vnf_embedding_dim
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        print(f"âœ… EdgeAwareç¼–ç å™¨åˆå§‹åŒ–: VNFä¸Šä¸‹æ–‡æ”¯æŒ")
        
    def forward_with_vnf_context(self, data, vnf_context=None):
        """å¸¦VNFä¸Šä¸‹æ–‡çš„å‰å‘ä¼ æ’­"""
        # åŸºç¡€å›¾ç¼–ç 
        graph_embedding = self.forward(data)
        
        # VNFä¸Šä¸‹æ–‡èåˆ
        if vnf_context is not None:
            if isinstance(vnf_context, torch.Tensor):
                vnf_tensor = vnf_context.float()
            else:
                vnf_tensor = torch.tensor(vnf_context, dtype=torch.float32)
            
            if vnf_tensor.dim() == 1:
                vnf_tensor = vnf_tensor.unsqueeze(0)
            
            # VNFä¸Šä¸‹æ–‡ç¼–ç 
            vnf_embedding = self.vnf_requirement_encoder(vnf_tensor)
            
            # ğŸ”§ ä¿®å¤ï¼šç‰¹å¾èåˆç»´åº¦åŒ¹é…
            if graph_embedding.size(0) == vnf_embedding.size(0):
                fused_features = torch.cat([graph_embedding, vnf_embedding], dim=1)
                enhanced_embedding = self.feature_fusion(fused_features)
            else:
                # å¹¿æ’­å¤„ç†
                enhanced_embedding = graph_embedding + 0.3 * vnf_embedding.mean(dim=0, keepdim=True)
            
            return enhanced_embedding
        else:
            return graph_embedding
    
    def compute_edge_attention(self, data):
        """è®¡ç®—è¾¹æ³¨æ„åŠ›æƒé‡"""
        if hasattr(data, 'edge_attr') and data.edge_attr is not None:
            attention_weights = self.edge_importance_net(data.edge_attr.float())
            return attention_weights.squeeze(-1)
        else:
            return torch.ones(data.edge_index.size(1), device=data.edge_index.device)


def create_gnn_encoder(config: dict, mode: str = 'edge_aware'):
    """åˆ›å»ºGNNç¼–ç å™¨çš„å·¥å‚å‡½æ•°"""
    if mode == 'edge_aware':
        gnn_config = config.get('gnn', {}).get('edge_aware', {})
        encoder = EdgeAwareGNNEncoder(
            node_dim=8,
            edge_dim=gnn_config.get('edge_dim', 4),
            hidden_dim=gnn_config.get('hidden_dim', 128),
            output_dim=gnn_config.get('output_dim', 256),
            num_layers=gnn_config.get('layers', 6)
        )
    else:  # baseline
        gnn_config = config.get('gnn', {}).get('baseline', {})
        encoder = GNNEncoder(
            node_dim=8,
            edge_dim=gnn_config.get('edge_dim', 2),
            hidden_dim=gnn_config.get('hidden_dim', 64),
            output_dim=gnn_config.get('output_dim', 256),
            num_layers=gnn_config.get('layers', 4)
        )
    
    print(f"âœ… åˆ›å»º{mode}æ¨¡å¼GNNç¼–ç å™¨")
    return encoder


def test_gnn_encoder_fixed():
    """æµ‹è¯•ä¿®å¤ç‰ˆGNNç¼–ç å™¨"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤ç‰ˆGNNç¼–ç å™¨...")
    print("=" * 50)
    
    # æµ‹è¯•å‚æ•°
    num_nodes = 10
    num_edges = 20
    node_dim = 8
    edge_dim_full = 4
    edge_dim_baseline = 2
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    x = torch.randn(num_nodes, node_dim)
    edge_index = torch.randint(0, num_nodes, (2, num_edges))
    edge_attr_full = torch.randn(num_edges, edge_dim_full)
    edge_attr_baseline = torch.randn(num_edges, edge_dim_baseline)
    
    # æµ‹è¯•1: EdgeAwareæ¨¡å¼
    print("\n1. æµ‹è¯•EdgeAwareæ¨¡å¼:")
    data_full = Data(x=x, edge_index=edge_index, edge_attr=edge_attr_full)
    encoder_full = EdgeAwareGNNEncoder(node_dim=node_dim, edge_dim=edge_dim_full)
    
    with torch.no_grad():
        output_full = encoder_full(data_full)
        print(f"   âœ… è¾“å…¥: {num_nodes}èŠ‚ç‚¹Ã—{node_dim}ç»´, {num_edges}è¾¹Ã—{edge_dim_full}ç»´")
        print(f"   âœ… è¾“å‡º: {output_full.shape}")
    
    # æµ‹è¯•2: Baselineæ¨¡å¼
    print("\n2. æµ‹è¯•Baselineæ¨¡å¼:")
    data_baseline = Data(x=x, edge_index=edge_index, edge_attr=edge_attr_baseline)
    encoder_baseline = GNNEncoder(node_dim=node_dim, edge_dim=edge_dim_baseline)
    
    with torch.no_grad():
        output_baseline = encoder_baseline(data_baseline)
        print(f"   âœ… è¾“å…¥: {num_nodes}èŠ‚ç‚¹Ã—{node_dim}ç»´, {num_edges}è¾¹Ã—{edge_dim_baseline}ç»´")
        print(f"   âœ… è¾“å‡º: {output_baseline.shape}")
    
    # æµ‹è¯•3: VNFä¸Šä¸‹æ–‡
    print("\n3. æµ‹è¯•VNFä¸Šä¸‹æ–‡èåˆ:")
    vnf_context = torch.tensor([0.05, 0.03, 0.04, 0.33, 0.5, 0.5])
    
    with torch.no_grad():
        output_with_context = encoder_full.forward_with_vnf_context(data_full, vnf_context)
        print(f"   âœ… VNFä¸Šä¸‹æ–‡: {vnf_context.shape}")
        print(f"   âœ… èåˆè¾“å‡º: {output_with_context.shape}")
    
    # æµ‹è¯•4: ç»´åº¦ä¸€è‡´æ€§éªŒè¯
    print("\n4. ç»´åº¦ä¸€è‡´æ€§éªŒè¯:")
    assert output_full.shape == output_baseline.shape == output_with_context.shape, "è¾“å‡ºç»´åº¦ä¸ä¸€è‡´!"
    print(f"   âœ… æ‰€æœ‰æ¨¡å¼è¾“å‡ºç»´åº¦ä¸€è‡´: {output_full.shape}")
    
    print(f"\nğŸ‰ GNNç¼–ç å™¨ä¿®å¤ç‰ˆæµ‹è¯•é€šè¿‡!")
    return True

if __name__ == "__main__":
    test_gnn_encoder_fixed()
