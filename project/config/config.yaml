edge_dim: 8

train:
  agent: ddqn                # 可选: ddqn, dqn, ppo, hrl
  lr: 0.001
  batch_size: 32
  gamma: 0.99
  epsilon_start: 1.0
  epsilon_decay: 0.995
  epsilon_min: 0.01
  buffer_size: 10000
  eps_clip: 0.2              # PPO 特有：裁剪范围
  entropy_coef: 0.01         # PPO 特有：熵正则系数
  target_update: 10
  episodes: 300

gnn:
  edge_dim: 8
  hidden_dim: 64
  output_dim: 32

topology:
  type: fat-tree
  k: 4

hrl:
  high_level:
    state_dim: 8
    action_dim: 10
    config:
      gnn:
        edge_dim: 4
        hidden_dim: 64
        output_dim: 32
      train:
        lr: 0.001
        batch_size: 32
        gamma: 0.99
        epsilon_start: 1.0
        epsilon_decay: 0.995
        epsilon_min: 0.01
        buffer_size: 10000
        target_update: 10

  low_level:
    state_dim: 8
    action_dim: 10
    config:
      gnn:
        edge_dim: 8
        hidden_dim: 64
        output_dim: 32
      train:
        lr: 0.001
        batch_size: 32
        gamma: 0.99
        epsilon_start: 1.0
        epsilon_decay: 0.995
        epsilon_min: 0.01
        buffer_size: 10000
        target_update: 10
  
reward:
  alpha: 0.5
  beta: 0.2
  gamma: 0.2
  delta: 0.1
  penalty: 1.0        # 新增惩罚系数
  hop_weight: 0.1     # 新增跳数惩罚