# main_quiet.py - ç²¾ç®€ç‰ˆä¸»è®­ç»ƒè„šæœ¬ï¼Œå¤§å¹…å‡å°‘è¾“å‡º

import os
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, Any
import argparse

from env.vnf_env_multi import MultiVNFEmbeddingEnv
from env.topology_loader import generate_topology
from agents.base_agent import create_agent
from config_loader import get_scenario_config, load_config

class QuietTrainer:
    """
    ç²¾ç®€ç‰ˆå¤šæ™ºèƒ½ä½“è®­ç»ƒå™¨
    
    ç‰¹ç‚¹ï¼š
    - æœ€å°‘è¾“å‡ºä¿¡æ¯
    - åªæ˜¾ç¤ºå…³é”®è¿›åº¦
    - ä¸“æ³¨äºç»“æœ
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = load_config(config_path)
        self.episodes = self.config['train']['episodes']
        self.agent_types = ['ddqn', 'dqn', 'ppo']
        
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # åœºæ™¯ä¿¡æ¯
        self.scenarios = {
            'normal_operation': {'name': 'æ­£å¸¸è¿è¥æœŸ', 'range': [1, 25]},
            'peak_congestion': {'name': 'é«˜å³°æ‹¥å¡æœŸ', 'range': [26, 50]},
            'failure_recovery': {'name': 'æ•…éšœæ¢å¤æœŸ', 'range': [51, 75]},
            'extreme_pressure': {'name': 'æé™å‹åŠ›æœŸ', 'range': [76, 100]}
        }
        
        self._setup_environments()
        self._setup_agents()
        
        print(f"ğŸš€ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ | Episodes: {self.episodes} | æ™ºèƒ½ä½“: {len(self.agent_types)} Ã— 2ç§æ¨¡å¼")
    
    def _setup_environments(self):
        """è®¾ç½®ç¯å¢ƒ"""
        print("ğŸŒ åˆå§‹åŒ–ç¯å¢ƒ...", end="")
        
        full_config = {
            'topology': self.config['topology'],
            'vnf_requirements': self.config['vnf_requirements'],
            'dimensions': self.config['dimensions']
        }
        
        self.graph, self.node_features, self.edge_features = generate_topology(config=full_config)
        
        env_config = {
            'topology': self.config['topology'],
            'vnf_requirements': self.config['vnf_requirements'],
            'reward': self.config['reward'],
            'train': self.config['train'],
            'dimensions': self.config['dimensions']
        }
        
        # Edge-awareç¯å¢ƒ
        self.env_edge_aware = MultiVNFEmbeddingEnv(
            graph=self.graph.copy(),
            node_features=self.node_features.copy(),
            edge_features=self.edge_features.copy(),
            reward_config=self.config['reward'],
            chain_length_range=tuple(self.config['vnf_requirements']['chain_length_range']),
            config=env_config.copy()
        )
        
        # Baselineç¯å¢ƒ
        self.env_baseline = MultiVNFEmbeddingEnv(
            graph=self.graph.copy(),
            node_features=self.node_features.copy(),
            edge_features=self.edge_features.copy(),
            reward_config=self.config['reward'],
            chain_length_range=tuple(self.config['vnf_requirements']['chain_length_range']),
            config=env_config.copy()
        )
        self.env_baseline.is_baseline_mode = True
        
        print(" âœ…")
    
    def _setup_agents(self):
        """è®¾ç½®æ™ºèƒ½ä½“"""
        print("ğŸ¤– åˆå§‹åŒ–æ™ºèƒ½ä½“...", end="")
        
        state_dim = self.config['dimensions']['node_feature_dim']
        action_dim = len(self.graph.nodes())
        
        # Edge-awareæ™ºèƒ½ä½“
        self.agents_edge_aware = {}
        for agent_type in self.agent_types:
            self.agents_edge_aware[agent_type] = create_agent(
                agent_type=agent_type,
                agent_id=f"{agent_type}_edge_aware",
                state_dim=state_dim,
                action_dim=action_dim,
                edge_dim=self.config['gnn']['edge_aware']['edge_dim'],
                config=self.config
            )
        
        # Baselineæ™ºèƒ½ä½“
        self.agents_baseline = {}
        for agent_type in self.agent_types:
            self.agents_baseline[agent_type] = create_agent(
                agent_type=agent_type,
                agent_id=f"{agent_type}_baseline",
                state_dim=state_dim,
                action_dim=action_dim,
                edge_dim=self.config['gnn']['baseline']['edge_dim'],
                config=self.config
            )
        
        print(" âœ…")
    
    def _get_current_scenario(self, episode: int) -> str:
        """è·å–å½“å‰åœºæ™¯"""
        if episode <= 25:
            return 'normal_operation'
        elif episode <= 50:
            return 'peak_congestion'
        elif episode <= 75:
            return 'failure_recovery'
        else:
            return 'extreme_pressure'
    
    def _train_episode(self, agent, env, agent_id: str) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªepisode - é™é»˜ç‰ˆ"""
        state = env.reset()
        total_reward = 0.0
        step_count = 0
        success = False
        info = {}
        
        max_steps = getattr(env, 'max_episode_steps', 20)
        
        while step_count < max_steps:
            valid_actions = env.get_valid_actions()
            if not valid_actions:
                break
            
            action = agent.select_action(state, valid_actions=valid_actions)
            if action not in valid_actions:
                action = np.random.choice(valid_actions)
            
            next_state, reward, done, info = env.step(action)
            agent.store_transition(state, action, reward, next_state, done)
            
            state = next_state
            total_reward += reward
            step_count += 1
            
            # é™é»˜å­¦ä¹ 
            try:
                if hasattr(agent, 'should_update') and agent.should_update():
                    agent.learn()
            except:
                pass
            
            if done:
                success = info.get('success', False)
                break
        
        return {
            'reward': total_reward,
            'success': success,
            'sar': 1.0 if success else 0.0,
            'scenario': getattr(env, 'current_scenario_name', 'unknown')
        }
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯ - ç²¾ç®€ç‰ˆ"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ | ç›®æ ‡: {self.episodes} episodes")
        print("=" * 60)
        
        # ç»“æœå­˜å‚¨
        results = {
            'edge_aware': {agent: {'rewards': [], 'sar': [], 'scenarios': []} 
                          for agent in self.agent_types},
            'baseline': {agent: {'rewards': [], 'sar': [], 'scenarios': []} 
                        for agent in self.agent_types}
        }
        
        current_scenario = None
        
        for episode in range(1, self.episodes + 1):
            # æ£€æŸ¥åœºæ™¯åˆ‡æ¢
            new_scenario = self._get_current_scenario(episode)
            if new_scenario != current_scenario:
                current_scenario = new_scenario
                scenario_info = self.scenarios[current_scenario]
                
                print(f"\nğŸ¯ Episode {episode}: {scenario_info['name']}")
                
                # åº”ç”¨åœºæ™¯é…ç½®
                scenario_config = get_scenario_config(episode)
                self.env_edge_aware.apply_scenario_config(scenario_config)
                self.env_baseline.apply_scenario_config(scenario_config)
            
            # è®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“ - é™é»˜æ¨¡å¼
            for agent_type in self.agent_types:
                # Edge-aware
                stats = self._train_episode(
                    self.agents_edge_aware[agent_type], 
                    self.env_edge_aware, 
                    f"{agent_type}_edge_aware"
                )
                results['edge_aware'][agent_type]['rewards'].append(stats['reward'])
                results['edge_aware'][agent_type]['sar'].append(stats['sar'])
                results['edge_aware'][agent_type]['scenarios'].append(stats['scenario'])
                
                # Baseline
                stats = self._train_episode(
                    self.agents_baseline[agent_type], 
                    self.env_baseline, 
                    f"{agent_type}_baseline"
                )
                results['baseline'][agent_type]['rewards'].append(stats['reward'])
                results['baseline'][agent_type]['sar'].append(stats['sar'])
                results['baseline'][agent_type]['scenarios'].append(stats['scenario'])
            
            # æ¯25ä¸ªepisodeæ˜¾ç¤ºè¿›åº¦
            if episode % 25 == 0:
                self._print_progress(episode, current_scenario, results)
        
        # æœ€ç»ˆåˆ†æ
        self._final_analysis(results)
        return results
    
    def _print_progress(self, episode: int, scenario: str, results: Dict):
        """ç²¾ç®€è¿›åº¦æ˜¾ç¤º"""
        scenario_name = self.scenarios[scenario]['name']
        window = 25
        start_idx = max(0, episode - window)
        
        print(f"\nğŸ“Š Episode {episode} | {scenario_name}")
        print("-" * 40)
        
        for variant in ['edge_aware', 'baseline']:
            avg_sar = np.mean([
                np.mean(results[variant][agent]['sar'][start_idx:episode]) 
                for agent in self.agent_types
            ])
            avg_reward = np.mean([
                np.mean(results[variant][agent]['rewards'][start_idx:episode]) 
                for agent in self.agent_types
            ])
            
            print(f"{variant.upper():11} | SAR: {avg_sar:.3f} | Reward: {avg_reward:.1f}")
    
    def _final_analysis(self, results: Dict):
        """æœ€ç»ˆç»“æœåˆ†æ"""
        print(f"\nğŸ¯ æœ€ç»ˆç»“æœæ±‡æ€»")
        print("=" * 60)
        
        summary_data = []
        
        for scenario_key, scenario_info in self.scenarios.items():
            print(f"\nğŸ“‹ {scenario_info['name']}:")
            
            # è®¡ç®—è¯¥åœºæ™¯çš„episodeèŒƒå›´
            start_ep, end_ep = scenario_info['range']
            episode_indices = list(range(start_ep-1, end_ep))  # è½¬æ¢ä¸º0-basedç´¢å¼•
            
            for variant in ['edge_aware', 'baseline']:
                # è·å–è¯¥åœºæ™¯çš„å¹³å‡æ€§èƒ½
                scenario_sar = []
                scenario_rewards = []
                
                for agent_type in self.agent_types:
                    agent_sar = [results[variant][agent_type]['sar'][i] for i in episode_indices 
                               if i < len(results[variant][agent_type]['sar'])]
                    agent_rewards = [results[variant][agent_type]['rewards'][i] for i in episode_indices 
                                   if i < len(results[variant][agent_type]['rewards'])]
                    
                    if agent_sar:  # å¦‚æœæœ‰æ•°æ®
                        scenario_sar.extend(agent_sar)
                        scenario_rewards.extend(agent_rewards)
                
                if scenario_sar:
                    avg_sar = np.mean(scenario_sar)
                    avg_reward = np.mean(scenario_rewards)
                    
                    print(f"  {variant.upper():11} | SAR: {avg_sar:.3f} | Reward: {avg_reward:.1f}")
                    
                    summary_data.append({
                        'Scenario': scenario_info['name'],
                        'Variant': variant.upper(),
                        'SAR': avg_sar,
                        'Reward': avg_reward
                    })
        
        # ä¿å­˜ç»“æœ
        df = pd.DataFrame(summary_data)
        df.to_csv(os.path.join(self.results_dir, 'summary_results.csv'), index=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {os.path.join(self.results_dir, 'summary_results.csv')}")
        
        # Edge-aware vs Baseline æ¯”è¾ƒ
        print(f"\nğŸ†š Edge-aware vs Baseline æ¯”è¾ƒ:")
        for scenario_key, scenario_info in self.scenarios.items():
            edge_data = [d for d in summary_data if d['Scenario'] == scenario_info['name'] and d['Variant'] == 'EDGE_AWARE']
            baseline_data = [d for d in summary_data if d['Scenario'] == scenario_info['name'] and d['Variant'] == 'BASELINE']
            
            if edge_data and baseline_data:
                edge_sar = edge_data[0]['SAR']
                baseline_sar = baseline_data[0]['SAR']
                improvement = ((edge_sar - baseline_sar) / max(baseline_sar, 0.001)) * 100
                
                status = "ğŸŸ¢" if improvement > 5 else "ğŸŸ¡" if improvement > -5 else "ğŸ”´"
                print(f"  {scenario_info['name']:8} | {status} {improvement:+5.1f}% SARæå‡")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='VNFåµŒå…¥ç²¾ç®€ç‰ˆè®­ç»ƒ')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶')
    parser.add_argument('--episodes', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    args = parser.parse_args()
    
    trainer = QuietTrainer(config_path=args.config)
    
    if args.episodes:
        trainer.episodes = args.episodes
    
    results = trainer.train()
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {trainer.results_dir}")

if __name__ == "__main__":
    main()