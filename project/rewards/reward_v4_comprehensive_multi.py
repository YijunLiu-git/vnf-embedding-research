# rewards/enhanced_edge_aware_reward.py - å¢å¼ºçš„Edge-Awareå¥–åŠ±ç³»ç»Ÿ

import numpy as np
import torch
from typing import Dict, List, Any, Tuple

class EdgeAwareRewardCalculator:
    """
    å¢å¼ºçš„Edge-Awareå¥–åŠ±è®¡ç®—å™¨
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. è·¯å¾„è´¨é‡æ„ŸçŸ¥å¥–åŠ±
    2. ç½‘ç»œæ•ˆç‡ä¼˜åŒ–å¥–åŠ±
    3. Edgeç‰¹å¾åˆ©ç”¨ç‡è¯„ä¼°
    4. åŠ¨æ€é€‚åº”æ€§å¥–åŠ±
    5. å¤šç›®æ ‡å¹³è¡¡æœºåˆ¶
    """
    
    def __init__(self, reward_config: Dict[str, Any]):
        self.config = reward_config
        
        # æ ¸å¿ƒæƒé‡é…ç½®
        self.sar_weight = reward_config.get("sar_weight", 0.4)
        self.latency_weight = reward_config.get("latency_weight", 0.25)
        self.efficiency_weight = reward_config.get("efficiency_weight", 0.15)
        self.quality_weight = reward_config.get("quality_weight", 0.1)
        self.edge_aware_weight = reward_config.get("edge_aware_weight", 0.1)  # æ–°å¢
        
        # å¥–åŠ±é˜ˆå€¼
        self.excellent_sar = reward_config.get("excellent_sar", 0.95)
        self.good_sar = reward_config.get("good_sar", 0.9)
        self.acceptable_sar = reward_config.get("acceptable_sar", 0.8)
        
        self.excellent_latency = reward_config.get("excellent_latency", 30.0)
        self.good_latency = reward_config.get("good_latency", 50.0)
        self.sla_latency = reward_config.get("sla_latency", 100.0)
        
        # Edge-Awareç‰¹å®šé˜ˆå€¼
        self.quality_threshold = reward_config.get("quality_threshold", 0.8)
        self.efficiency_threshold = reward_config.get("efficiency_threshold", 0.7)
        self.path_diversity_threshold = reward_config.get("path_diversity_threshold", 0.6)
        
        print(f"ğŸ¯ å¢å¼ºEdge-Awareå¥–åŠ±ç³»ç»Ÿåˆå§‹åŒ–")
        print(f"   æƒé‡é…ç½®: SAR({self.sar_weight}) + å»¶è¿Ÿ({self.latency_weight}) + ")
        print(f"            æ•ˆç‡({self.efficiency_weight}) + è´¨é‡({self.quality_weight}) + Edge({self.edge_aware_weight})")
    
    def compute_enhanced_reward(self, info: Dict[str, Any], is_edge_aware: bool = True) -> Dict[str, Any]:
        """
        è®¡ç®—å¢å¼ºçš„Edge-Awareå¥–åŠ±
        
        Args:
            info: ç¯å¢ƒä¿¡æ¯å­—å…¸
            is_edge_aware: æ˜¯å¦ä¸ºEdge-Awareç‰ˆæœ¬
            
        Returns:
            reward_breakdown: è¯¦ç»†çš„å¥–åŠ±åˆ†è§£
        """
        # åŸºç¡€ä¿¡æ¯æå–
        total_vnfs = info.get("total_vnfs", 0)
        deployed_vnfs = info.get("deployed_vnfs", 0)
        
        if total_vnfs == 0:
            return self._get_default_reward("no_vnfs")
        
        # è®¡ç®—åŸºç¡€SAR
        sar = deployed_vnfs / total_vnfs
        
        # åˆå§‹åŒ–å¥–åŠ±åˆ†è§£
        reward_breakdown = {
            "total_reward": 0.0,
            "sar_reward": 0.0,
            "latency_reward": 0.0,
            "efficiency_reward": 0.0,
            "quality_reward": 0.0,
            "edge_aware_bonus": 0.0,
            "path_quality_bonus": 0.0,
            "network_efficiency_bonus": 0.0,
            "adaptive_bonus": 0.0,
            "details": {}
        }
        
        # 1. åŸºç¡€SARå¥–åŠ±
        sar_reward = self._compute_sar_reward(sar)
        reward_breakdown["sar_reward"] = sar_reward
        
        # 2. å¦‚æœæœ‰è·¯å¾„ä¿¡æ¯ï¼Œè®¡ç®—è¯¦ç»†å¥–åŠ±
        if "paths" in info and info["paths"]:
            paths_info = self._extract_enhanced_path_metrics(info["paths"])
            
            # å»¶è¿Ÿå¥–åŠ±
            latency_reward = self._compute_latency_reward(paths_info["avg_delay"])
            reward_breakdown["latency_reward"] = latency_reward
            
            # æ•ˆç‡å¥–åŠ±
            efficiency_reward = self._compute_efficiency_reward(info, paths_info)
            reward_breakdown["efficiency_reward"] = efficiency_reward
            
            # è´¨é‡å¥–åŠ±
            quality_reward = self._compute_quality_reward(paths_info, is_edge_aware)
            reward_breakdown["quality_reward"] = quality_reward
            
            # 3. Edge-Awareç‰¹æœ‰å¥–åŠ±
            if is_edge_aware:
                edge_aware_bonus = self._compute_edge_aware_bonus(info, paths_info)
                reward_breakdown["edge_aware_bonus"] = edge_aware_bonus
                
                # è·¯å¾„è´¨é‡å¥–åŠ±
                path_quality_bonus = self._compute_path_quality_bonus(paths_info)
                reward_breakdown["path_quality_bonus"] = path_quality_bonus
                
                # ç½‘ç»œæ•ˆç‡å¥–åŠ±
                network_efficiency_bonus = self._compute_network_efficiency_bonus(info)
                reward_breakdown["network_efficiency_bonus"] = network_efficiency_bonus
                
                # è‡ªé€‚åº”æ€§å¥–åŠ±
                adaptive_bonus = self._compute_adaptive_bonus(info, paths_info)
                reward_breakdown["adaptive_bonus"] = adaptive_bonus
        
        # 4. è®¡ç®—æ€»å¥–åŠ±
        total_reward = (
            reward_breakdown["sar_reward"] * self.sar_weight +
            reward_breakdown["latency_reward"] * self.latency_weight +
            reward_breakdown["efficiency_reward"] * self.efficiency_weight +
            reward_breakdown["quality_reward"] * self.quality_weight +
            (reward_breakdown["edge_aware_bonus"] + 
             reward_breakdown["path_quality_bonus"] + 
             reward_breakdown["network_efficiency_bonus"] + 
             reward_breakdown["adaptive_bonus"]) * self.edge_aware_weight
        )
        
        reward_breakdown["total_reward"] = total_reward
        
        # 5. æ·»åŠ å®Œæˆå¥–åŠ±
        if sar >= 0.95:
            completion_bonus = self.config.get("completion_bonus", 20.0)
            reward_breakdown["total_reward"] += completion_bonus
            reward_breakdown["details"]["completion_bonus"] = completion_bonus
        
        # 6. è®°å½•è¯¦ç»†ä¿¡æ¯
        reward_breakdown["details"].update({
            "sar": sar,
            "is_edge_aware": is_edge_aware,
            "avg_path_quality": paths_info.get("avg_quality_score", 0.0) if "paths" in info else 0.0,
            "network_efficiency": info.get("network_efficiency", 0.0),
            "congestion_level": info.get("congestion_level", 0.0)
        })
        
        return reward_breakdown
    
    def _compute_sar_reward(self, sar: float) -> float:
        """è®¡ç®—SARå¥–åŠ±"""
        if sar >= self.excellent_sar:
            return 100.0
        elif sar >= self.good_sar:
            return 80.0 + (sar - self.good_sar) / (self.excellent_sar - self.good_sar) * 20.0
        elif sar >= self.acceptable_sar:
            return 60.0 + (sar - self.acceptable_sar) / (self.good_sar - self.acceptable_sar) * 20.0
        else:
            return max(0.0, sar * 60.0)
    
    def _compute_latency_reward(self, avg_delay: float) -> float:
        """è®¡ç®—å»¶è¿Ÿå¥–åŠ±"""
        if avg_delay <= self.excellent_latency:
            return 100.0
        elif avg_delay <= self.good_latency:
            return 80.0 - (avg_delay - self.excellent_latency) / (self.good_latency - self.excellent_latency) * 20.0
        elif avg_delay <= self.sla_latency:
            return 40.0 - (avg_delay - self.good_latency) / (self.sla_latency - self.good_latency) * 40.0
        else:
            return max(0.0, 40.0 - (avg_delay - self.sla_latency) / self.sla_latency * 40.0)
    
    def _compute_efficiency_reward(self, info: Dict, paths_info: Dict) -> float:
        """è®¡ç®—æ•ˆç‡å¥–åŠ±"""
        # èµ„æºåˆ©ç”¨ç‡æ•ˆç‡
        resource_util = info.get("resource_utilization", 0.5)
        optimal_util = 0.7  # æœ€ä¼˜åˆ©ç”¨ç‡
        util_efficiency = 1.0 - abs(resource_util - optimal_util) / optimal_util
        
        # è·¯å¾„è·³æ•°æ•ˆç‡
        avg_hops = paths_info.get("avg_hops", 5.0)
        max_reasonable_hops = 6.0
        hop_efficiency = max(0.0, (max_reasonable_hops - avg_hops) / max_reasonable_hops)
        
        # ç»¼åˆæ•ˆç‡è¯„åˆ†
        efficiency_score = (util_efficiency * 0.6 + hop_efficiency * 0.4) * 100.0
        
        return max(0.0, efficiency_score)
    
    def _compute_quality_reward(self, paths_info: Dict, is_edge_aware: bool) -> float:
        """è®¡ç®—è´¨é‡å¥–åŠ±"""
        avg_jitter = paths_info.get("avg_jitter", 0.0)
        avg_loss = paths_info.get("avg_loss", 0.0)
        
        # è´¨é‡è¯„åˆ†
        jitter_score = max(0.0, 1.0 - avg_jitter / 0.01) if avg_jitter > 0 else 1.0
        loss_score = max(0.0, 1.0 - avg_loss / 0.01) if avg_loss > 0 else 1.0
        
        base_quality_score = (jitter_score + loss_score) / 2 * 100.0
        
        # Edge-Awareè´¨é‡æå‡
        if is_edge_aware:
            quality_multiplier = 1.3  # Edge-Awareç‰ˆæœ¬è´¨é‡å¥–åŠ±æå‡30%
            return base_quality_score * quality_multiplier
        else:
            return base_quality_score
    
    def _compute_edge_aware_bonus(self, info: Dict, paths_info: Dict) -> float:
        """è®¡ç®—Edge-Awareç‰¹æœ‰å¥–åŠ±"""
        bonus = 0.0
        
        # 1. è¾¹ç‰¹å¾åˆ©ç”¨å¥–åŠ±
        if "edge_importance_map" in info:
            edge_utilization = self._calculate_edge_utilization(info["edge_importance_map"])
            bonus += edge_utilization * 30.0
        
        # 2. ç½‘ç»œçŠ¶æ€æ„ŸçŸ¥å¥–åŠ±
        if "network_state_vector" in info:
            state_awareness = self._calculate_state_awareness(info["network_state_vector"])
            bonus += state_awareness * 25.0
        
        # 3. VNFé€‚åº”æ€§å¥–åŠ±
        if "vnf_adaptation_score" in info:
            adaptation_score = info["vnf_adaptation_score"]
            bonus += adaptation_score * 20.0
        
        return bonus
    
    def _compute_path_quality_bonus(self, paths_info: Dict) -> float:
        """è®¡ç®—è·¯å¾„è´¨é‡å¥–åŠ±"""
        avg_quality = paths_info.get("avg_quality_score", 0.0)
        
        if avg_quality >= self.quality_threshold:
            # é«˜è´¨é‡è·¯å¾„å¥–åŠ±
            quality_bonus = (avg_quality - self.quality_threshold) / (1.0 - self.quality_threshold) * 40.0
            
            # è·¯å¾„å¤šæ ·æ€§å¥–åŠ±
            diversity_score = paths_info.get("path_diversity", 0.0)
            if diversity_score >= self.path_diversity_threshold:
                quality_bonus += 20.0
            
            return quality_bonus
        else:
            return 0.0
    
    def _compute_network_efficiency_bonus(self, info: Dict) -> float:
        """è®¡ç®—ç½‘ç»œæ•ˆç‡å¥–åŠ±"""
        network_efficiency = info.get("network_efficiency", 0.0)
        congestion_level = info.get("congestion_level", 1.0)
        
        if network_efficiency >= self.efficiency_threshold:
            efficiency_bonus = (network_efficiency - self.efficiency_threshold) / (1.0 - self.efficiency_threshold) * 30.0
            
            # æ‹¥å¡é¿å…å¥–åŠ±
            congestion_bonus = (1.0 - congestion_level) * 20.0
            
            return efficiency_bonus + congestion_bonus
        else:
            return 0.0
    
    def _compute_adaptive_bonus(self, info: Dict, paths_info: Dict) -> float:
        """è®¡ç®—è‡ªé€‚åº”æ€§å¥–åŠ±"""
        bonus = 0.0
        
        # å‹åŠ›ç­‰çº§é€‚åº”
        pressure_level = info.get("pressure_level", "medium")
        if pressure_level in ["high", "extreme"]:
            # é«˜å‹åŠ›ä¸‹çš„ä¼˜ç§€è¡¨ç°
            sar = info.get("deployed_vnfs", 0) / info.get("total_vnfs", 1)
            if sar >= 0.7:
                bonus += 35.0  # é«˜å‹åŠ›é«˜SARå¥–åŠ±
            
            # è´¨é‡ä¿æŒå¥–åŠ±
            avg_quality = paths_info.get("avg_quality_score", 0.0)
            if avg_quality >= 0.6:
                bonus += 25.0  # é«˜å‹åŠ›è´¨é‡ä¿æŒå¥–åŠ±
        
        # åœºæ™¯è½¬æ¢é€‚åº”å¥–åŠ±
        scenario_name = info.get("scenario_name", "")
        if "extreme" in scenario_name or "failure" in scenario_name:
            # å›°éš¾åœºæ™¯ä¸‹çš„é¢å¤–å¥–åŠ±
            bonus += 20.0
        
        return bonus
    
    def _extract_enhanced_path_metrics(self, paths: List[Dict]) -> Dict[str, float]:
        """æå–å¢å¼ºçš„è·¯å¾„æŒ‡æ ‡"""
        if not paths:
            return self._get_default_path_metrics()
        
        total_delay = 0.0
        total_jitter = 0.0
        total_loss = 0.0
        total_hops = 0
        total_quality = 0.0
        min_bandwidth = float('inf')
        
        quality_scores = []
        hop_counts = []
        
        for path in paths:
            # åŸºç¡€æŒ‡æ ‡
            delay = path.get("delay", 0.0)
            jitter = path.get("jitter", 0.0)
            loss = path.get("loss", 0.0)
            hops = path.get("hops", 0)
            bandwidth = path.get("bandwidth", 0.0)
            
            total_delay += delay
            total_jitter += jitter
            total_loss += loss
            total_hops += hops
            min_bandwidth = min(min_bandwidth, bandwidth) if bandwidth > 0 else min_bandwidth
            
            # è®¡ç®—è·¯å¾„è´¨é‡è¯„åˆ†
            quality_score = self._calculate_path_quality_score(path)
            quality_scores.append(quality_score)
            total_quality += quality_score
            
            hop_counts.append(hops)
        
        num_paths = len(paths)
        
        # è®¡ç®—è·¯å¾„å¤šæ ·æ€§
        path_diversity = np.std(hop_counts) / max(np.mean(hop_counts), 1.0) if hop_counts else 0.0
        
        return {
            "avg_delay": total_delay / num_paths,
            "avg_jitter": total_jitter / num_paths,
            "avg_loss": total_loss / num_paths,
            "avg_hops": total_hops / num_paths,
            "min_bandwidth": min_bandwidth if min_bandwidth != float('inf') else 0.0,
            "avg_quality_score": total_quality / num_paths,
            "path_diversity": min(path_diversity, 1.0),
            "num_paths": num_paths
        }
    
    def _calculate_path_quality_score(self, path: Dict) -> float:
        """è®¡ç®—å•æ¡è·¯å¾„çš„è´¨é‡è¯„åˆ†"""
        delay = path.get("delay", 0.0)
        jitter = path.get("jitter", 0.0)
        loss = path.get("loss", 0.0)
        bandwidth = path.get("bandwidth", 0.0)
        reliability = path.get("reliability", 1.0)
        
        # å½’ä¸€åŒ–å„ä¸ªæŒ‡æ ‡ (0-1, è¶Šé«˜è¶Šå¥½)
        delay_score = max(0.0, 1.0 - delay / 100.0)  # å‡è®¾100msä¸ºæœ€å·®å»¶è¿Ÿ
        jitter_score = max(0.0, 1.0 - jitter / 5.0)   # å‡è®¾5msä¸ºæœ€å·®æŠ–åŠ¨
        loss_score = max(0.0, 1.0 - loss / 0.05)      # å‡è®¾5%ä¸ºæœ€å·®ä¸¢åŒ…ç‡
        bandwidth_score = min(1.0, bandwidth / 100.0)  # å‡è®¾100Mbpsä¸ºæ»¡åˆ†å¸¦å®½
        
        # åŠ æƒç»¼åˆè¯„åˆ†
        quality_score = (
            delay_score * 0.3 +
            jitter_score * 0.2 +
            loss_score * 0.2 +
            bandwidth_score * 0.2 +
            reliability * 0.1
        )
        
        return quality_score
    
    def _calculate_edge_utilization(self, edge_importance_map: Dict) -> float:
        """è®¡ç®—è¾¹ç‰¹å¾åˆ©ç”¨ç‡"""
        if not edge_importance_map:
            return 0.0
        
        attention_weights = [info.get("attention_weight", 0.0) for info in edge_importance_map.values()]
        importance_levels = [info.get("importance_level", 0) for info in edge_importance_map.values()]
        
        # å¹³å‡æ³¨æ„åŠ›æƒé‡
        avg_attention = np.mean(attention_weights) if attention_weights else 0.0
        
        # é«˜é‡è¦æ€§è¾¹çš„æ¯”ä¾‹
        high_importance_ratio = np.mean([1 if level == 2 else 0 for level in importance_levels])
        
        # ç»¼åˆåˆ©ç”¨ç‡
        utilization = (avg_attention * 0.6 + high_importance_ratio * 0.4)
        
        return utilization
    
    def _calculate_state_awareness(self, network_state_vector) -> float:
        """è®¡ç®—ç½‘ç»œçŠ¶æ€æ„ŸçŸ¥èƒ½åŠ›"""
        if network_state_vector is None:
            return 0.0
        
        if isinstance(network_state_vector, (list, np.ndarray)):
            state_vector = np.array(network_state_vector)
        elif hasattr(network_state_vector, 'cpu'):
            state_vector = network_state_vector.cpu().numpy()
        else:
            return 0.0
        
        # çŠ¶æ€å‘é‡çš„ä¿¡æ¯ç†µä½œä¸ºæ„ŸçŸ¥èƒ½åŠ›æŒ‡æ ‡
        # ä¿¡æ¯ç†µè¶Šé«˜ï¼Œè¡¨ç¤ºçŠ¶æ€æ„ŸçŸ¥è¶Šä¸°å¯Œ
        state_normalized = (state_vector - state_vector.min()) / (state_vector.max() - state_vector.min() + 1e-8)
        
        # è®¡ç®—ç®€åŒ–çš„ä¿¡æ¯ç†µ
        entropy = -np.sum(state_normalized * np.log(state_normalized + 1e-8))
        
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        max_entropy = np.log(len(state_vector))
        awareness_score = entropy / max_entropy if max_entropy > 0 else 0.0
        
        return awareness_score
    
    def _get_default_path_metrics(self) -> Dict[str, float]:
        """è·å–é»˜è®¤è·¯å¾„æŒ‡æ ‡"""
        return {
            "avg_delay": float('inf'),
            "avg_jitter": 0.0,
            "avg_loss": 0.0,
            "avg_hops": float('inf'),
            "min_bandwidth": 0.0,
            "avg_quality_score": 0.0,
            "path_diversity": 0.0,
            "num_paths": 0
        }
    
    def _get_default_reward(self, reason: str) -> Dict[str, Any]:
        """è·å–é»˜è®¤å¥–åŠ±"""
        penalty = self.config.get("penalty", 20.0)
        
        return {
            "total_reward": -penalty,
            "sar_reward": 0.0,
            "latency_reward": 0.0,
            "efficiency_reward": 0.0,
            "quality_reward": 0.0,
            "edge_aware_bonus": 0.0,
            "path_quality_bonus": 0.0,
            "network_efficiency_bonus": 0.0,
            "adaptive_bonus": 0.0,
            "details": {"reason": reason}
        }


def compute_enhanced_edge_aware_reward(info: Dict[str, Any], reward_config: Dict[str, Any]) -> float:
    """
    å¢å¼ºEdge-Awareå¥–åŠ±è®¡ç®—çš„ä¸»æ¥å£å‡½æ•°
    
    Args:
        info: ç¯å¢ƒä¿¡æ¯å­—å…¸
        reward_config: å¥–åŠ±é…ç½®
        
    Returns:
        total_reward: æ€»å¥–åŠ±å€¼
    """
    # æ£€æµ‹æ˜¯å¦ä¸ºEdge-Awareç‰ˆæœ¬
    is_edge_aware = info.get("is_edge_aware", False)
    
    # åˆ›å»ºå¥–åŠ±è®¡ç®—å™¨
    calculator = EdgeAwareRewardCalculator(reward_config)
    
    # è®¡ç®—å¥–åŠ±
    reward_breakdown = calculator.compute_enhanced_reward(info, is_edge_aware)
    
    # è¾“å‡ºè¯¦ç»†ä¿¡æ¯
    if info.get("verbose", False):
        print(f"\nğŸ¯ å¢å¼ºEdge-Awareå¥–åŠ±åˆ†è§£:")
        print(f"   ç‰ˆæœ¬: {'Edge-Aware' if is_edge_aware else 'Baseline'}")
        print(f"   SARå¥–åŠ±: {reward_breakdown['sar_reward']:.2f}")
        print(f"   å»¶è¿Ÿå¥–åŠ±: {reward_breakdown['latency_reward']:.2f}")
        print(f"   æ•ˆç‡å¥–åŠ±: {reward_breakdown['efficiency_reward']:.2f}")
        print(f"   è´¨é‡å¥–åŠ±: {reward_breakdown['quality_reward']:.2f}")
        
        if is_edge_aware:
            print(f"   Edge-Awareå¥–åŠ±: {reward_breakdown['edge_aware_bonus']:.2f}")
            print(f"   è·¯å¾„è´¨é‡å¥–åŠ±: {reward_breakdown['path_quality_bonus']:.2f}")
            print(f"   ç½‘ç»œæ•ˆç‡å¥–åŠ±: {reward_breakdown['network_efficiency_bonus']:.2f}")
            print(f"   è‡ªé€‚åº”å¥–åŠ±: {reward_breakdown['adaptive_bonus']:.2f}")
        
        print(f"   æ€»å¥–åŠ±: {reward_breakdown['total_reward']:.2f}")
    
    return reward_breakdown["total_reward"]


# å…¼å®¹æ€§æ¥å£
def compute_reward(info: Dict[str, Any], reward_config: Dict[str, Any]) -> float:
    """å…¼å®¹åŸæœ‰å¥–åŠ±æ¥å£"""
    return compute_enhanced_edge_aware_reward(info, reward_config)


# æµ‹è¯•å‡½æ•°
def test_enhanced_reward_system():
    """æµ‹è¯•å¢å¼ºå¥–åŠ±ç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºEdge-Awareå¥–åŠ±ç³»ç»Ÿ...")
    print("=" * 60)
    
    # æµ‹è¯•é…ç½®
    reward_config = {
        "sar_weight": 0.4,
        "latency_weight": 0.25,
        "efficiency_weight": 0.15,
        "quality_weight": 0.1,
        "edge_aware_weight": 0.1,
        "excellent_sar": 0.95,
        "good_sar": 0.9,
        "acceptable_sar": 0.8,
        "excellent_latency": 30.0,
        "good_latency": 50.0,
        "sla_latency": 100.0,
        "quality_threshold": 0.8,
        "efficiency_threshold": 0.7,
        "completion_bonus": 20.0,
        "penalty": 20.0
    }
    
    # æµ‹è¯•æ•°æ®1: Edge-Awareé«˜æ€§èƒ½åœºæ™¯
    edge_aware_info = {
        "total_vnfs": 5,
        "deployed_vnfs": 5,
        "is_edge_aware": True,
        "paths": [
            {"delay": 25.0, "jitter": 0.5, "loss": 0.001, "bandwidth": 80.0, "hops": 2, "reliability": 0.99},
            {"delay": 30.0, "jitter": 0.8, "loss": 0.002, "bandwidth": 70.0, "hops": 3, "reliability": 0.98},
            {"delay": 28.0, "jitter": 0.6, "loss": 0.001, "bandwidth": 85.0, "hops": 2, "reliability": 0.99},
            {"delay": 32.0, "jitter": 0.7, "loss": 0.002, "bandwidth": 75.0, "hops": 3, "reliability": 0.97}
        ],
        "resource_utilization": 0.65,
        "network_efficiency": 0.85,
        "congestion_level": 0.2,
        "edge_importance_map": {
            (0, 1): {"attention_weight": 0.8, "importance_level": 2},
            (1, 2): {"attention_weight": 0.9, "importance_level": 2},
            (2, 3): {"attention_weight": 0.7, "importance_level": 1}
        },
        "network_state_vector": [0.8, 0.3, 0.2, 0.9, 0.7, 0.4, 0.1, 0.8],
        "vnf_adaptation_score": 0.85,
        "verbose": True
    }
    
    # æµ‹è¯•æ•°æ®2: Baselineä¸­ç­‰æ€§èƒ½åœºæ™¯
    baseline_info = {
        "total_vnfs": 5,
        "deployed_vnfs": 4,
        "is_edge_aware": False,
        "paths": [
            {"delay": 45.0, "jitter": 1.2, "loss": 0.005, "bandwidth": 60.0, "hops": 4, "reliability": 0.95},
            {"delay": 50.0, "jitter": 1.5, "loss": 0.008, "bandwidth": 55.0, "hops": 5, "reliability": 0.94},
            {"delay": 48.0, "jitter": 1.3, "loss": 0.006, "bandwidth": 58.0, "hops": 4, "reliability": 0.95}
        ],
        "resource_utilization": 0.55,
        "verbose": True
    }
    
    # åˆ›å»ºå¥–åŠ±è®¡ç®—å™¨
    calculator = EdgeAwareRewardCalculator(reward_config)
    
    # æµ‹è¯•Edge-Awareå¥–åŠ±
    print("1. Edge-Awareé«˜æ€§èƒ½åœºæ™¯æµ‹è¯•:")
    edge_reward = calculator.compute_enhanced_reward(edge_aware_info, True)
    
    # æµ‹è¯•Baselineå¥–åŠ±
    print("\n2. Baselineä¸­ç­‰æ€§èƒ½åœºæ™¯æµ‹è¯•:")
    baseline_reward = calculator.compute_enhanced_reward(baseline_info, False)
    
    # å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“Š å¯¹æ¯”åˆ†æ:")
    print(f"   Edge-Awareæ€»å¥–åŠ±: {edge_reward['total_reward']:.2f}")
    print(f"   Baselineæ€»å¥–åŠ±: {baseline_reward['total_reward']:.2f}")
    print(f"   Edge-Awareä¼˜åŠ¿: {edge_reward['total_reward'] - baseline_reward['total_reward']:.2f}")
    
    improvement = ((edge_reward['total_reward'] - baseline_reward['total_reward']) / 
                   abs(baseline_reward['total_reward']) * 100) if baseline_reward['total_reward'] != 0 else 0
    print(f"   æ€§èƒ½æå‡: {improvement:.1f}%")
    
    # æµ‹è¯•æé™åœºæ™¯
    print(f"\n3. æé™å‹åŠ›åœºæ™¯æµ‹è¯•:")
    extreme_info = edge_aware_info.copy()
    extreme_info.update({
        "deployed_vnfs": 3,  # æ›´ä½çš„SAR
        "pressure_level": "extreme",
        "scenario_name": "extreme_pressure"
    })
    
    extreme_reward = calculator.compute_enhanced_reward(extreme_info, True)
    print(f"   æé™åœºæ™¯Edge-Awareå¥–åŠ±: {extreme_reward['total_reward']:.2f}")
    
    print(f"\nâœ… å¢å¼ºEdge-Awareå¥–åŠ±ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")
    print(f"æ ¸å¿ƒéªŒè¯:")
    print(f"  âœ… Edge-Awareå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿")
    print(f"  âœ… å¤šç»´åº¦å¥–åŠ±æœºåˆ¶æœ‰æ•ˆ")
    print(f"  âœ… è‡ªé€‚åº”å‹åŠ›åœºæ™¯")
    print(f"  âœ… è·¯å¾„è´¨é‡æ„ŸçŸ¥")


if __name__ == "__main__":
    test_enhanced_reward_system()