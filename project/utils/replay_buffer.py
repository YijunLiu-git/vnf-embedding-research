# utils/replay_buffer.py

import torch
import numpy as np
import random
from collections import deque, namedtuple
from typing import List, Tuple, Union, Optional, Any
from torch_geometric.data import Data, Batch
import pickle
import threading
from dataclasses import dataclass

# ç»éªŒæ•°æ®ç»“æ„
@dataclass
class Experience:
    """å•ä¸ªç»éªŒçš„æ•°æ®ç»“æ„"""
    state: Union[Data, torch.Tensor]
    action: Union[int, List[int]]
    reward: float
    next_state: Union[Data, torch.Tensor]
    done: bool
    priority: float = 1.0
    agent_id: str = "default"
    timestamp: float = 0.0
    info: dict = None

class ReplayBuffer:
    """
    æ ‡å‡†ç»éªŒå›æ”¾ç¼“å†²åŒº
    
    åŠŸèƒ½ï¼š
    1. å­˜å‚¨å’Œé‡‡æ ·ç»éªŒ
    2. æ”¯æŒå›¾æ•°æ®å’Œå¼ é‡æ•°æ®
    3. è‡ªåŠ¨å†…å­˜ç®¡ç†
    4. æ‰¹é‡é‡‡æ ·ä¼˜åŒ–
    """
    
    def __init__(self, capacity: int = 10000, seed: Optional[int] = None):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.position = 0
        
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_added = 0
        self.total_sampled = 0
        
    def add(self, state, action, reward, next_state, done, **kwargs):
        """
        æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº
        
        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
            **kwargs: é¢å¤–ä¿¡æ¯
        """
        
        # åˆ›å»ºç»éªŒå¯¹è±¡
        experience = Experience(
            state=self._process_state(state),
            action=action,
            reward=float(reward),
            next_state=self._process_state(next_state),
            done=bool(done),
            agent_id=kwargs.get('agent_id', 'default'),
            timestamp=kwargs.get('timestamp', 0.0),
            info=kwargs.get('info', {})
        )
        
        self.buffer.append(experience)
        self.total_added += 1
    
    def _process_state(self, state):
        """å¤„ç†çŠ¶æ€æ•°æ®ï¼Œç¡®ä¿æ­£ç¡®å­˜å‚¨"""
        if isinstance(state, Data):
            # PyTorch Geometricæ•°æ®ï¼šå¤åˆ¶åˆ°CPUé¿å…GPUå†…å­˜æ³„æ¼
            return Data(
                x=state.x.cpu() if state.x is not None else None,
                edge_index=state.edge_index.cpu() if state.edge_index is not None else None,
                edge_attr=state.edge_attr.cpu() if state.edge_attr is not None else None,
                batch=state.batch.cpu() if hasattr(state, 'batch') and state.batch is not None else None
            )
        elif isinstance(state, torch.Tensor):
            return state.cpu().clone()
        elif isinstance(state, np.ndarray):
            return torch.tensor(state, dtype=torch.float32)
        else:
            return state
    
    def sample(self, batch_size: int, device: torch.device = None) -> Tuple:
        """
        éšæœºé‡‡æ ·ç»éªŒæ‰¹æ¬¡
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: ç›®æ ‡è®¾å¤‡
            
        Returns:
            (states, actions, rewards, next_states, dones): æ‰¹æ¬¡æ•°æ®
        """
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)
        
        # éšæœºé‡‡æ ·
        experiences = random.sample(self.buffer, batch_size)
        self.total_sampled += batch_size
        
        return self._batch_experiences(experiences, device)
    
    def _batch_experiences(self, experiences: List[Experience], device: torch.device = None):
        """å°†ç»éªŒåˆ—è¡¨è½¬æ¢ä¸ºæ‰¹æ¬¡å¼ é‡"""
        if device is None:
            device = torch.device("cpu")
        
        if not experiences:
            # å¦‚æœç»éªŒåˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºå¼ é‡
            empty_tensor = torch.empty(0)
            return empty_tensor, [], empty_tensor, empty_tensor, empty_tensor
        
        # åˆ†ç¦»å„ä¸ªç»„ä»¶
        states = [exp.state for exp in experiences]
        actions = [exp.action for exp in experiences]
        rewards = [exp.reward for exp in experiences]
        next_states = [exp.next_state for exp in experiences]
        dones = [exp.done for exp in experiences]
        
        # å¤„ç†çŠ¶æ€æ‰¹æ¬¡
        if all(isinstance(s, Data) for s in states):
            # å›¾æ•°æ®æ‰¹å¤„ç†
            try:
                states_batch = Batch.from_data_list(states).to(device)
                next_states_batch = Batch.from_data_list(next_states).to(device)
            except Exception as e:
                print(f"å›¾æ•°æ®æ‰¹å¤„ç†å¤±è´¥: {e}")
                # å›é€€åˆ°å¼ é‡å¤„ç†
                states_batch = torch.stack([torch.randn(128) for _ in states]).to(device)
                next_states_batch = torch.stack([torch.randn(128) for _ in next_states]).to(device)
        else:
            # å¼ é‡æ•°æ®æ‰¹å¤„ç†
            states_batch = torch.stack([s if isinstance(s, torch.Tensor) else torch.tensor(s) 
                                      for s in states]).to(device)
            next_states_batch = torch.stack([s if isinstance(s, torch.Tensor) else torch.tensor(s) 
                                           for s in next_states]).to(device)
        
        # å¤„ç†åŠ¨ä½œ
        if all(isinstance(a, (int, np.integer)) for a in actions):
            actions_batch = torch.tensor(actions, dtype=torch.long).to(device)
        else:
            # å¤šåŠ¨ä½œæƒ…å†µ
            actions_batch = actions  # ä¿æŒåˆ—è¡¨æ ¼å¼
        
        # å¤„ç†å¥–åŠ±å’Œå®Œæˆæ ‡å¿—
        rewards_batch = torch.tensor(rewards, dtype=torch.float32).to(device)
        dones_batch = torch.tensor(dones, dtype=torch.bool).to(device)
        
        return states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch
    
    def __len__(self):
        return len(self.buffer)
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.buffer.clear()
        self.position = 0
    
    def get_stats(self) -> dict:
        """è·å–ç¼“å†²åŒºç»Ÿè®¡ä¿¡æ¯"""
        return {
            "capacity": self.capacity,
            "current_size": len(self.buffer),
            "utilization": len(self.buffer) / self.capacity,
            "total_added": self.total_added,
            "total_sampled": self.total_sampled
        }


class PrioritizedReplayBuffer(ReplayBuffer):
    """
    ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº
    
    åŸºäºTDè¯¯å·®çš„é‡è¦æ€§é‡‡æ ·ï¼Œè®©æ™ºèƒ½ä½“æ›´å¤šåœ°ä»é‡è¦ç»éªŒä¸­å­¦ä¹ 
    """
    
    def __init__(self, capacity: int = 10000, alpha: float = 0.6, beta: float = 0.4, 
                 beta_increment: float = 0.001, seed: Optional[int] = None):
        super().__init__(capacity, seed)
        
        self.alpha = alpha  # ä¼˜å…ˆçº§æŒ‡æ•°
        self.beta = beta    # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°
        self.beta_increment = beta_increment
        self.max_priority = 1.0
        
        # ä½¿ç”¨æ®µæ ‘ï¼ˆSegment Treeï¼‰é«˜æ•ˆå®ç°ä¼˜å…ˆçº§é‡‡æ ·
        self.tree_capacity = 1
        while self.tree_capacity < capacity:
            self.tree_capacity *= 2
        
        self.sum_tree = np.zeros(2 * self.tree_capacity - 1)
        self.min_tree = np.full(2 * self.tree_capacity - 1, float('inf'))
        self.data_pointer = 0
    
    def add(self, state, action, reward, next_state, done, priority: Optional[float] = None, **kwargs):
        """æ·»åŠ ç»éªŒï¼Œæ”¯æŒæŒ‡å®šä¼˜å…ˆçº§"""
        if priority is None:
            priority = self.max_priority
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•å­˜å‚¨ç»éªŒ
        super().add(state, action, reward, next_state, done, **kwargs)
        
        # æ›´æ–°ä¼˜å…ˆçº§æ ‘
        self._update_tree(self.data_pointer, priority)
        self.data_pointer = (self.data_pointer + 1) % self.capacity
    
    def _update_tree(self, idx: int, priority: float):
        """æ›´æ–°æ®µæ ‘ä¸­çš„ä¼˜å…ˆçº§"""
        priority = max(priority, 1e-6)  # é¿å…é›¶ä¼˜å…ˆçº§
        self.max_priority = max(self.max_priority, priority)
        
        tree_idx = idx + self.tree_capacity - 1
        delta = priority ** self.alpha - self.sum_tree[tree_idx]
        
        self.sum_tree[tree_idx] = priority ** self.alpha
        self.min_tree[tree_idx] = priority ** self.alpha
        
        # å‘ä¸Šä¼ æ’­æ›´æ–°
        while tree_idx != 0:
            tree_idx = (tree_idx - 1) // 2
            self.sum_tree[tree_idx] += delta
            self.min_tree[tree_idx] = min(
                self.min_tree[2 * tree_idx + 1],
                self.min_tree[2 * tree_idx + 2]
            )
    
    def _sample_proportional(self, batch_size: int) -> List[int]:
        """æŒ‰æ¯”ä¾‹é‡‡æ ·ç´¢å¼•"""
        indices = []
        total_priority = self.sum_tree[0]
        
        if total_priority <= 0:
            # å¦‚æœæ²¡æœ‰ä¼˜å…ˆçº§ï¼Œéšæœºé‡‡æ ·
            available_indices = list(range(len(self.buffer)))
            return random.sample(available_indices, min(batch_size, len(available_indices)))
        
        for _ in range(batch_size):
            mass = random.random() * total_priority
            idx = self._retrieve_leaf(0, mass)
            # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
            if idx < len(self.buffer):
                indices.append(idx)
            elif len(self.buffer) > 0:
                indices.append(random.randint(0, len(self.buffer) - 1))
        
        return indices
    
    def _retrieve_leaf(self, idx: int, mass: float) -> int:
        """æ£€ç´¢å¶å­èŠ‚ç‚¹"""
        left = 2 * idx + 1
        right = left + 1
        
        if left >= len(self.sum_tree):
            return idx
        
        if mass <= self.sum_tree[left]:
            return self._retrieve_leaf(left, mass)
        else:
            return self._retrieve_leaf(right, mass - self.sum_tree[left])
    
    def sample(self, batch_size: int, device: torch.device = None) -> Tuple:
        """ä¼˜å…ˆçº§é‡‡æ ·"""
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)
        
        if len(self.buffer) == 0:
            # å¦‚æœç¼“å†²åŒºä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ
            empty_tensor = torch.empty(0)
            return (empty_tensor, [], empty_tensor, empty_tensor, empty_tensor, 
                   torch.empty(0), [])
        
        # æŒ‰ä¼˜å…ˆçº§é‡‡æ ·ç´¢å¼•
        indices = self._sample_proportional(batch_size)
        
        # è¿‡æ»¤æœ‰æ•ˆç´¢å¼•
        valid_indices = [i for i in indices if i < len(self.buffer)]
        if not valid_indices:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç´¢å¼•ï¼Œéšæœºé‡‡æ ·
            valid_indices = [random.randint(0, len(self.buffer) - 1) for _ in range(min(batch_size, len(self.buffer)))]
        
        # è·å–å¯¹åº”çš„ç»éªŒ
        experiences = [self.buffer[i] for i in valid_indices]
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        weights = self._calculate_weights(valid_indices, len(valid_indices))
        
        # æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        batch_data = self._batch_experiences(experiences, device)
        return batch_data + (weights, valid_indices)
    
    def _calculate_weights(self, indices: List[int], batch_size: int) -> torch.Tensor:
        """è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡"""
        min_prob = self.min_tree[0] / self.sum_tree[0]
        max_weight = (min_prob * len(self.buffer)) ** (-self.beta)
        
        weights = []
        for idx in indices:
            if idx < len(self.buffer):
                tree_idx = idx + self.tree_capacity - 1
                prob = self.sum_tree[tree_idx] / self.sum_tree[0]
                weight = (prob * len(self.buffer)) ** (-self.beta)
                weights.append(weight / max_weight)
            else:
                weights.append(1.0)
        
        return torch.tensor(weights, dtype=torch.float32)
    
    def update_priorities(self, indices: List[int], priorities: np.ndarray):
        """æ›´æ–°é‡‡æ ·ç»éªŒçš„ä¼˜å…ˆçº§"""
        for idx, priority in zip(indices, priorities):
            if idx < len(self.buffer):
                self._update_tree(idx, priority)


class MultiAgentReplayBuffer:
    """
    å¤šæ™ºèƒ½ä½“ç»éªŒå›æ”¾ç¼“å†²åŒº
    
    æ”¯æŒï¼š
    1. æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹çš„ç»éªŒå­˜å‚¨
    2. æ™ºèƒ½ä½“é—´çš„ç»éªŒå…±äº«
    3. åè°ƒå­¦ä¹ çš„æ‰¹é‡é‡‡æ ·
    """
    
    def __init__(self, capacity_per_agent: int = 10000, shared_capacity: int = 5000, 
                 prioritized: bool = True):
        self.capacity_per_agent = capacity_per_agent
        self.shared_capacity = shared_capacity
        self.prioritized = prioritized
        
        # æ¯ä¸ªæ™ºèƒ½ä½“çš„ç‹¬ç«‹ç¼“å†²åŒº
        self.agent_buffers = {}
        
        # å…±äº«ç¼“å†²åŒº
        if prioritized:
            self.shared_buffer = PrioritizedReplayBuffer(shared_capacity)
        else:
            self.shared_buffer = ReplayBuffer(shared_capacity)
        
        # å…±äº«ç­–ç•¥
        self.sharing_probability = 0.1  # 10%çš„ç»éªŒä¼šè¢«å…±äº«
    
    def add_agent(self, agent_id: str):
        """æ·»åŠ æ–°æ™ºèƒ½ä½“"""
        if agent_id not in self.agent_buffers:
            if self.prioritized:
                self.agent_buffers[agent_id] = PrioritizedReplayBuffer(self.capacity_per_agent)
            else:
                self.agent_buffers[agent_id] = ReplayBuffer(self.capacity_per_agent)
    
    def add(self, agent_id: str, state, action, reward, next_state, done, **kwargs):
        """æ·»åŠ ç»éªŒåˆ°æŒ‡å®šæ™ºèƒ½ä½“çš„ç¼“å†²åŒº"""
        # ç¡®ä¿æ™ºèƒ½ä½“ç¼“å†²åŒºå­˜åœ¨
        if agent_id not in self.agent_buffers:
            self.add_agent(agent_id)
        
        # æ·»åŠ åˆ°æ™ºèƒ½ä½“ä¸“ç”¨ç¼“å†²åŒº
        self.agent_buffers[agent_id].add(state, action, reward, next_state, done, 
                                        agent_id=agent_id, **kwargs)
        
        # éšæœºå…±äº«åˆ°å…±äº«ç¼“å†²åŒº
        if random.random() < self.sharing_probability:
            self.shared_buffer.add(state, action, reward, next_state, done, 
                                 agent_id=agent_id, **kwargs)
    
    def sample(self, agent_id: str, batch_size: int, use_shared: bool = True, 
               shared_ratio: float = 0.3, device: torch.device = None):
        """
        ä¸ºæŒ‡å®šæ™ºèƒ½ä½“é‡‡æ ·ç»éªŒæ‰¹æ¬¡
        
        Args:
            agent_id: æ™ºèƒ½ä½“ID
            batch_size: æ‰¹æ¬¡å¤§å°
            use_shared: æ˜¯å¦ä½¿ç”¨å…±äº«ç»éªŒ
            shared_ratio: å…±äº«ç»éªŒåœ¨æ‰¹æ¬¡ä¸­çš„æ¯”ä¾‹
            device: ç›®æ ‡è®¾å¤‡
        """
        if agent_id not in self.agent_buffers:
            raise ValueError(f"æ™ºèƒ½ä½“ {agent_id} ä¸å­˜åœ¨")
        
        agent_buffer = self.agent_buffers[agent_id]
        
        if not use_shared or len(self.shared_buffer) == 0:
            # åªä½¿ç”¨æ™ºèƒ½ä½“è‡ªå·±çš„ç»éªŒ
            return agent_buffer.sample(batch_size, device)
        
        # æ··åˆé‡‡æ ·ï¼šæ™ºèƒ½ä½“ç»éªŒ + å…±äº«ç»éªŒ
        shared_size = int(batch_size * shared_ratio)
        agent_size = batch_size - shared_size
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç»éªŒå¯é‡‡æ ·
        if len(agent_buffer) < agent_size:
            agent_size = len(agent_buffer)
            shared_size = batch_size - agent_size
        
        if len(self.shared_buffer) < shared_size:
            shared_size = len(self.shared_buffer)
            agent_size = batch_size - shared_size
        
        experiences = []
        
        # ä»æ™ºèƒ½ä½“ç¼“å†²åŒºé‡‡æ ·
        if agent_size > 0:
            agent_experiences = random.sample(list(agent_buffer.buffer), agent_size)
            experiences.extend(agent_experiences)
        
        # ä»å…±äº«ç¼“å†²åŒºé‡‡æ ·
        if shared_size > 0:
            shared_experiences = random.sample(list(self.shared_buffer.buffer), shared_size)
            experiences.extend(shared_experiences)
        
        # æ‰¹å¤„ç†
        return agent_buffer._batch_experiences(experiences, device)
    
    def get_stats(self) -> dict:
        """è·å–æ‰€æœ‰ç¼“å†²åŒºçš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "shared_buffer": self.shared_buffer.get_stats(),
            "agent_buffers": {}
        }
        
        for agent_id, buffer in self.agent_buffers.items():
            stats["agent_buffers"][agent_id] = buffer.get_stats()
        
        return stats


# æµ‹è¯•å‡½æ•°
def test_replay_buffers():
    """æµ‹è¯•å›æ”¾ç¼“å†²åŒºåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å›æ”¾ç¼“å†²åŒº...")
    
    # æµ‹è¯•æ ‡å‡†ç¼“å†²åŒº
    buffer = ReplayBuffer(capacity=100)
    
    # æ·»åŠ æµ‹è¯•æ•°æ®
    for i in range(50):
        state = torch.randn(4, 8)  # æ¨¡æ‹ŸèŠ‚ç‚¹ç‰¹å¾
        action = i % 10
        reward = random.random()
        next_state = torch.randn(4, 8)
        done = (i % 20 == 0)
        
        buffer.add(state, action, reward, next_state, done)
    
    # æµ‹è¯•é‡‡æ ·
    states, actions, rewards, next_states, dones = buffer.sample(batch_size=16)
    
    print(f"âœ… æ ‡å‡†ç¼“å†²åŒºæµ‹è¯•:")
    print(f"   - ç¼“å†²åŒºå¤§å°: {len(buffer)}")
    print(f"   - æ‰¹æ¬¡çŠ¶æ€å½¢çŠ¶: {states.shape}")
    print(f"   - æ‰¹æ¬¡åŠ¨ä½œå½¢çŠ¶: {actions.shape}")
    
    # æµ‹è¯•ä¼˜å…ˆçº§ç¼“å†²åŒº
    print("\nğŸ§ª æµ‹è¯•ä¼˜å…ˆçº§ç¼“å†²åŒº...")
    
    priority_buffer = PrioritizedReplayBuffer(capacity=100)
    
    # æ·»åŠ å¸¦ä¼˜å…ˆçº§çš„æ•°æ®
    for i in range(30):
        state = torch.randn(4, 8)
        action = i % 10
        reward = random.random()
        next_state = torch.randn(4, 8)
        done = False
        priority = random.random()
        
        priority_buffer.add(state, action, reward, next_state, done, priority=priority)
    
    # æµ‹è¯•ä¼˜å…ˆçº§é‡‡æ ·
    batch_data = priority_buffer.sample(batch_size=8)
    states, actions, rewards, next_states, dones, weights, indices = batch_data
    
    print(f"âœ… ä¼˜å…ˆçº§ç¼“å†²åŒºæµ‹è¯•:")
    print(f"   - ç¼“å†²åŒºå¤§å°: {len(priority_buffer)}")
    print(f"   - é‡è¦æ€§æƒé‡å½¢çŠ¶: {weights.shape}")
    print(f"   - é‡‡æ ·ç´¢å¼•æ•°é‡: {len(indices)}")
    
    # æµ‹è¯•å¤šæ™ºèƒ½ä½“ç¼“å†²åŒº
    print("\nğŸ§ª æµ‹è¯•å¤šæ™ºèƒ½ä½“ç¼“å†²åŒº...")
    
    multi_buffer = MultiAgentReplayBuffer(capacity_per_agent=50, shared_capacity=30)
    
    # æ·»åŠ å¤šä¸ªæ™ºèƒ½ä½“çš„ç»éªŒ
    agents = ["ddqn", "dqn", "ppo"]
    for agent in agents:
        for i in range(20):
            state = torch.randn(4, 8)
            action = i % 10
            reward = random.random()
            next_state = torch.randn(4, 8)
            done = False
            
            multi_buffer.add(agent, state, action, reward, next_state, done)
    
    # æµ‹è¯•æ··åˆé‡‡æ ·
    mixed_batch = multi_buffer.sample("ddqn", batch_size=12, use_shared=True)
    states, actions, rewards, next_states, dones = mixed_batch
    
    print(f"âœ… å¤šæ™ºèƒ½ä½“ç¼“å†²åŒºæµ‹è¯•:")
    print(f"   - æ™ºèƒ½ä½“æ•°é‡: {len(multi_buffer.agent_buffers)}")
    print(f"   - æ··åˆæ‰¹æ¬¡å¤§å°: {len(actions)}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = multi_buffer.get_stats()
    print(f"   - å…±äº«ç¼“å†²åŒºåˆ©ç”¨ç‡: {stats['shared_buffer']['utilization']:.2f}")
    
    print("\nâœ… æ‰€æœ‰ç¼“å†²åŒºæµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    test_replay_buffers()