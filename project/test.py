# test_system.py

"""
å¿«é€Ÿç³»ç»Ÿæµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰ä¿®å¤çš„ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import yaml
import torch
import numpy as np
import networkx as nx
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ‰€æœ‰ç»„ä»¶
try:
    from env.vnf_env_multi import MultiVNFEmbeddingEnv
    from env.topology_loader import generate_topology
    from agents.base_agent import create_agent
    from models.gnn_encoder import GNNEncoder
    from utils.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
    from utils.logger import Logger
    from utils.metrics import calculate_sar, calculate_splat
    from utils.visualization import plot_training_curves
    print("âœ… æ‰€æœ‰ç»„ä»¶å¯¼å…¥æˆåŠŸ!")
except ImportError as e:
    print(f"âŒ ç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def test_basic_components():
    """æµ‹è¯•åŸºç¡€ç»„ä»¶"""
    print("\nğŸ§ª æµ‹è¯•åŸºç¡€ç»„ä»¶...")
    
    # 1. æµ‹è¯•GNNç¼–ç å™¨
    print("   æµ‹è¯•GNNç¼–ç å™¨...")
    encoder = GNNEncoder(node_dim=8, edge_dim=4, hidden_dim=64, output_dim=128)
    
    # åˆ›å»ºæµ‹è¯•å›¾æ•°æ®
    from torch_geometric.data import Data
    test_data = Data(
        x=torch.randn(10, 8),
        edge_index=torch.randint(0, 10, (2, 20)),
        edge_attr=torch.randn(20, 4)
    )
    
    with torch.no_grad():
        output = encoder(test_data)
    print(f"      âœ… GNNç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # 2. æµ‹è¯•å›æ”¾ç¼“å†²åŒº
    print("   æµ‹è¯•å›æ”¾ç¼“å†²åŒº...")
    buffer = ReplayBuffer(capacity=100)
    
    for i in range(10):
        buffer.add(
            state=torch.randn(8),
            action=i % 5,
            reward=np.random.random(),
            next_state=torch.randn(8),
            done=False
        )
    
    states, actions, rewards, next_states, dones = buffer.sample(5)
    print(f"      âœ… å›æ”¾ç¼“å†²åŒºé‡‡æ ·æˆåŠŸ: {len(states)} æ ·æœ¬")
    
    # 3. æµ‹è¯•ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒº
    print("   æµ‹è¯•ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒº...")
    priority_buffer = PrioritizedReplayBuffer(capacity=100)
    
    for i in range(10):
        priority_buffer.add(
            state=torch.randn(8),
            action=i % 5,
            reward=np.random.random(),
            next_state=torch.randn(8),
            done=False,
            priority=np.random.random()
        )
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®å†é‡‡æ ·
    if len(priority_buffer) >= 5:
        batch_data = priority_buffer.sample(5)
        print(f"      âœ… ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºé‡‡æ ·æˆåŠŸ: {len(batch_data)} ä¸ªç»„ä»¶")
    else:
        print(f"      âš ï¸ ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºæ•°æ®ä¸è¶³ï¼Œè·³è¿‡é‡‡æ ·æµ‹è¯•")
    
    print("âœ… åŸºç¡€ç»„ä»¶æµ‹è¯•å®Œæˆ")

def test_environment():
    """æµ‹è¯•VNFåµŒå…¥ç¯å¢ƒ"""
    print("\nğŸŒ æµ‹è¯•VNFåµŒå…¥ç¯å¢ƒ...")
    
    # åˆ›å»ºæµ‹è¯•ç½‘ç»œ
    G = nx.erdos_renyi_graph(n=10, p=0.4, seed=42)
    node_features = np.random.rand(10, 4) * 0.8 + 0.2
    edge_features = np.random.rand(len(G.edges()), 4)
    edge_features[:, 0] = edge_features[:, 0] * 80 + 20  # å¸¦å®½
    edge_features[:, 1] = edge_features[:, 1] * 5 + 1    # å»¶è¿Ÿ
    
    reward_config = {
        "alpha": 0.5, "beta": 0.2, "gamma": 0.2, "delta": 0.1, "penalty": 1.0
    }
    
    # åˆ›å»ºç¯å¢ƒ
    env = MultiVNFEmbeddingEnv(
        graph=G,
        node_features=node_features,
        edge_features=edge_features,
        reward_config=reward_config
    )
    
    print(f"   âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"      ç½‘ç»œèŠ‚ç‚¹: {len(G.nodes())}")
    print(f"      åŠ¨ä½œç©ºé—´: {env.action_space}")
    
    # æµ‹è¯•ç¯å¢ƒäº¤äº’
    state = env.reset()
    print(f"      åˆå§‹çŠ¶æ€ç±»å‹: {type(state)}")
    print(f"      çŠ¶æ€ç‰¹å¾å½¢çŠ¶: {state.x.shape}")
    
    # æ‰§è¡Œå‡ æ­¥
    for step in range(3):
        valid_actions = env.get_valid_actions()
        if valid_actions:
            action = np.random.choice(valid_actions)
            next_state, reward, done, info = env.step(action)
            print(f"      æ­¥éª¤ {step+1}: åŠ¨ä½œ={action}, å¥–åŠ±={reward:.2f}, å®Œæˆ={done}")
            if done:
                break
        else:
            print(f"      æ­¥éª¤ {step+1}: æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ")
            break

def test_agents():
    """æµ‹è¯•æ™ºèƒ½ä½“"""
    print("\nğŸ¤– æµ‹è¯•æ™ºèƒ½ä½“...")
    
    # é…ç½®
    config = {
        "gnn": {"hidden_dim": 64, "output_dim": 128},
        "train": {
            "lr": 0.001, "gamma": 0.99, "batch_size": 16,
            "epsilon_start": 1.0, "epsilon_decay": 0.995, "epsilon_min": 0.01,
            "buffer_size": 1000, "target_update": 10,
            "eps_clip": 0.2, "entropy_coef": 0.01, "value_coef": 0.5,
            "ppo_epochs": 2, "mini_batch_size": 8, "rollout_length": 16
        },
        "network": {"hidden_dim": 256}
    }
    
    # æµ‹è¯•æ¯ç§æ™ºèƒ½ä½“
    agent_types = ['ddqn', 'dqn', 'ppo']
    agents = {}
    
    for agent_type in agent_types:
        print(f"   æµ‹è¯• {agent_type.upper()} æ™ºèƒ½ä½“...")
        
        try:
            agent = create_agent(
                agent_type=agent_type,
                agent_id=f"test_{agent_type}",
                state_dim=8,
                action_dim=10,
                edge_dim=4,
                config=config
            )
            agents[agent_type] = agent
            print(f"      âœ… {agent_type.upper()} æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
            test_state = torch.randn(1, 128)
            action = agent.select_action(test_state)
            print(f"      åŠ¨ä½œé€‰æ‹©æµ‹è¯•: {action}")
            
            # æµ‹è¯•ç»éªŒå­˜å‚¨
            agent.store_transition(
                state=test_state,
                action=action,
                reward=1.0,
                next_state=torch.randn(1, 128),
                done=False
            )
            print(f"      âœ… ç»éªŒå­˜å‚¨æˆåŠŸ")
            
        except Exception as e:
            print(f"      âŒ {agent_type.upper()} æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥: {e}")

def test_integration():
    """æµ‹è¯•å®Œæ•´é›†æˆ"""
    print("\nğŸ”„ æµ‹è¯•å®Œæ•´é›†æˆ...")
    
    # åˆ›å»ºç®€å•çš„é…ç½®
    config = {
        "gnn": {"hidden_dim": 32, "output_dim": 64},
        "train": {
            "lr": 0.001, "gamma": 0.99, "batch_size": 8,
            "epsilon_start": 1.0, "epsilon_decay": 0.995, "epsilon_min": 0.01,
            "buffer_size": 100, "target_update": 10,
            "episodes": 5  # åªè¿è¡Œ5ä¸ªepisodeæµ‹è¯•
        },
        "network": {"hidden_dim": 128}
    }
    
    # åˆ›å»ºç½‘ç»œå’Œç¯å¢ƒ
    G = nx.erdos_renyi_graph(n=8, p=0.5, seed=42)
    node_features = np.random.rand(8, 4) * 0.8 + 0.2
    edge_features = np.random.rand(len(G.edges()), 4)
    edge_features[:, 0] = edge_features[:, 0] * 80 + 20
    edge_features[:, 1] = edge_features[:, 1] * 5 + 1
    
    reward_config = {"alpha": 0.5, "beta": 0.2, "gamma": 0.2, "delta": 0.1, "penalty": 1.0}
    
    env = MultiVNFEmbeddingEnv(
        graph=G,
        node_features=node_features,
        edge_features=edge_features,
        reward_config=reward_config
    )
    
    # åˆ›å»ºDDQNæ™ºèƒ½ä½“
    # æ³¨æ„ï¼šç¯å¢ƒçš„çŠ¶æ€åŒ…å«å¢å¼ºç‰¹å¾ï¼Œæ‰€ä»¥state_dimåº”è¯¥æ˜¯ node_features + status_features
    actual_state_dim = 4 + 4  # åŸå§‹èŠ‚ç‚¹ç‰¹å¾4ç»´ + çŠ¶æ€ä¿¡æ¯4ç»´
    agent = create_agent(
        agent_type='ddqn',
        agent_id='integration_test',
        state_dim=actual_state_dim,  # ä½¿ç”¨æ­£ç¡®çš„çŠ¶æ€ç»´åº¦
        action_dim=8,
        edge_dim=4,
        config=config
    )
    
    print(f"   è¿è¡Œ {config['train']['episodes']} ä¸ªepisode...")
    print(f"   ç½‘ç»œèŠ‚ç‚¹ç‰¹å¾å½¢çŠ¶: {node_features.shape}")
    print(f"   ç¯å¢ƒå®é™…çŠ¶æ€ç»´åº¦: {env.actual_state_dim}")
    print(f"   æ™ºèƒ½ä½“æœŸæœ›çŠ¶æ€ç»´åº¦: {actual_state_dim}")
    
    # è¿è¡Œè®­ç»ƒå¾ªç¯
    episode_rewards = []
    episode_sars = []
    
    for episode in range(config['train']['episodes']):
        state = env.reset()
        total_reward = 0.0
        step_count = 0
        max_steps = 10
        
        while step_count < max_steps:
            # è·å–æœ‰æ•ˆåŠ¨ä½œ
            valid_actions = env.get_valid_actions()
            if not valid_actions:
                break
            
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, valid_actions=valid_actions)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, action, reward, next_state, done)
            
            # å­¦ä¹ 
            if hasattr(agent, 'replay_buffer') and len(agent.replay_buffer) >= agent.batch_size:
                learning_info = agent.learn()
            
            state = next_state
            total_reward += reward
            step_count += 1
            
            if done:
                break
        
        # è®°å½•ç»“æœ
        episode_rewards.append(total_reward)
        sar = 1.0 if info.get('success', False) else 0.0
        episode_sars.append(sar)
        
        print(f"      Episode {episode+1}: å¥–åŠ±={total_reward:.2f}, SAR={sar:.2f}, æ­¥æ•°={step_count}")
    
    # è®¡ç®—å¹³å‡æ€§èƒ½
    avg_reward = np.mean(episode_rewards)
    avg_sar = np.mean(episode_sars)
    
    print(f"   âœ… é›†æˆæµ‹è¯•å®Œæˆ!")
    print(f"      å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"      å¹³å‡SAR: {avg_sar:.2f}")
    
    return avg_reward, avg_sar

def test_logging():
    """æµ‹è¯•æ—¥å¿—åŠŸèƒ½"""
    print("\nğŸ“Š æµ‹è¯•æ—¥å¿—åŠŸèƒ½...")
    
    # åˆ›å»ºä¸´æ—¶æ—¥å¿—ç›®å½•
    log_dir = f"test_logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger = Logger(log_dir)
    
    # è®°å½•æµ‹è¯•æ•°æ®
    for episode in range(3):
        episode_stats = {
            'total_reward': np.random.uniform(10, 50),
            'steps': np.random.randint(5, 15),
            'success': np.random.choice([True, False]),
            'sar': np.random.uniform(0.5, 1.0),
            'splat': np.random.uniform(2, 8)
        }
        logger.log_episode(episode + 1, episode_stats)
    
    print(f"   âœ… æ—¥å¿—è®°å½•æˆåŠŸ: {log_dir}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import shutil
    try:
        shutil.rmtree(log_dir)
        print(f"   ğŸ§¹ æ¸…ç†æµ‹è¯•æ–‡ä»¶: {log_dir}")
    except:
        pass

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç³»ç»Ÿå®Œæ•´æ€§æµ‹è¯•...")
    print("="*60)
    
    try:
        # 1. åŸºç¡€ç»„ä»¶æµ‹è¯•
        test_basic_components()
        
        # 2. ç¯å¢ƒæµ‹è¯•
        test_environment()
        
        # 3. æ™ºèƒ½ä½“æµ‹è¯•
        test_agents()
        
        # 4. é›†æˆæµ‹è¯•
        avg_reward, avg_sar = test_integration()
        
        # 5. æ—¥å¿—æµ‹è¯•
        test_logging()
        
        print("\n" + "="*60)
        print("ğŸ‰ ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")
        print(f"   é›†æˆæµ‹è¯•ç»“æœ: å¹³å‡å¥–åŠ±={avg_reward:.2f}, å¹³å‡SAR={avg_sar:.2f}")
        
        if avg_sar > 0.3:  # å¦‚æœSARå¤§äº30%ï¼Œè®¤ä¸ºç³»ç»ŸåŸºæœ¬æ­£å¸¸
            print("âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹å®Œæ•´è®­ç»ƒ!")
            print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
            print("   1. è¿è¡Œ: python main_multi_agent.py --episodes 300")
            print("   2. æ£€æŸ¥ results/ ç›®å½•ä¸­çš„è®­ç»ƒç»“æœ")
            print("   3. å¯¹æ¯” edge-aware å’Œ baseline çš„æ€§èƒ½å·®å¼‚")
        else:
            print("âš ï¸  ç³»ç»Ÿå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")
            print("   å»ºè®®æ£€æŸ¥å¥–åŠ±å‡½æ•°å’Œç½‘ç»œå‚æ•°è®¾ç½®")
        
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)