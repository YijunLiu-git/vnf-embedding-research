
import os
import torch
import numpy as np
from datetime import datetime
from typing import Dict, List, Any
import argparse

# å¯¼å…¥ç°æœ‰ç»„ä»¶
from env.enhanced_vnf_env_multi import EnhancedVNFEmbeddingEnv  # ä½¿ç”¨æ‚¨ç°æœ‰çš„å¢å¼ºç¯å¢ƒ
from env.topology_loader import generate_topology
from agents.base_agent import create_agent
from utils.logger import Logger
from config_loader import get_scenario_config, load_config

class SafeEnhancedTrainer:
    """
    å®‰å…¨çš„å¢å¼ºè®­ç»ƒå™¨ - é¿å…ç»´åº¦å†²çª
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        print(f"ğŸ›¡ï¸ åˆå§‹åŒ–å®‰å…¨å¢å¼ºè®­ç»ƒç³»ç»Ÿ...")
        
        self.config = load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.episodes = self.config['train']['episodes']
        self.agent_types = ['ddqn', 'dqn', 'ppo']
        
        self.results_dir = f"safe_enhanced_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.results_dir, exist_ok=True)
        
        self.current_scenario = "normal_operation"
        
        # è®¾ç½®ç»„ä»¶
        self._setup_safe_components()
        
        print(f"âœ… å®‰å…¨å¢å¼ºè®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _setup_safe_components(self):
        """å®‰å…¨è®¾ç½®ç»„ä»¶"""
        
        # 1. ç”Ÿæˆæ‹“æ‰‘
        full_config = {
            'topology': self.config['topology'],
            'vnf_requirements': self.config['vnf_requirements'],
            'dimensions': self.config['dimensions']
        }
        self.graph, self.node_features, self.edge_features = generate_topology(config=full_config)
        
        # 2. åˆ›å»ºç¯å¢ƒ
        reward_config = self.config['reward']
        chain_length_range = tuple(self.config['vnf_requirements']['chain_length_range'])
        
        env_config = {
            'topology': self.config['topology'],
            'vnf_requirements': self.config['vnf_requirements'],
            'reward': self.config['reward'],
            'train': self.config['train'],
            'dimensions': self.config['dimensions']
        }
        
        # ä½¿ç”¨æ‚¨ç°æœ‰çš„å¢å¼ºç¯å¢ƒ
        self.env_edge_aware = EnhancedVNFEmbeddingEnv(
            graph=self.graph.copy(),
            node_features=self.node_features.copy(),
            edge_features=self.edge_features.copy(),
            reward_config=reward_config,
            chain_length_range=chain_length_range,
            config=env_config.copy()
        )
        
        self.env_baseline = EnhancedVNFEmbeddingEnv(
            graph=self.graph.copy(),
            node_features=self.node_features.copy(),
            edge_features=self.edge_features.copy(),
            reward_config=reward_config,
            chain_length_range=chain_length_range,
            config=env_config.copy()
        )
        self.env_baseline.is_baseline_mode = True
        
        # 3. åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆä¸æ›¿æ¢ç¼–ç å™¨ï¼Œä½¿ç”¨æ ‡å‡†ç‰ˆæœ¬ï¼‰
        expected_node_dim = self.config['dimensions']['node_feature_dim']
        action_dim = len(self.graph.nodes())
        
        print(f"ğŸ¤– åˆ›å»ºå®‰å…¨æ™ºèƒ½ä½“:")
        
        # Edge-awareæ™ºèƒ½ä½“
        self.agents_edge_aware = {}
        for agent_type in self.agent_types:
            agent_id = f"{agent_type}_safe_edge_aware"
            edge_dim = self.config['gnn']['edge_aware']['edge_dim']
            
            agent = create_agent(
                agent_type=agent_type,
                agent_id=agent_id,
                state_dim=expected_node_dim,
                action_dim=action_dim,
                edge_dim=edge_dim,
                config=self.config
            )
            
            self.agents_edge_aware[agent_type] = agent
            print(f"   âœ… {agent_id}: æ ‡å‡†GNNç¼–ç å™¨")
        
        # Baselineæ™ºèƒ½ä½“
        self.agents_baseline = {}
        for agent_type in self.agent_types:
            agent_id = f"{agent_type}_safe_baseline"
            edge_dim = self.config['gnn']['baseline']['edge_dim']
            
            self.agents_baseline[agent_type] = create_agent(
                agent_type=agent_type,
                agent_id=agent_id,
                state_dim=expected_node_dim,
                action_dim=action_dim,
                edge_dim=edge_dim,
                config=self.config
            )
            print(f"   âœ… {agent_id}: æ ‡å‡†GNNç¼–ç å™¨")
        
        # 4. è®¾ç½®æ—¥å¿—
        self._setup_logging()
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.loggers = {}
        
        for agent_type in self.agent_types:
            self.loggers[f"{agent_type}_edge_aware"] = Logger(
                log_dir=os.path.join(self.results_dir, f"safe_{agent_type}_edge_aware_{timestamp}")
            )
            self.loggers[f"{agent_type}_baseline"] = Logger(
                log_dir=os.path.join(self.results_dir, f"safe_{agent_type}_baseline_{timestamp}")
            )
    
    def train_safe_episode(self, agent, env, agent_id: str) -> Dict[str, Any]:
        """å®‰å…¨è®­ç»ƒå•ä¸ªepisode"""
        state = env.reset()
        total_reward = 0.0
        step_count = 0
        success = False
        info = {}
        
        max_steps = 20
        
        while step_count < max_steps:
            try:
                # è·å–æœ‰æ•ˆåŠ¨ä½œ
                if hasattr(env, 'get_enhanced_valid_actions'):
                    valid_actions = env.get_enhanced_valid_actions()
                else:
                    valid_actions = env.get_valid_actions()
                
                if not valid_actions:
                    info = {'success': False, 'reason': 'no_valid_actions'}
                    break
                
                # é€‰æ‹©åŠ¨ä½œ
                action = agent.select_action(state, valid_actions=valid_actions)
                if action not in valid_actions:
                    action = np.random.choice(valid_actions)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, step_info = env.step(action)
                
                # å­˜å‚¨ç»éªŒ
                agent.store_transition(state, action, reward, next_state, done)
                
                # æ›´æ–°çŠ¶æ€
                state = next_state
                total_reward += reward
                step_count += 1
                
                # å­¦ä¹ æ›´æ–°
                try:
                    if hasattr(agent, 'should_update') and agent.should_update():
                        learning_info = agent.learn()
                    elif hasattr(agent, 'replay_buffer') and len(getattr(agent, 'replay_buffer', [])) >= 16:
                        learning_info = agent.learn()
                except Exception as e:
                    print(f"âš ï¸ å­¦ä¹ æ›´æ–°å¤±è´¥: {e}")
                
                if done:
                    success = step_info.get('success', False)
                    info = step_info
                    break
                    
            except Exception as e:
                print(f"âš ï¸ æ­¥éª¤æ‰§è¡Œå¤±è´¥: {e}")
                break
        
        # è®¡ç®—ç»Ÿè®¡
        sar = 1.0 if success else 0.0
        splat = info.get('splat', info.get('avg_delay', float('inf'))) if success else float('inf')
        
        return {
            'total_reward': total_reward,
            'steps': step_count,
            'success': success,
            'sar': sar,
            'splat': splat,
            'info': info,
            'scenario': self.current_scenario
        }
    
    def train_safe(self):
        """å®‰å…¨è®­ç»ƒä¸»å¾ªç¯"""
        print(f"\nğŸ›¡ï¸ å¼€å§‹å®‰å…¨å¢å¼ºè®­ç»ƒ...")
        
        performance_results = {
            'edge_aware': {agent: {'sar': [], 'splat': [], 'rewards': []} for agent in self.agent_types},
            'baseline': {agent: {'sar': [], 'splat': [], 'rewards': []} for agent in self.agent_types}
        }
        
        # åº”ç”¨åˆå§‹åœºæ™¯
        initial_scenario_config = get_scenario_config(1)
        self.env_edge_aware.apply_scenario_config(initial_scenario_config)
        self.env_baseline.apply_scenario_config(initial_scenario_config)
        
        for episode in range(1, self.episodes + 1):
            # æ›´æ–°åœºæ™¯
            if episode <= 25:
                new_scenario = "normal_operation"
            elif episode <= 50:
                new_scenario = "peak_congestion"
            elif episode <= 75:
                new_scenario = "failure_recovery"
            else:
                new_scenario = "extreme_pressure"
            
            if new_scenario != self.current_scenario:
                print(f"ğŸ¯ åœºæ™¯åˆ‡æ¢: {self.current_scenario} â†’ {new_scenario}")
                self.current_scenario = new_scenario
                scenario_config = get_scenario_config(episode)
                self.env_edge_aware.apply_scenario_config(scenario_config)
                self.env_baseline.apply_scenario_config(scenario_config)
            
            print(f"Episode {episode}/{self.episodes} - {self.current_scenario}")
            
            # è®­ç»ƒEdge-awareæ™ºèƒ½ä½“
            for agent_type in self.agent_types:
                agent = self.agents_edge_aware[agent_type]
                env = self.env_edge_aware
                episode_stats = self.train_safe_episode(agent, env, f"{agent_type}_edge_aware")
                
                performance_results['edge_aware'][agent_type]['sar'].append(episode_stats['sar'])
                performance_results['edge_aware'][agent_type]['splat'].append(episode_stats['splat'])
                performance_results['edge_aware'][agent_type]['rewards'].append(episode_stats['total_reward'])
                
                # è®°å½•æ—¥å¿—
                if f"{agent_type}_edge_aware" in self.loggers:
                    self.loggers[f"{agent_type}_edge_aware"].log_episode(episode, episode_stats)
            
            # è®­ç»ƒBaselineæ™ºèƒ½ä½“
            for agent_type in self.agent_types:
                agent = self.agents_baseline[agent_type]
                env = self.env_baseline
                episode_stats = self.train_safe_episode(agent, env, f"{agent_type}_baseline")
                
                performance_results['baseline'][agent_type]['sar'].append(episode_stats['sar'])
                performance_results['baseline'][agent_type]['splat'].append(episode_stats['splat'])
                performance_results['baseline'][agent_type]['rewards'].append(episode_stats['total_reward'])
                
                # è®°å½•æ—¥å¿—
                if f"{agent_type}_baseline" in self.loggers:
                    self.loggers[f"{agent_type}_baseline"].log_episode(episode, episode_stats)
            
            # æ‰“å°è¿›åº¦
            if episode % 10 == 0:
                print(f"ğŸ“Š Episode {episode} è¿›åº¦æŠ¥å‘Š:")
                for variant in ['edge_aware', 'baseline']:
                    recent_sar = np.mean([performance_results[variant][agent]['sar'][-5:] for agent in self.agent_types])
                    print(f"  {variant} å¹³å‡SAR (æœ€è¿‘5è½®): {recent_sar:.3f}")
        
        print(f"\nâœ… å®‰å…¨å¢å¼ºè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.results_dir}")
        
        return performance_results

def main_safe():
    """å®‰å…¨è®­ç»ƒä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®‰å…¨å¢å¼ºEdge-Awareè®­ç»ƒ')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    args = parser.parse_args()
    
    trainer = SafeEnhancedTrainer(config_path=args.config)
    
    if args.episodes:
        trainer.episodes = args.episodes
    
    results = trainer.train_safe()
    
    print(f"\nğŸ‰ å®‰å…¨è®­ç»ƒå®Œæˆ!")
    return results

if __name__ == "__main__":
    main_safe()