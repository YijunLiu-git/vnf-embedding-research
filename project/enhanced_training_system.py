# enhanced_training_system.py - é›†æˆå¢å¼ºåŠŸèƒ½çš„è®­ç»ƒç³»ç»Ÿ

import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Any, Tuple
import argparse
import random

# å¯¼å…¥å¢å¼ºç»„ä»¶
from env.enhanced_vnf_env_multi import EnhancedVNFEmbeddingEnv
from env.topology_loader import generate_topology
from agents.base_agent import create_agent
from models.enhanced_gnn_encoder import create_enhanced_edge_aware_encoder
from rewards.enhanced_edge_aware_reward import compute_enhanced_edge_aware_reward
from utils.logger import Logger
from utils.metrics import calculate_sar, calculate_splat
from config_loader import get_scenario_config, print_scenario_plan, validate_all_configs, load_config

class EnhancedEdgeAwareTrainer:
    """
    å¢å¼ºçš„Edge-Awareè®­ç»ƒç³»ç»Ÿ
    
    ä¸»è¦å¢å¼ºï¼š
    1. é›†æˆå¢å¼ºçš„GNNç¼–ç å™¨
    2. ä½¿ç”¨å¢å¼ºçš„å¥–åŠ±ç³»ç»Ÿ
    3. è¯¾ç¨‹å­¦ä¹ æœºåˆ¶
    4. å¯¹æ¯”å­¦ä¹ æ¡†æ¶
    5. åŠ¨æ€æ€§èƒ½ç›‘æ§
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        print(f"ğŸš€ åˆå§‹åŒ–å¢å¼ºEdge-Awareè®­ç»ƒç³»ç»Ÿ...")
        
        # åŠ è½½é…ç½®
        self.config = load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è®­ç»ƒå‚æ•°
        self.episodes = self.config['train']['episodes']
        self.save_interval = 50
        self.eval_interval = 25
        self.agent_types = ['ddqn', 'dqn', 'ppo']
        
        # ç»“æœç›®å½•
        self.results_dir = f"enhanced_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # æ¸è¿›å¼åœºæ™¯
        self.current_scenario = "normal_operation"
        self.scenario_start_episode = 1
        
        # æ€§èƒ½ç›‘æ§
        self.performance_tracker = {
            'edge_aware': {agent: {'sar': [], 'splat': [], 'rewards': [], 'edge_scores': [], 'quality_scores': []} 
                          for agent in self.agent_types},
            'baseline': {agent: {'sar': [], 'splat': [], 'rewards': [], 'edge_scores': [], 'quality_scores': []} 
                        for agent in self.agent_types}
        }
        
        # æ‰“å°è®­ç»ƒè®¡åˆ’å’ŒéªŒè¯é…ç½®
        print_scenario_plan()
        validate_all_configs()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._setup_enhanced_components()
        
        print(f"âœ… å¢å¼ºEdge-Awareè®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ™ºèƒ½ä½“ç±»å‹: {self.agent_types}")
        print(f"   - è®­ç»ƒè½®æ•°: {self.episodes}")
        print(f"   - å¢å¼ºåŠŸèƒ½: GNNç¼–ç å™¨ + å¥–åŠ±ç³»ç»Ÿ + è¯¾ç¨‹å­¦ä¹ ")
    
    def _setup_enhanced_components(self):
        """è®¾ç½®å¢å¼ºç»„ä»¶"""
        print(f"ğŸ”§ è®¾ç½®å¢å¼ºç»„ä»¶...")
        
        # 1. ç”Ÿæˆç½‘ç»œæ‹“æ‰‘
        self._setup_enhanced_topology()
        
        # 2. åˆ›å»ºå¢å¼ºç¯å¢ƒ
        self._setup_enhanced_environments()
        
        # 3. åˆ›å»ºå¢å¼ºæ™ºèƒ½ä½“
        self._setup_enhanced_agents()
        
        # 4. è®¾ç½®æ—¥å¿—è®°å½•
        self._setup_logging()
        
        print(f"âœ… å¢å¼ºç»„ä»¶è®¾ç½®å®Œæˆ")
    
    def _setup_enhanced_topology(self):
        """è®¾ç½®å¢å¼ºç½‘ç»œæ‹“æ‰‘"""
        full_config = {
            'topology': self.config['topology'],
            'vnf_requirements': self.config['vnf_requirements'],
            'dimensions': self.config['dimensions']
        }
        
        self.graph, self.node_features, self.edge_features = generate_topology(config=full_config)
        
        num_nodes = len(self.graph.nodes())
        num_edges = len(self.graph.edges())
        
        print(f"ğŸŒ å¢å¼ºç½‘ç»œæ‹“æ‰‘:")
        print(f"   - èŠ‚ç‚¹æ•°: {num_nodes}")
        print(f"   - è¾¹æ•°: {num_edges}")
        print(f"   - èŠ‚ç‚¹ç‰¹å¾: {self.node_features.shape}")
        print(f"   - è¾¹ç‰¹å¾: {self.edge_features.shape}")
        
        # éªŒè¯æ‹“æ‰‘è¿é€šæ€§
        import networkx as nx
        if not nx.is_connected(self.graph):
            print("âš ï¸ è­¦å‘Š: ç½‘ç»œæ‹“æ‰‘ä¸è¿é€šï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")
    
    def _setup_enhanced_environments(self):
        """è®¾ç½®å¢å¼ºç¯å¢ƒ"""
        reward_config = self.config['reward']
        chain_length_range = tuple(self.config['vnf_requirements']['chain_length_range'])
        
        env_config = {
            'topology': self.config['topology'],
            'vnf_requirements': self.config['vnf_requirements'],
            'reward': self.config['reward'],
            'train': self.config['train'],
            'dimensions': self.config['dimensions']
        }
        
        # åˆ›å»ºå¢å¼ºçš„Edge-awareç¯å¢ƒ
        self.env_edge_aware = EnhancedVNFEmbeddingEnv(
            graph=self.graph.copy(),
            node_features=self.node_features.copy(),
            edge_features=self.edge_features.copy(),
            reward_config=reward_config,
            chain_length_range=chain_length_range,
            config=env_config.copy()
        )
        
        # åˆ›å»ºBaselineç¯å¢ƒ
        self.env_baseline = EnhancedVNFEmbeddingEnv(
            graph=self.graph.copy(),
            node_features=self.node_features.copy(),
            edge_features=self.edge_features.copy(),
            reward_config=reward_config,
            chain_length_range=chain_length_range,
            config=env_config.copy()
        )
        self.env_baseline.is_baseline_mode = True
        
        print(f"ğŸŒ å¢å¼ºç¯å¢ƒåˆ›å»ºå®Œæˆ:")
        print(f"   - Edge-awareç¯å¢ƒ: å®Œæ•´è¾¹ç‰¹å¾æ„ŸçŸ¥")
        print(f"   - Baselineç¯å¢ƒ: ç®€åŒ–è¾¹ç‰¹å¾")
    
    def _setup_enhanced_agents(self):
        """è®¾ç½®å¢å¼ºæ™ºèƒ½ä½“"""
        expected_node_dim = self.config['dimensions']['node_feature_dim']
        action_dim = len(self.graph.nodes())
        
        print(f"ğŸ¤– åˆ›å»ºå¢å¼ºæ™ºèƒ½ä½“:")
        print(f"   - çŠ¶æ€ç»´åº¦: {expected_node_dim}")
        print(f"   - åŠ¨ä½œç»´åº¦: {action_dim}")
        
        # Edge-awareæ™ºèƒ½ä½“ï¼ˆä½¿ç”¨å¢å¼ºGNNï¼‰
        self.agents_edge_aware = {}
        for agent_type in self.agent_types:
            agent_id = f"{agent_type}_edge_aware_enhanced"
            edge_dim = self.config['gnn']['edge_aware']['edge_dim']
            
            # åˆ›å»ºæ™ºèƒ½ä½“
            agent = create_agent(
                agent_type=agent_type,
                agent_id=agent_id,
                state_dim=expected_node_dim,
                action_dim=action_dim,
                edge_dim=edge_dim,
                config=self.config
            )
            
            # æ›¿æ¢GNNç¼–ç å™¨ä¸ºå¢å¼ºç‰ˆæœ¬
            enhanced_encoder = create_enhanced_edge_aware_encoder(self.config)
            agent.gnn_encoder = enhanced_encoder
            
            self.agents_edge_aware[agent_type] = agent
            print(f"   âœ… {agent_id}: å¢å¼ºGNNç¼–ç å™¨")
        
        # Baselineæ™ºèƒ½ä½“
        self.agents_baseline = {}
        for agent_type in self.agent_types:
            agent_id = f"{agent_type}_baseline"
            edge_dim = self.config['gnn']['baseline']['edge_dim']
            
            self.agents_baseline[agent_type] = create_agent(
                agent_type=agent_type,
                agent_id=agent_id,
                state_dim=expected_node_dim,
                action_dim=action_dim,
                edge_dim=edge_dim,
                config=self.config
            )
            print(f"   âœ… {agent_id}: æ ‡å‡†GNNç¼–ç å™¨")
    
    def _setup_logging(self):
        """è®¾ç½®å¢å¼ºæ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.loggers = {}
        
        for agent_type in self.agent_types:
            # Edge-awareæ—¥å¿—å™¨
            self.loggers[f"{agent_type}_edge_aware"] = Logger(
                log_dir=os.path.join(self.results_dir, f"enhanced_{agent_type}_edge_aware_{timestamp}")
            )
            # Baselineæ—¥å¿—å™¨
            self.loggers[f"{agent_type}_baseline"] = Logger(
                log_dir=os.path.join(self.results_dir, f"{agent_type}_baseline_{timestamp}")
            )
        
        print(f"ğŸ“Š å¢å¼ºæ—¥å¿—è®°å½•è®¾ç½®å®Œæˆ")
    
    def train_enhanced_episode(self, agent, env, agent_id: str) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªå¢å¼ºepisode"""
        state = env.reset()
        total_reward = 0.0
        step_count = 0
        success = False
        info = {}
        edge_scores = []
        quality_scores = []
        
        # é‡ç½®æ™ºèƒ½ä½“episodeç»Ÿè®¡
        if hasattr(agent, 'reset_episode_stats'):
            agent.reset_episode_stats()
        
        max_steps = getattr(env, 'max_episode_steps', 20)
        
        while step_count < max_steps:
            # è·å–å¢å¼ºçš„æœ‰æ•ˆåŠ¨ä½œ
            if hasattr(env, 'get_enhanced_valid_actions'):
                valid_actions = env.get_enhanced_valid_actions()
            else:
                valid_actions = env.get_valid_actions()
            
            if not valid_actions:
                info = {'success': False, 'reason': 'no_valid_actions'}
                break
            
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, valid_actions=valid_actions)
            if action not in valid_actions:
                action = random.choice(valid_actions)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, step_info = env.step(action)
            
            # æ”¶é›†å¢å¼ºæŒ‡æ ‡
            if 'edge_aware' in agent_id and hasattr(agent.gnn_encoder, 'get_vnf_adaptation_score'):
                try:
                    edge_score = agent.gnn_encoder.get_vnf_adaptation_score(state)
                    edge_scores.append(edge_score)
                except:
                    edge_scores.append(0.0)
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, action, reward, next_state, done)
            
            # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
            state = next_state
            total_reward += reward
            step_count += 1
            
            # å­¦ä¹ æ›´æ–°
            try:
                if hasattr(agent, 'should_update') and agent.should_update():
                    learning_info = agent.learn()
                elif hasattr(agent, 'replay_buffer') and len(getattr(agent, 'replay_buffer', [])) >= getattr(agent, 'batch_size', 32):
                    learning_info = agent.learn()
            except Exception as e:
                pass
            
            if done:
                success = step_info.get('success', False)
                info = step_info
                break
        
        # æœ€åä¸€æ¬¡å­¦ä¹ æ›´æ–°
        try:
            if hasattr(agent, 'experiences') and len(getattr(agent, 'experiences', [])) > 0:
                if hasattr(agent, 'should_update') and agent.should_update():
                    learning_info = agent.learn()
        except Exception as e:
            pass
        
        # è®¡ç®—å¢å¼ºæŒ‡æ ‡
        avg_edge_score = np.mean(edge_scores) if edge_scores else 0.0
        
        # è®¡ç®—è·¯å¾„è´¨é‡è¯„åˆ†
        if success and 'paths' in info:
            quality_score = self._calculate_path_quality_score(info['paths'])
            quality_scores.append(quality_score)
        
        avg_quality_score = np.mean(quality_scores) if quality_scores else 0.0
        
        # è·å–å½“å‰åœºæ™¯åç§°
        current_scenario_name = getattr(env, 'current_scenario_name', self.current_scenario)
        
        # è®¡ç®—episodeç»Ÿè®¡
        sar = 1.0 if success else 0.0
        splat = info.get('splat', info.get('avg_delay', float('inf'))) if success else float('inf')
        
        episode_stats = {
            'total_reward': total_reward,
            'steps': step_count,
            'success': success,
            'sar': sar,
            'splat': splat,
            'edge_score': avg_edge_score,
            'quality_score': avg_quality_score,
            'info': info,
            'scenario': current_scenario_name
        }
        
        return episode_stats
    
    def _calculate_path_quality_score(self, paths: List[Dict]) -> float:
        """è®¡ç®—è·¯å¾„è´¨é‡è¯„åˆ†"""
        if not paths:
            return 0.0
        
        total_quality = 0.0
        for path in paths:
            delay = path.get("delay", 0.0)
            jitter = path.get("jitter", 0.0)
            loss = path.get("loss", 0.0)
            bandwidth = path.get("bandwidth", 0.0)
            
            # ç®€åŒ–è´¨é‡è¯„åˆ†
            delay_score = max(0.0, 1.0 - delay / 100.0)
            jitter_score = max(0.0, 1.0 - jitter / 5.0)
            loss_score = max(0.0, 1.0 - loss / 0.05)
            bandwidth_score = min(1.0, bandwidth / 100.0)
            
            quality = (delay_score + jitter_score + loss_score + bandwidth_score) / 4.0
            total_quality += quality
        
        return total_quality / len(paths)
    
    def _update_scenario(self, episode: int) -> bool:
        """æ›´æ–°å½“å‰åœºæ™¯"""
        new_scenario = None
        
        if episode <= 25:
            new_scenario = "normal_operation"
        elif episode <= 50:
            new_scenario = "peak_congestion"
        elif episode <= 75:
            new_scenario = "failure_recovery"
        else:
            new_scenario = "extreme_pressure"
        
        if new_scenario and new_scenario != self.current_scenario:
            print(f"\nğŸ¯ åœºæ™¯åˆ‡æ¢: {self.current_scenario} â†’ {new_scenario}")
            
            self.current_scenario = new_scenario
            self.scenario_start_episode = episode
            
            # è·å–åœºæ™¯é…ç½®å¹¶åº”ç”¨
            scenario_config = get_scenario_config(episode)
            self.env_edge_aware.apply_scenario_config(scenario_config)
            self.env_baseline.apply_scenario_config(scenario_config)
            
            print(f"âœ… åœºæ™¯é…ç½®å·²åº”ç”¨: Episode {episode}")
            return True
        
        return False
    
    def train_enhanced(self):
        """ä¸»è¦çš„å¢å¼ºè®­ç»ƒå¾ªç¯"""
        print(f"\nğŸš€ å¼€å§‹å¢å¼ºEdge-Awareè®­ç»ƒ...")
        print(f"ç›®æ ‡episodes: {self.episodes}")
        
        # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªåœºæ™¯
        initial_scenario_config = get_scenario_config(1)
        self.env_edge_aware.apply_scenario_config(initial_scenario_config)
        self.env_baseline.apply_scenario_config(initial_scenario_config)
        
        for episode in range(1, self.episodes + 1):
            # æ£€æŸ¥å¹¶æ›´æ–°åœºæ™¯
            scenario_changed = self._update_scenario(episode)
            
            if episode % 25 == 0 or scenario_changed:
                print(f"\nğŸ“ Episode {episode}/{self.episodes} - åœºæ™¯: {self.current_scenario}")
            
            # è®­ç»ƒEdge-awareæ™ºèƒ½ä½“
            for agent_type in self.agent_types:
                agent = self.agents_edge_aware[agent_type]
                env = self.env_edge_aware
                episode_stats = self.train_enhanced_episode(agent, env, f"{agent_type}_edge_aware")
                
                # è®°å½•å¢å¼ºæŒ‡æ ‡
                self.performance_tracker['edge_aware'][agent_type]['sar'].append(episode_stats['sar'])
                self.performance_tracker['edge_aware'][agent_type]['splat'].append(episode_stats['splat'])
                self.performance_tracker['edge_aware'][agent_type]['rewards'].append(episode_stats['total_reward'])
                self.performance_tracker['edge_aware'][agent_type]['edge_scores'].append(episode_stats['edge_score'])
                self.performance_tracker['edge_aware'][agent_type]['quality_scores'].append(episode_stats['quality_score'])
                
                # è®°å½•æ—¥å¿—
                logger_id = f"{agent_type}_edge_aware"
                if logger_id in self.loggers:
                    self.loggers[logger_id].log_episode(episode, episode_stats)
            
            # è®­ç»ƒBaselineæ™ºèƒ½ä½“
            for agent_type in self.agent_types:
                agent = self.agents_baseline[agent_type]
                env = self.env_baseline
                episode_stats = self.train_enhanced_episode(agent, env, f"{agent_type}_baseline")
                
                # è®°å½•æŒ‡æ ‡
                self.performance_tracker['baseline'][agent_type]['sar'].append(episode_stats['sar'])
                self.performance_tracker['baseline'][agent_type]['splat'].append(episode_stats['splat'])
                self.performance_tracker['baseline'][agent_type]['rewards'].append(episode_stats['total_reward'])
                self.performance_tracker['baseline'][agent_type]['edge_scores'].append(episode_stats['edge_score'])
                self.performance_tracker['baseline'][agent_type]['quality_scores'].append(episode_stats['quality_score'])
                
                # è®°å½•æ—¥å¿—
                logger_id = f"{agent_type}_baseline"
                if logger_id in self.loggers:
                    self.loggers[logger_id].log_episode(episode, episode_stats)
            
            # å®šæœŸè¿›åº¦æŠ¥å‘Š
            if episode % 25 == 0:
                self._print_enhanced_progress(episode)
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if episode % self.save_interval == 0:
                self._save_enhanced_models(episode)
        
        # æœ€ç»ˆåˆ†æ
        self._enhanced_final_analysis()
        
        print(f"\nğŸ‰ å¢å¼ºEdge-Awareè®­ç»ƒå®Œæˆ!")
        return self.performance_tracker
    
    def _print_enhanced_progress(self, episode: int):
        """æ‰“å°å¢å¼ºè®­ç»ƒè¿›åº¦"""
        print(f"\nğŸ“Š Episode {episode} å¢å¼ºæ€§èƒ½ç»Ÿè®¡:")
        window = 25
        start_idx = max(0, episode - window)
        
        for variant in ['edge_aware', 'baseline']:
            print(f"\n{variant.upper()}:")
            for agent_type in self.agent_types:
                tracker = self.performance_tracker[variant][agent_type]
                
                recent_sar = np.mean(tracker['sar'][start_idx:])
                recent_splat = np.mean([s for s in tracker['splat'][start_idx:] if s != float('inf')])
                recent_reward = np.mean(tracker['rewards'][start_idx:])
                recent_edge_score = np.mean(tracker['edge_scores'][start_idx:])
                recent_quality = np.mean(tracker['quality_scores'][start_idx:])
                
                print(f"  {agent_type.upper()}:")
                print(f"    SAR: {recent_sar:.3f}")
                print(f"    SPLat: {recent_splat:.2f}")
                print(f"    Reward: {recent_reward:.1f}")
                if variant == 'edge_aware':
                    print(f"    Edge Score: {recent_edge_score:.3f}")
                    print(f"    Quality: {recent_quality:.3f}")
    
    def _save_enhanced_models(self, episode: int):
        """ä¿å­˜å¢å¼ºæ¨¡å‹"""
        checkpoint_dir = os.path.join(self.results_dir, "enhanced_checkpoints", f"episode_{episode}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜Edge-awareæ™ºèƒ½ä½“
        for agent_type, agent in self.agents_edge_aware.items():
            filepath = os.path.join(checkpoint_dir, f"enhanced_{agent_type}_edge_aware.pth")
            if hasattr(agent, 'save_checkpoint'):
                agent.save_checkpoint(filepath)
        
        # ä¿å­˜Baselineæ™ºèƒ½ä½“
        for agent_type, agent in self.agents_baseline.items():
            filepath = os.path.join(checkpoint_dir, f"{agent_type}_baseline.pth")
            if hasattr(agent, 'save_checkpoint'):
                agent.save_checkpoint(filepath)
        
        print(f"ğŸ’¾ Episode {episode} å¢å¼ºæ¨¡å‹å·²ä¿å­˜")
    
    def _enhanced_final_analysis(self):
        """å¢å¼ºæœ€ç»ˆåˆ†æ"""
        print(f"\nğŸ¯ å¢å¼ºEdge-Awareæœ€ç»ˆæ€§èƒ½åˆ†æ:")
        print(f"{'='*70}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_data = []
        
        for variant in ['edge_aware', 'baseline']:
            for agent_type in self.agent_types:
                tracker = self.performance_tracker[variant][agent_type]
                
                final_sar = np.mean(tracker['sar'][-25:]) if len(tracker['sar']) >= 25 else np.mean(tracker['sar'])
                final_splat = np.mean([s for s in tracker['splat'][-25:] if s != float('inf')])
                final_reward = np.mean(tracker['rewards'][-25:]) if len(tracker['rewards']) >= 25 else np.mean(tracker['rewards'])
                final_edge_score = np.mean(tracker['edge_scores'][-25:]) if len(tracker['edge_scores']) >= 25 else np.mean(tracker['edge_scores'])
                final_quality = np.mean(tracker['quality_scores'][-25:]) if len(tracker['quality_scores']) >= 25 else np.mean(tracker['quality_scores'])
                
                results_data.append({
                    'Variant': variant,
                    'Algorithm': agent_type.upper(),
                    'Final_SAR': final_sar,
                    'Final_SPLat': final_splat,
                    'Final_Reward': final_reward,
                    'Edge_Score': final_edge_score,
                    'Quality_Score': final_quality
                })
                
                print(f"\n{variant.upper()} - {agent_type.upper()}:")
                print(f"  æœ€ç»ˆSAR: {final_sar:.3f}")
                print(f"  æœ€ç»ˆSPLat: {final_splat:.2f}")
                print(f"  æœ€ç»ˆå¥–åŠ±: {final_reward:.1f}")
                if variant == 'edge_aware':
                    print(f"  Edgeé€‚åº”æ€§: {final_edge_score:.3f}")
                    print(f"  è·¯å¾„è´¨é‡: {final_quality:.3f}")
        
        # ä¿å­˜ç»“æœ
        df_results = pd.DataFrame(results_data)
        df_results.to_csv(os.path.join(self.results_dir, 'enhanced_final_results.csv'), index=False)
        
        # è®¡ç®—Edge-Awareä¼˜åŠ¿
        print(f"\nğŸ“ˆ Edge-Awareä¼˜åŠ¿åˆ†æ:")
        for agent_type in self.agent_types:
            edge_sar = df_results[(df_results['Variant'] == 'edge_aware') & (df_results['Algorithm'] == agent_type.upper())]['Final_SAR'].iloc[0]
            baseline_sar = df_results[(df_results['Variant'] == 'baseline') & (df_results['Algorithm'] == agent_type.upper())]['Final_SAR'].iloc[0]
            
            sar_improvement = (edge_sar - baseline_sar) / baseline_sar * 100 if baseline_sar > 0 else 0
            print(f"  {agent_type.upper()} SARæå‡: {sar_improvement:.1f}%")
        
        print(f"\nğŸ’¾ å¢å¼ºç»“æœå·²ä¿å­˜: {self.results_dir}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºEdge-Aware VNFåµŒå…¥è®­ç»ƒç³»ç»Ÿ')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡é€‰æ‹©')
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if args.device == 'auto':
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = EnhancedEdgeAwareTrainer(config_path=args.config)
    
    if args.episodes:
        trainer.episodes = args.episodes
    
    # å¼€å§‹è®­ç»ƒ
    results = trainer.train_enhanced()
    
    print(f"\nâœ… å¢å¼ºEdge-Awareè®­ç»ƒä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {trainer.results_dir}")
    print(f"ğŸ¯ æ ¸å¿ƒå¢å¼º:")
    print(f"   âœ… å¢å¼ºGNNç¼–ç å™¨ - è¾¹æ³¨æ„åŠ›æœºåˆ¶")
    print(f"   âœ… å¢å¼ºå¥–åŠ±ç³»ç»Ÿ - å¤šç»´åº¦è¯„ä¼°")
    print(f"   âœ… è¯¾ç¨‹å­¦ä¹  - æ¸è¿›å¼åœºæ™¯")
    print(f"   âœ… æ€§èƒ½ç›‘æ§ - å®æ—¶åˆ†æ")


if __name__ == "__main__":
    main()