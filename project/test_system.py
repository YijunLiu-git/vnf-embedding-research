#!/usr/bin/env python3
# test_system.py - ç³»ç»Ÿé›†æˆæµ‹è¯•è„šæœ¬

import os
import sys
import torch
import numpy as np
import warnings
warnings.filterwarnings('ignore')

def test_imports():
    """æµ‹è¯•æ‰€æœ‰æ¨¡å—å¯¼å…¥"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        # æµ‹è¯•é…ç½®åŠ è½½
        from config_loader import get_scenario_config, load_config
        print("âœ… é…ç½®åŠ è½½å™¨å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•ç¯å¢ƒæ¨¡å—
        from env.topology_loader import generate_topology
        from env.enhanced_vnf_env_multi import EnhancedVNFEmbeddingEnv
        print("âœ… ç¯å¢ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•æ™ºèƒ½ä½“æ¨¡å—
        from agents.base_agent import create_agent
        from agents.multi_ddqn_agent import MultiDDQNAgent
        from agents.multi_dqn_agent import MultiDQNAgent
        from agents.multi_ppo_agent import MultiPPOAgent
        print("âœ… æ™ºèƒ½ä½“æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹æ¨¡å—
        from models.enhanced_gnn_encoder import EdgeAttentionLayer, EnhancedEdgeAwareGNN
        print("âœ… æ¨¡å‹æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•å·¥å…·æ¨¡å—
        from utils.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
        from utils.metrics import calculate_sar, calculate_splat
        from utils.logger import Logger
        print("âœ… å·¥å…·æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•å¥–åŠ±æ¨¡å—
        from rewards.reward_v4_comprehensive_multi import compute_reward
        print("âœ… å¥–åŠ±æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•ä¸»è®­ç»ƒå™¨
        from main_multi_agent import MultiAgentTrainer
        print("âœ… ä¸»è®­ç»ƒå™¨å¯¼å…¥æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_config_system():
    """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
    print("\nğŸ§ª æµ‹è¯•é…ç½®ç³»ç»Ÿ...")
    
    try:
        from config_loader import get_scenario_config, load_config, validate_all_configs
        
        # æµ‹è¯•é…ç½®åŠ è½½
        base_config = load_config("config.yaml")
        print(f"âœ… åŸºç¡€é…ç½®åŠ è½½æˆåŠŸ: {len(base_config)} ä¸ªé…ç½®ç»„")
        
        # æµ‹è¯•åœºæ™¯é…ç½®
        scenarios = ['normal_operation', 'peak_congestion', 'failure_recovery', 'extreme_pressure']
        for scenario in scenarios:
            config = get_scenario_config(scenario)
            print(f"âœ… {scenario} åœºæ™¯é…ç½®åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•episode-basedé…ç½®
        for episode in [1, 25, 50, 75, 100]:
            config = get_scenario_config(episode)
            print(f"âœ… Episode {episode} é…ç½®åŠ è½½æˆåŠŸ")
        
        # éªŒè¯é…ç½®
        is_valid = validate_all_configs()
        print(f"âœ… é…ç½®éªŒè¯: {'é€šè¿‡' if is_valid else 'éœ€è¦è°ƒæ•´'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_topology_generation():
    """æµ‹è¯•æ‹“æ‰‘ç”Ÿæˆ"""
    print("\nğŸ§ª æµ‹è¯•æ‹“æ‰‘ç”Ÿæˆ...")
    
    try:
        from env.topology_loader import generate_topology
        from config_loader import load_config
        
        config = load_config("config.yaml")
        graph, node_features, edge_features = generate_topology(config)
        
        print(f"âœ… æ‹“æ‰‘ç”ŸæˆæˆåŠŸ:")
        print(f"   - èŠ‚ç‚¹æ•°: {len(graph.nodes())}")
        print(f"   - è¾¹æ•°: {len(graph.edges())}")
        print(f"   - èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {node_features.shape}")
        print(f"   - è¾¹ç‰¹å¾ç»´åº¦: {edge_features.shape}")
        print(f"   - å›¾è¿é€šæ€§: {nx.is_connected(graph) if 'nx' in globals() else 'Unknown'}")
        
        # éªŒè¯ç‰¹å¾ç»´åº¦
        assert node_features.shape[1] == 4, f"èŠ‚ç‚¹ç‰¹å¾åº”ä¸º4ç»´ï¼Œå®é™…{node_features.shape[1]}ç»´"
        assert edge_features.shape[1] == 4, f"è¾¹ç‰¹å¾åº”ä¸º4ç»´ï¼Œå®é™…{edge_features.shape[1]}ç»´"
        
        return graph, node_features, edge_features
        
    except Exception as e:
        print(f"âŒ æ‹“æ‰‘ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return None, None, None

def test_environment():
    """æµ‹è¯•ç¯å¢ƒ"""
    print("\nğŸ§ª æµ‹è¯•ç¯å¢ƒ...")
    
    try:
        import networkx as nx
        from env.enhanced_vnf_env_multi import EnhancedVNFEmbeddingEnv
        from config_loader import load_config, get_scenario_config
        
        # ç”Ÿæˆæµ‹è¯•æ‹“æ‰‘
        graph, node_features, edge_features = test_topology_generation()
        if graph is None:
            return False
        
        # åŠ è½½é…ç½®
        config = load_config("config.yaml")
        scenario_config = get_scenario_config('normal_operation')
        
        # åˆ›å»ºç¯å¢ƒ
        env = EnhancedVNFEmbeddingEnv(
            graph=graph,
            node_features=node_features,
            edge_features=edge_features,
            reward_config=scenario_config['reward'],
            config=config
        )
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ:")
        print(f"   - åŠ¨ä½œç»´åº¦: {env.action_dim}")
        print(f"   - çŠ¶æ€ç»´åº¦: {env.state_dim}")
        
        # åº”ç”¨åœºæ™¯é…ç½®
        env.apply_scenario_config(scenario_config)
        print(f"âœ… åœºæ™¯é…ç½®åº”ç”¨æˆåŠŸ")
        
        # æµ‹è¯•é‡ç½®
        state = env.reset()
        print(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸ: çŠ¶æ€ç±»å‹ {type(state)}")
        
        # æµ‹è¯•æ­¥éª¤
        valid_actions = env.get_valid_actions()
        if valid_actions:
            action = valid_actions[0]
            next_state, reward, done, info = env.step(action)
            print(f"âœ… ç¯å¢ƒæ­¥éª¤æµ‹è¯•æˆåŠŸ: reward={reward:.2f}, done={done}")
        
        return env
        
    except Exception as e:
        print(f"âŒ ç¯å¢ƒæµ‹è¯•å¤±è´¥: {e}")
        return None

def test_agents():
    """æµ‹è¯•æ™ºèƒ½ä½“"""
    print("\nğŸ§ª æµ‹è¯•æ™ºèƒ½ä½“...")
    
    try:
        from agents.base_agent import create_agent
        from config_loader import load_config
        
        config = load_config("config.yaml")
        
        # æµ‹è¯•å‚æ•°
        state_dim = 8  # ç»Ÿä¸€çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
        action_dim = 42  # èŠ‚ç‚¹æ•°é‡
        edge_dim = 4    # è¾¹ç‰¹å¾ç»´åº¦
        
        agent_types = ['ddqn', 'dqn', 'ppo']
        agents = {}
        
        for agent_type in agent_types:
            try:
                agent_id = f"{agent_type}_test"
                agent = create_agent(
                    agent_type=agent_type,
                    agent_id=agent_id,
                    state_dim=state_dim,
                    action_dim=action_dim,
                    edge_dim=edge_dim,
                    config=config
                )
                agents[agent_type] = agent
                print(f"âœ… {agent_type.upper()} æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
                
                # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
                test_state = torch.randn(1, 256)  # GNNè¾“å‡ºç»´åº¦
                action = agent.select_action(test_state)
                print(f"   - åŠ¨ä½œé€‰æ‹©æµ‹è¯•: {action}")
                
            except Exception as e:
                print(f"âŒ {agent_type.upper()} æ™ºèƒ½ä½“åˆ›å»ºå¤±è´¥: {e}")
        
        return agents
        
    except Exception as e:
        print(f"âŒ æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥: {e}")
        return {}

def test_gnn_encoder():
    """æµ‹è¯•GNNç¼–ç å™¨"""
    print("\nğŸ§ª æµ‹è¯•GNNç¼–ç å™¨...")
    
    try:
        from models.enhanced_gnn_encoder import EdgeAttentionLayer, EnhancedEdgeAwareGNN, create_enhanced_edge_aware_encoder_fixed
        from config_loader import load_config
        import torch
        from torch_geometric.data import Data
        
        config = load_config("config.yaml")
        
        # æµ‹è¯•æ•°æ®
        num_nodes = 42
        num_edges = 100
        node_dim = 8  # ç»Ÿä¸€8ç»´
        edge_dim_full = 4  # edge-aware
        edge_dim_baseline = 2  # baseline
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        x = torch.randn(num_nodes, node_dim)
        edge_index = torch.randint(0, num_nodes, (2, num_edges))
        edge_attr_full = torch.randn(num_edges, edge_dim_full)
        edge_attr_baseline = torch.randn(num_edges, edge_dim_baseline)
        
        # æµ‹è¯•EdgeAwareç¼–ç å™¨
        encoder_edge_aware = create_enhanced_edge_aware_encoder_fixed(config, mode='edge_aware')
        data_full = Data(x=x, edge_index=edge_index, edge_attr=edge_attr_full)
        
        with torch.no_grad():
            output_edge_aware = encoder_edge_aware(data_full)
            print(f"âœ… EdgeAwareç¼–ç å™¨æµ‹è¯•æˆåŠŸ: {output_edge_aware.shape}")
        
        # æµ‹è¯•Baselineç¼–ç å™¨
        encoder_baseline = create_enhanced_edge_aware_encoder_fixed(config, mode='baseline')
        data_baseline = Data(x=x, edge_index=edge_index, edge_attr=edge_attr_baseline)
        
        with torch.no_grad():
            output_baseline = encoder_baseline(data_baseline)
            print(f"âœ… Baselineç¼–ç å™¨æµ‹è¯•æˆåŠŸ: {output_baseline.shape}")
        
        # æµ‹è¯•VNFä¸Šä¸‹æ–‡èåˆ
        if hasattr(encoder_edge_aware, 'forward_with_vnf_context'):
            vnf_context = torch.tensor([0.05, 0.03, 0.04, 0.33, 0.5, 0.5])
            with torch.no_grad():
                output_with_context = encoder_edge_aware.forward_with_vnf_context(data_full, vnf_context)
                print(f"âœ… VNFä¸Šä¸‹æ–‡èåˆæµ‹è¯•æˆåŠŸ: {output_with_context.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ GNNç¼–ç å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_reward_system():
    """æµ‹è¯•å¥–åŠ±ç³»ç»Ÿ"""
    print("\nğŸ§ª æµ‹è¯•å¥–åŠ±ç³»ç»Ÿ...")
    
    try:
        from rewards.reward_v4_comprehensive_multi import compute_reward
        from config_loader import get_scenario_config
        
        # è·å–å¥–åŠ±é…ç½®
        scenario_config = get_scenario_config('normal_operation')
        reward_config = scenario_config['reward']
        
        # æµ‹è¯•æˆåŠŸæ¡ˆä¾‹
        success_info = {
            'total_vnfs': 3,
            'deployed_vnfs': 3,
            'paths': [
                {'delay': 25.0, 'bandwidth': 80.0, 'hops': 2, 'jitter': 0.005, 'loss': 0.001},
                {'delay': 30.0, 'bandwidth': 70.0, 'hops': 3, 'jitter': 0.008, 'loss': 0.002}
            ],
            'resource_utilization': 0.6,
            'success': True,
            'is_edge_aware': True,
            'pressure_level': 'medium'
        }
        
        reward = compute_reward(success_info, reward_config)
        print(f"âœ… æˆåŠŸæ¡ˆä¾‹å¥–åŠ±: {reward:.2f}")
        
        # æµ‹è¯•å¤±è´¥æ¡ˆä¾‹
        failure_info = {
            'total_vnfs': 3,
            'deployed_vnfs': 1,
            'success': False,
            'is_edge_aware': False,
            'pressure_level': 'high'
        }
        
        reward = compute_reward(failure_info, reward_config)
        print(f"âœ… å¤±è´¥æ¡ˆä¾‹å¥–åŠ±: {reward:.2f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¥–åŠ±ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_integration():
    """é›†æˆæµ‹è¯•"""
    print("\nğŸ§ª é›†æˆæµ‹è¯•...")
    
    try:
        # åˆ›å»ºç¯å¢ƒ
        env = test_environment()
        if env is None:
            return False
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agents = test_agents()
        if not agents:
            return False
        
        # é€‰æ‹©ä¸€ä¸ªæ™ºèƒ½ä½“è¿›è¡Œæµ‹è¯•
        agent = agents.get('ddqn')
        if agent is None:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„DDQNæ™ºèƒ½ä½“")
            return False
        
        print("ğŸ”„ æ‰§è¡Œå®Œæ•´çš„episodeæµ‹è¯•...")
        
        # é‡ç½®ç¯å¢ƒ
        state = env.reset()
        total_reward = 0.0
        step_count = 0
        max_steps = 10
        
        while step_count < max_steps:
            # è·å–æœ‰æ•ˆåŠ¨ä½œ
            valid_actions = env.get_valid_actions()
            if not valid_actions:
                print(f"âš ï¸ ç¬¬{step_count}æ­¥æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ")
                break
            
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, valid_actions=valid_actions)
            if action not in valid_actions:
                action = valid_actions[0]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, action, reward, next_state, done)
            
            total_reward += reward
            step_count += 1
            
            print(f"   æ­¥éª¤ {step_count}: åŠ¨ä½œ={action}, å¥–åŠ±={reward:.2f}, å®Œæˆ={done}")
            
            if done:
                success = info.get('success', False)
                print(f"   Episodeå®Œæˆ: æˆåŠŸ={success}, æ€»å¥–åŠ±={total_reward:.2f}")
                break
            
            state = next_state
        
        print(f"âœ… é›†æˆæµ‹è¯•å®Œæˆ: {step_count}æ­¥, æ€»å¥–åŠ±={total_reward:.2f}")
        return True
        
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_training_system():
    """æµ‹è¯•è®­ç»ƒç³»ç»Ÿ"""
    print("\nğŸ§ª æµ‹è¯•è®­ç»ƒç³»ç»Ÿ...")
    
    try:
        from main_multi_agent import MultiAgentTrainer
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MultiAgentTrainer("config.yaml")
        print("âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
        
        # ä¿®æ”¹ä¸ºçŸ­è®­ç»ƒæµ‹è¯•
        trainer.episodes = 5  # åªæµ‹è¯•5ä¸ªepisode
        
        print("ğŸ”„ æ‰§è¡ŒçŸ­æœŸè®­ç»ƒæµ‹è¯•...")
        results = trainer.train()
        
        print("âœ… è®­ç»ƒæµ‹è¯•å®Œæˆ")
        print(f"   - ç»“æœç±»å‹: {type(results)}")
        if isinstance(results, dict):
            print(f"   - ç»“æœé”®: {list(results.keys())}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ VNFåµŒå…¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå…¨é¢æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥Pythonç¯å¢ƒ
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = ["config.yaml"]
    for file in required_files:
        if not os.path.exists(file):
            print(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {file}")
            return False
    
    print("âœ… å¿…è¦æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    
    # æ‰§è¡Œæµ‹è¯•åºåˆ—
    test_results = []
    
    # 1. æ¨¡å—å¯¼å…¥æµ‹è¯•
    test_results.append(("æ¨¡å—å¯¼å…¥", test_imports()))
    
    # 2. é…ç½®ç³»ç»Ÿæµ‹è¯•
    test_results.append(("é…ç½®ç³»ç»Ÿ", test_config_system()))
    
    # 3. æ‹“æ‰‘ç”Ÿæˆæµ‹è¯•
    graph, node_features, edge_features = test_topology_generation()
    test_results.append(("æ‹“æ‰‘ç”Ÿæˆ", graph is not None))
    
    # 4. ç¯å¢ƒæµ‹è¯•
    test_results.append(("ç¯å¢ƒç³»ç»Ÿ", test_environment() is not None))
    
    # 5. æ™ºèƒ½ä½“æµ‹è¯•
    test_results.append(("æ™ºèƒ½ä½“ç³»ç»Ÿ", len(test_agents()) > 0))
    
    # 6. GNNç¼–ç å™¨æµ‹è¯•
    test_results.append(("GNNç¼–ç å™¨", test_gnn_encoder()))
    
    # 7. å¥–åŠ±ç³»ç»Ÿæµ‹è¯•
    test_results.append(("å¥–åŠ±ç³»ç»Ÿ", test_reward_system()))
    
    # 8. é›†æˆæµ‹è¯•
    test_results.append(("é›†æˆæµ‹è¯•", test_integration()))
    
    # 9. è®­ç»ƒç³»ç»Ÿæµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    run_training_test = input("\næ˜¯å¦æ‰§è¡Œè®­ç»ƒç³»ç»Ÿæµ‹è¯•ï¼Ÿ(y/N): ").lower().strip() == 'y'
    if run_training_test:
        test_results.append(("è®­ç»ƒç³»ç»Ÿ", test_training_system()))
    
    # æ‰“å°æµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    print("=" * 60)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:15s}: {status}")
        if result:
            passed += 1
    
    print("=" * 60)
    print(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡ ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥è¿è¡Œã€‚")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ¨¡å—ã€‚")
        return False

if __name__ == "__main__":
    main()