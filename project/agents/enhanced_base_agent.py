# agents/enhanced_base_agent.py - ä¿®å¤å¢å¼ºæ™ºèƒ½ä½“é›†æˆ

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from abc import ABC, abstractmethod
from typing import Union, List, Dict, Any, Optional
from torch_geometric.data import Data, Batch

from models.gnn_encoder import GNNEncoder
from models.enhanced_gnn_encoder import EnhancedEdgeAwareGNN

class EnhancedBaseAgent(ABC):
    """
    å¢å¼ºçš„åŸºç¡€æ™ºèƒ½ä½“ç±» - æ”¯æŒEnhanced GNNç¼–ç å™¨
    
    ä¸»è¦ä¿®å¤ï¼š
    1. æ­£ç¡®å¤„ç†å¢å¼ºGNNç¼–ç å™¨çš„è¾“å…¥
    2. å¤„ç†VNFä¸Šä¸‹æ–‡å’Œç½‘ç»œçŠ¶æ€
    3. å…¼å®¹åŸæœ‰å’Œå¢å¼ºçš„ç¼–ç å™¨
    """
    
    def __init__(self, 
                 agent_id: str,
                 state_dim: int, 
                 action_dim: int, 
                 edge_dim: int,
                 config: Dict[str, Any],
                 use_enhanced_gnn: bool = False):
        
        self.agent_id = agent_id
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.edge_dim = edge_dim
        self.config = config
        self.use_enhanced_gnn = use_enhanced_gnn
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ¤– Agent {agent_id} ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # é€‰æ‹© GNN é…ç½®
        gnn_config = config.get("gnn", {}).get("edge_aware" if "edge_aware" in agent_id else "baseline", {})
        self.hidden_dim = gnn_config.get("hidden_dim", 128)
        self.output_dim = gnn_config.get("output_dim", 256)
        self.num_layers = gnn_config.get("layers", 4)
        
        # è®­ç»ƒé…ç½®
        self.learning_rate = config.get("train", {}).get("lr", 0.001)
        self.gamma = config.get("train", {}).get("gamma", 0.99)
        self.batch_size = config.get("train", {}).get("batch_size", 32)
        
        # æ¢ç´¢é…ç½®
        self.epsilon = config.get("train", {}).get("epsilon_start", 1.0)
        self.epsilon_decay = config.get("train", {}).get("epsilon_decay", 0.995)
        self.epsilon_min = config.get("train", {}).get("epsilon_min", 0.01)
        
        # åˆå§‹åŒ–GNNç¼–ç å™¨
        self._setup_gnn_encoder()
        
        # ç­–ç•¥ç½‘ç»œï¼ˆå­ç±»å®ç°å…·ä½“ç»“æ„ï¼‰
        self.policy_network = None
        self.target_network = None  # DQNç³»åˆ—ä½¿ç”¨
        self.optimizer = None
        
        # è®­ç»ƒçŠ¶æ€
        self.training_step = 0
        self.episode_count = 0
        self.is_training = True
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_reward": 0.0,
            "episodes": 0,
            "steps": 0,
            "losses": [],
            "q_values": [],
            "actions_taken": {}
        }
        
        # å¤šæ™ºèƒ½ä½“åè°ƒï¼ˆé¢„ç•™ï¼‰
        self.other_agents = {}
        self.communication_enabled = False
        
    def _setup_gnn_encoder(self):
        """è®¾ç½®GNNç¼–ç å™¨"""
        if self.use_enhanced_gnn:
            # ä½¿ç”¨å¢å¼ºçš„GNNç¼–ç å™¨
            self.gnn_encoder = EnhancedEdgeAwareGNN(
                node_dim=self.state_dim,
                edge_dim=self.edge_dim,
                hidden_dim=self.hidden_dim,
                output_dim=self.output_dim,
                num_layers=self.num_layers,
                vnf_context_dim=self.config.get('dimensions', {}).get('vnf_context_dim', 6)
            ).to(self.device)
            print(f"âœ… {self.agent_id}: ä½¿ç”¨å¢å¼ºGNNç¼–ç å™¨")
        else:
            # ä½¿ç”¨æ ‡å‡†GNNç¼–ç å™¨
            self.gnn_encoder = GNNEncoder(
                node_dim=self.state_dim,
                edge_dim=self.edge_dim,
                hidden_dim=self.hidden_dim,
                output_dim=self.output_dim,
                num_layers=self.num_layers
            ).to(self.device)
            print(f"âœ… {self.agent_id}: ä½¿ç”¨æ ‡å‡†GNNç¼–ç å™¨")
    
    def process_state(self, state: Union[Data, Dict, np.ndarray]) -> torch.Tensor:
        """
        å¤„ç†çŠ¶æ€è¾“å…¥ - å¢å¼ºç‰ˆæœ¬
        
        Args:
            state: å¯ä»¥æ˜¯PyG Dataå¯¹è±¡ã€å­—å…¸æˆ–numpyæ•°ç»„
            
        Returns:
            processed_state: å¤„ç†åçš„çŠ¶æ€tensor [1, output_dim]
        """
        self.gnn_encoder.eval()
        
        with torch.no_grad():
            if isinstance(state, Data):
                state = state.to(self.device)
                
                if self.use_enhanced_gnn:
                    # å¢å¼ºGNNç¼–ç å™¨ï¼šç›´æ¥å¤„ç†åŸå§‹æ•°æ®
                    encoded_state = self.gnn_encoder(state)
                else:
                    # æ ‡å‡†GNNç¼–ç å™¨ï¼šå¯èƒ½éœ€è¦é¢„å¤„ç†
                    encoded_state = self.gnn_encoder(state)
                
            elif isinstance(state, dict) and 'graph_data' in state:
                graph_data = state['graph_data'].to(self.device)
                
                if self.use_enhanced_gnn:
                    encoded_state = self.gnn_encoder(graph_data)
                else:
                    encoded_state = self.gnn_encoder(graph_data)
                
            elif isinstance(state, (np.ndarray, torch.Tensor)):
                if isinstance(state, np.ndarray):
                    state = torch.tensor(state, dtype=torch.float32)
                encoded_state = state.unsqueeze(0).to(self.device)
                
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„çŠ¶æ€æ ¼å¼: {type(state)}")
        
        if self.is_training:
            self.gnn_encoder.train()
            
        return encoded_state
    
    def update_target_network(self, tau: float = None):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆç”¨äºDQNç³»åˆ—ç®—æ³•ï¼‰"""
        if self.target_network is None:
            return
            
        if tau is None:
            self.target_network.load_state_dict(self.policy_network.state_dict())
        else:
            for target_param, policy_param in zip(
                self.target_network.parameters(), 
                self.policy_network.parameters()
            ):
                target_param.data.copy_(
                    tau * policy_param.data + (1 - tau) * target_param.data
                )
    
    def decay_epsilon(self):
        """æ›´æ–°æ¢ç´¢ç‡"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def get_valid_actions(self, state: Union[Data, Dict], **kwargs) -> List[int]:
        """è·å–å½“å‰çŠ¶æ€ä¸‹çš„æœ‰æ•ˆåŠ¨ä½œ"""
        return list(range(self.action_dim))
    
    def mask_invalid_actions(self, q_values: torch.Tensor, valid_actions: List[int]) -> torch.Tensor:
        """å±è”½æ— æ•ˆåŠ¨ä½œçš„Qå€¼"""
        masked_q_values = q_values.clone()
        invalid_actions = [a for a in range(self.action_dim) if a not in valid_actions]
        
        if invalid_actions:
            masked_q_values[:, invalid_actions] = -float('inf')
        
        return masked_q_values
    
    def update_stats(self, reward: float, action: int, loss: float = None, q_values: torch.Tensor = None):
        """æ›´æ–°æ™ºèƒ½ä½“ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["total_reward"] += reward
        self.stats["steps"] += 1
        
        if loss is not None:
            self.stats["losses"].append(loss)
        
        if q_values is not None:
            self.stats["q_values"].append(q_values.mean().item())
        
        if action not in self.stats["actions_taken"]:
            self.stats["actions_taken"][action] = 0
        self.stats["actions_taken"][action] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½ä½“ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        if stats["episodes"] > 0:
            stats["avg_reward"] = stats["total_reward"] / stats["episodes"]
        else:
            stats["avg_reward"] = 0.0
            
        if stats["losses"]:
            stats["avg_loss"] = np.mean(stats["losses"][-100:])
            
        if stats["q_values"]:
            stats["avg_q_value"] = np.mean(stats["q_values"][-100:])
            
        stats["epsilon"] = self.epsilon
        
        return stats
    
    def reset_episode_stats(self):
        """é‡ç½®episodeç»Ÿè®¡"""
        self.stats["total_reward"] = 0.0
        self.stats["episodes"] += 1
    
    def save_checkpoint(self, filepath: str):
        """ä¿å­˜æ™ºèƒ½ä½“æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'agent_id': self.agent_id,
            'training_step': self.training_step,
            'episode_count': self.episode_count,
            'epsilon': self.epsilon,
            'stats': self.stats,
            'gnn_encoder_state': self.gnn_encoder.state_dict(),
            'use_enhanced_gnn': self.use_enhanced_gnn,
        }
        
        if self.policy_network is not None:
            checkpoint['policy_network_state'] = self.policy_network.state_dict()
            
        if self.target_network is not None:
            checkpoint['target_network_state'] = self.target_network.state_dict()
            
        if self.optimizer is not None:
            checkpoint['optimizer_state'] = self.optimizer.state_dict()
        
        torch.save(checkpoint, filepath)
        print(f"ğŸ’¾ Agent {self.agent_id} æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """åŠ è½½æ™ºèƒ½ä½“æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.training_step = checkpoint['training_step']
        self.episode_count = checkpoint['episode_count']
        self.epsilon = checkpoint['epsilon']
        self.stats = checkpoint['stats']
        
        self.gnn_encoder.load_state_dict(checkpoint['gnn_encoder_state'])
        
        if 'policy_network_state' in checkpoint and self.policy_network is not None:
            self.policy_network.load_state_dict(checkpoint['policy_network_state'])
            
        if 'target_network_state' in checkpoint and self.target_network is not None:
            self.target_network.load_state_dict(checkpoint['target_network_state'])
            
        if 'optimizer_state' in checkpoint and self.optimizer is not None:
            self.optimizer.load_state_dict(checkpoint['optimizer_state'])
        
        print(f"ğŸ“‚ Agent {self.agent_id} æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}")
    
    def set_training_mode(self, training: bool = True):
        """è®¾ç½®è®­ç»ƒ/è¯„ä¼°æ¨¡å¼"""
        self.is_training = training
        
        if training:
            self.gnn_encoder.train()
            if self.policy_network is not None:
                self.policy_network.train()
        else:
            self.gnn_encoder.eval()
            if self.policy_network is not None:
                self.policy_network.eval()
    
    def register_other_agents(self, agents: Dict[str, 'EnhancedBaseAgent']):
        """æ³¨å†Œå…¶ä»–æ™ºèƒ½ä½“ï¼ˆç”¨äºå¤šæ™ºèƒ½ä½“åè°ƒï¼‰"""
        self.other_agents = agents
        self.communication_enabled = len(agents) > 0
        print(f"ğŸ¤ Agent {self.agent_id} å·²æ³¨å†Œ {len(agents)} ä¸ªå…¶ä»–æ™ºèƒ½ä½“")
    
    @abstractmethod
    def select_action(self, state: Union[Data, Dict], **kwargs) -> Union[int, List[int]]:
        pass
    
    @abstractmethod
    def store_transition(self, state, action, reward, next_state, done, **kwargs):
        pass
    
    @abstractmethod
    def learn(self) -> Dict[str, float]:
        pass


def create_enhanced_agent(agent_type: str, agent_id: str, state_dim: int, action_dim: int, 
                         edge_dim: int, config: Dict[str, Any], use_enhanced_gnn: bool = False) -> EnhancedBaseAgent:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“
    
    Args:
        agent_type: æ™ºèƒ½ä½“ç±»å‹ ('ddqn', 'dqn', 'ppo')
        agent_id: æ™ºèƒ½ä½“ID
        state_dim: çŠ¶æ€ç»´åº¦
        action_dim: åŠ¨ä½œç»´åº¦
        edge_dim: è¾¹ç‰¹å¾ç»´åº¦
        config: é…ç½®å­—å…¸
        use_enhanced_gnn: æ˜¯å¦ä½¿ç”¨å¢å¼ºGNNç¼–ç å™¨
        
    Returns:
        agent: åˆ›å»ºçš„å¢å¼ºæ™ºèƒ½ä½“å®ä¾‹
    """
    
    if agent_type.lower() == 'ddqn':
        from agents.enhanced_multi_ddqn_agent import EnhancedMultiDDQNAgent
        return EnhancedMultiDDQNAgent(agent_id, state_dim, action_dim, edge_dim, config, use_enhanced_gnn)
    elif agent_type.lower() == 'dqn':
        from agents.enhanced_multi_dqn_agent import EnhancedMultiDQNAgent
        return EnhancedMultiDQNAgent(agent_id, state_dim, action_dim, edge_dim, config, use_enhanced_gnn)
    elif agent_type.lower() == 'ppo':
        from agents.enhanced_multi_ppo_agent import EnhancedMultiPPOAgent
        return EnhancedMultiPPOAgent(agent_id, state_dim, action_dim, edge_dim, config, use_enhanced_gnn)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ™ºèƒ½ä½“ç±»å‹: {agent_type}")


# ä¿®å¤ç°æœ‰agentsçš„å…¼å®¹æ€§
def patch_existing_agents():
    """
    ä¸ºç°æœ‰æ™ºèƒ½ä½“æ·»åŠ å¢å¼ºGNNæ”¯æŒçš„è¡¥ä¸å‡½æ•°
    """
    
    # ä¿®è¡¥DDQNæ™ºèƒ½ä½“
    from agents.multi_ddqn_agent import MultiDDQNAgent
    
    original_init = MultiDDQNAgent.__init__
    
    def enhanced_init(self, agent_id, state_dim, action_dim, edge_dim, config):
        # è°ƒç”¨åŸå§‹åˆå§‹åŒ–
        original_init(self, agent_id, state_dim, action_dim, edge_dim, config)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨å¢å¼ºGNN
        if "enhanced" in agent_id or "edge_aware" in agent_id:
            from models.enhanced_gnn_encoder import EnhancedEdgeAwareGNN
            
            # æ›¿æ¢ä¸ºå¢å¼ºGNNç¼–ç å™¨
            self.gnn_encoder = EnhancedEdgeAwareGNN(
                node_dim=state_dim,
                edge_dim=edge_dim,
                hidden_dim=self.hidden_dim,
                output_dim=self.output_dim,
                num_layers=self.num_layers,
                vnf_context_dim=config.get('dimensions', {}).get('vnf_context_dim', 6)
            ).to(self.device)
            
            print(f"ğŸ”„ {agent_id}: å·²å‡çº§ä¸ºå¢å¼ºGNNç¼–ç å™¨")
    
    # åº”ç”¨è¡¥ä¸
    MultiDDQNAgent.__init__ = enhanced_init
    
    # ä¿®è¡¥DQNæ™ºèƒ½ä½“
    from agents.multi_dqn_agent import MultiDQNAgent
    
    original_dqn_init = MultiDQNAgent.__init__
    
    def enhanced_dqn_init(self, agent_id, state_dim, action_dim, edge_dim, config):
        original_dqn_init(self, agent_id, state_dim, action_dim, edge_dim, config)
        
        if "enhanced" in agent_id or "edge_aware" in agent_id:
            from models.enhanced_gnn_encoder import EnhancedEdgeAwareGNN
            
            self.gnn_encoder = EnhancedEdgeAwareGNN(
                node_dim=state_dim,
                edge_dim=edge_dim,
                hidden_dim=self.hidden_dim,
                output_dim=self.output_dim,
                num_layers=self.num_layers,
                vnf_context_dim=config.get('dimensions', {}).get('vnf_context_dim', 6)
            ).to(self.device)
            
            print(f"ğŸ”„ {agent_id}: å·²å‡çº§ä¸ºå¢å¼ºGNNç¼–ç å™¨")
    
    MultiDQNAgent.__init__ = enhanced_dqn_init
    
    # ä¿®è¡¥PPOæ™ºèƒ½ä½“
    from agents.multi_ppo_agent import MultiPPOAgent
    
    original_ppo_init = MultiPPOAgent.__init__
    
    def enhanced_ppo_init(self, agent_id, state_dim, action_dim, edge_dim, config):
        original_ppo_init(self, agent_id, state_dim, action_dim, edge_dim, config)
        
        if "enhanced" in agent_id or "edge_aware" in agent_id:
            from models.enhanced_gnn_encoder import EnhancedEdgeAwareGNN
            
            self.gnn_encoder = EnhancedEdgeAwareGNN(
                node_dim=state_dim,
                edge_dim=edge_dim,
                hidden_dim=self.hidden_dim,
                output_dim=self.output_dim,
                num_layers=self.num_layers,
                vnf_context_dim=config.get('dimensions', {}).get('vnf_context_dim', 6)
            ).to(self.device)
            
            print(f"ğŸ”„ {agent_id}: å·²å‡çº§ä¸ºå¢å¼ºGNNç¼–ç å™¨")
    
    MultiPPOAgent.__init__ = enhanced_ppo_init
    
    print("âœ… ç°æœ‰æ™ºèƒ½ä½“å¢å¼ºGNNè¡¥ä¸åº”ç”¨å®Œæˆ")


# æµ‹è¯•å‡½æ•°
def test_enhanced_agent_integration():
    """æµ‹è¯•å¢å¼ºæ™ºèƒ½ä½“é›†æˆ"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºæ™ºèƒ½ä½“é›†æˆ...")
    
    # åº”ç”¨è¡¥ä¸
    patch_existing_agents()
    
    # æµ‹è¯•é…ç½®
    config = {
        "gnn": {
            "edge_aware": {"hidden_dim": 128, "output_dim": 256, "layers": 4},
            "baseline": {"hidden_dim": 64, "output_dim": 256, "layers": 4}
        },
        "train": {"lr": 0.001, "gamma": 0.99, "batch_size": 16},
        "dimensions": {"vnf_context_dim": 6}
    }
    
    # æµ‹è¯•å¢å¼ºæ™ºèƒ½ä½“åˆ›å»º
    from agents.multi_ddqn_agent import MultiDDQNAgent
    
    # åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“
    enhanced_agent = MultiDDQNAgent(
        "ddqn_edge_aware_enhanced", 
        state_dim=8, 
        action_dim=10, 
        edge_dim=4, 
        config=config
    )
    
    # åˆ›å»ºæ ‡å‡†ç‰ˆæ™ºèƒ½ä½“
    baseline_agent = MultiDDQNAgent(
        "ddqn_baseline", 
        state_dim=8, 
        action_dim=10, 
        edge_dim=4, 
        config=config
    )
    
    print(f"âœ… å¢å¼ºæ™ºèƒ½ä½“ç±»å‹: {type(enhanced_agent.gnn_encoder).__name__}")
    print(f"âœ… æ ‡å‡†æ™ºèƒ½ä½“ç±»å‹: {type(baseline_agent.gnn_encoder).__name__}")
    
    # æµ‹è¯•çŠ¶æ€å¤„ç†
    test_state = torch.randn(1, 256)  # æ¨¡æ‹Ÿå¤„ç†åçš„çŠ¶æ€
    
    try:
        enhanced_output = enhanced_agent.process_state(test_state)
        baseline_output = baseline_agent.process_state(test_state)
        
        print(f"âœ… å¢å¼ºæ™ºèƒ½ä½“è¾“å‡º: {enhanced_output.shape}")
        print(f"âœ… æ ‡å‡†æ™ºèƒ½ä½“è¾“å‡º: {baseline_output.shape}")
        
    except Exception as e:
        print(f"âŒ çŠ¶æ€å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
    
    print("âœ… å¢å¼ºæ™ºèƒ½ä½“é›†æˆæµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_enhanced_agent_integration()