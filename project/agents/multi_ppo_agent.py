# agents/multi_ppo_agent.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Union, List, Dict, Any, Tuple
from torch_geometric.data import Data
from collections import namedtuple
from agents.enhanced_base_agent import EnhancedBaseAgent

# PPOç»éªŒæ•°æ®ç»“æ„
PPOExperience = namedtuple('PPOExperience', [
    'state', 'action', 'reward', 'next_state', 'done', 
    'log_prob', 'value', 'advantage', 'return_'
])

class PPONetwork(nn.Module):
    """
    PPOç½‘ç»œ - Actor-Criticæ¶æ„
    
    åŒ…å«ï¼š
    1. Actorç½‘ç»œï¼šè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
    2. Criticç½‘ç»œï¼šè¾“å‡ºçŠ¶æ€ä»·å€¼å‡½æ•°
    """
    def __init__(self, input_dim: int, action_dim: int, hidden_dim: int = 512):
        super().__init__()
        self.shared_layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        self.actor = nn.Linear(hidden_dim, action_dim)
        self.critic = nn.Linear(hidden_dim, 1)
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Xavieråˆå§‹åŒ–"""
        # å¤„ç† shared_layersï¼ˆnn.Sequentialï¼‰
        for layer in self.shared_layers:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.01)
        # å¤„ç† actor å’Œ criticï¼ˆnn.Linearï¼‰
        for layer in [self.actor, self.critic]:
            nn.init.xavier_uniform_(layer.weight)
            nn.init.constant_(layer.bias, 0.01)
    
    def forward(self, state_embedding: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            state_embedding: GNNç¼–ç åçš„çŠ¶æ€ [batch_size, input_dim]
            
        Returns:
            logits: åŠ¨ä½œlogits [batch_size, action_dim]
            values: çŠ¶æ€ä»·å€¼ [batch_size, 1]
        """
        shared_features = self.shared_layers(state_embedding)
        logits = self.actor(shared_features)
        values = self.critic(shared_features)
        return logits, values
    
    def get_action_and_value(self, state_embedding: torch.Tensor, action: torch.Tensor = None):
        """
        è·å–åŠ¨ä½œæ¦‚ç‡å’Œä»·å€¼
        
        Args:
            state_embedding: çŠ¶æ€ç¼–ç 
            action: åŠ¨ä½œï¼ˆå¯é€‰ï¼Œç”¨äºè®¡ç®—ç‰¹å®šåŠ¨ä½œçš„logæ¦‚ç‡ï¼‰
            
        Returns:
            action: é‡‡æ ·çš„åŠ¨ä½œ
            log_prob: åŠ¨ä½œçš„logæ¦‚ç‡
            entropy: ç­–ç•¥ç†µ
            value: çŠ¶æ€ä»·å€¼
        """
        logits, values = self.forward(state_embedding)
        probs = F.softmax(logits, dim=-1)
        log_probs = F.log_softmax(logits, dim=-1)
        dist = torch.distributions.Categorical(probs)
        
        if action is None:
            action = dist.sample()
        
        action_log_prob = dist.log_prob(action)
        entropy = dist.entropy()
        
        return action, action_log_prob, entropy, values.squeeze(-1)

class MultiPPOAgent(EnhancedBaseAgent):
    """
    å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ™ºèƒ½ä½“
    
    ç‰¹æ€§ï¼š
    1. Actor-Criticæ¶æ„
    2. è£å‰ªç­–ç•¥æŸå¤±é˜²æ­¢è¿‡å¤§æ›´æ–°
    3. å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰
    4. ä½¿ç”¨GNNç¼–ç å™¨å¤„ç†å›¾çŠ¶æ€ï¼ˆæ”¯æŒå¢å¼ºGNNï¼‰
    """
    
    def __init__(self, agent_id: str, state_dim: int, action_dim: int, edge_dim: int, config: Dict[str, Any], use_enhanced_gnn: bool = False):
        super().__init__(agent_id, state_dim, action_dim, edge_dim, config, use_enhanced_gnn)
        
        # PPOç‰¹å®šé…ç½®
        self.clip_epsilon = config.get("train", {}).get("eps_clip", 0.2)
        self.entropy_coef = config.get("train", {}).get("entropy_coef", 0.01)
        self.value_coef = config.get("train", {}).get("value_coef", 0.5)
        self.max_grad_norm = config.get("train", {}).get("max_grad_norm", 0.5)
        self.gae_lambda = config.get("train", {}).get("gae_lambda", 0.95)
        self.ppo_epochs = config.get("train", {}).get("ppo_epochs", 4)
        self.mini_batch_size = config.get("train", {}).get("mini_batch_size", 64)
        
        # ç½‘ç»œæ¶æ„
        network_input_dim = self.output_dim
        
        # PPOç½‘ç»œ
        self.policy_network = PPONetwork(
            input_dim=network_input_dim,
            action_dim=action_dim,
            hidden_dim=config.get("network", {}).get("hidden_dim", 512)
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            list(self.gnn_encoder.parameters()) + list(self.policy_network.parameters()),
            lr=self.learning_rate,
            eps=1e-5
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(
            self.optimizer, step_size=1000, gamma=0.95
        )
        
        # ç»éªŒå­˜å‚¨
        self.experiences = []
        self.rollout_length = config.get("train", {}).get("rollout_length", 128)
        
        # PPOç»Ÿè®¡
        self.ppo_stats = {
            "policy_loss": [],
            "value_loss": [],
            "entropy": []
        }
        
        print(f"ğŸš€ PPO Agent {agent_id} åˆå§‹åŒ–å®Œæˆï¼ˆå¢å¼ºæ¨¡å¼: {use_enhanced_gnn}ï¼‰")
        print(f"   - çŠ¶æ€ç»´åº¦: {state_dim} -> GNNç¼–ç  -> {network_input_dim}")
        print(f"   - åŠ¨ä½œç»´åº¦: {action_dim}")
        print(f"   - è£å‰ªå‚æ•°: {self.clip_epsilon}")
        print(f"   - è®¾å¤‡: {self.device}")
    
    def select_action(self, state: Union[Data, Dict], valid_actions: List[int] = None, **kwargs) -> int:
        state_embedding = self.process_state(state)
        if state_embedding.dim() == 1:
            state_embedding = state_embedding.unsqueeze(0)
        
        if valid_actions is None:
            valid_actions = self.get_valid_actions(state, **kwargs)
        
        self.policy_network.eval()
        with torch.no_grad():
            logits, values = self.policy_network(state_embedding)
            masked_logits = self._mask_invalid_logits(logits, valid_actions)
            probs = F.softmax(masked_logits, dim=-1)
            dist = torch.distributions.Categorical(probs)
            action = dist.sample()
            
            if self.is_training:
                log_prob = dist.log_prob(action)
                self._store_action_info(state, action.item(), log_prob.item(), values.item())
        
        if self.is_training:
            self.policy_network.train()
        
        return action.item()
    
    def _mask_invalid_logits(self, logits: torch.Tensor, valid_actions: List[int]) -> torch.Tensor:
        """å±è”½æ— æ•ˆåŠ¨ä½œçš„logits"""
        masked_logits = logits.clone()
        invalid_actions = [a for a in range(self.action_dim) if a not in valid_actions]
        if invalid_actions:
            masked_logits[:, invalid_actions] = -float('inf')
        return masked_logits
    
    def _store_action_info(self, state, action: int, log_prob: float, value: float):
        """å­˜å‚¨åŠ¨ä½œé€‰æ‹©æ—¶çš„ä¿¡æ¯"""
        self._last_action_info = {
            'state': state,
            'action': action,
            'log_prob': log_prob,
            'value': value
        }
    
    def get_valid_actions(self, state: Union[Data, Dict], **kwargs) -> List[int]:
        """è·å–VNFåµŒå…¥åœºæ™¯ä¸‹çš„æœ‰æ•ˆåŠ¨ä½œ"""
        available_nodes = kwargs.get('available_nodes', list(range(self.action_dim)))
        resource_constraints = kwargs.get('resource_constraints', {})
        
        valid_actions = []
        for node in available_nodes:
            if self._check_node_feasibility(node, resource_constraints):
                valid_actions.append(node)
        
        if not valid_actions:
            valid_actions = [0]
        
        return valid_actions
    
    def _check_node_feasibility(self, node_id: int, constraints: Dict) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ»¡è¶³VNFåµŒå…¥çº¦æŸ"""
        return True
    
    def store_transition(self, state, action, reward, next_state, done, **kwargs):
        """
        å­˜å‚¨ç»éªŒï¼ˆPPOä½¿ç”¨on-policyå­¦ä¹ ï¼‰
        """
        if hasattr(self, '_last_action_info'):
            log_prob = self._last_action_info['log_prob']
            value = self._last_action_info['value']
        else:
            with torch.no_grad():
                if isinstance(state, Data):
                    state_emb = self.process_state(state)
                else:
                    state_emb = torch.tensor(state, device=self.device).unsqueeze(0)
                
                _, log_prob, _, value = self.policy_network.get_action_and_value(
                    state_emb, torch.tensor([action], device=self.device)
                )
                log_prob = log_prob.item()
                value = value.item()
        
        experience = PPOExperience(
            state=state,
            action=action,
            reward=float(reward),
            next_state=next_state,
            done=bool(done),
            log_prob=log_prob,
            value=value,
            advantage=0.0,
            return_=0.0
        )
        
        self.experiences.append(experience)
    
    def _compute_gae(self):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰"""
        if len(self.experiences) == 0:
            return
        
        returns = []
        advantages = []
        
        if not self.experiences[-1].done:
            with torch.no_grad():
                if isinstance(self.experiences[-1].next_state, Data):
                    last_state_emb = self.process_state(self.experiences[-1].next_state)
                else:
                    last_state_emb = torch.tensor(self.experiences[-1].next_state, device=self.device).unsqueeze(0)
                
                _, last_value = self.policy_network(last_state_emb)
                last_value = last_value.item()
        else:
            last_value = 0.0
        
        gae = 0.0
        for i in reversed(range(len(self.experiences))):
            exp = self.experiences[i]
            
            if i == len(self.experiences) - 1:
                next_value = last_value
            else:
                next_value = self.experiences[i + 1].value
            
            delta = exp.reward + self.gamma * next_value * (1 - exp.done) - exp.value
            gae = delta + self.gamma * self.gae_lambda * (1 - exp.done) * gae
            return_ = gae + exp.value
            
            returns.insert(0, return_)
            advantages.insert(0, gae)
        
        advantages_tensor = torch.tensor(advantages, device=self.device)
        if len(advantages_tensor) > 1:
            advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)
            advantages = advantages_tensor.tolist()
        
        for i, exp in enumerate(self.experiences):
            self.experiences[i] = exp._replace(
                advantage=advantages[i],
                return_=returns[i]
            )
    
    def update_stats(self, reward: float, action: int, loss: float, **kwargs):
        """
        æ›´æ–° PPO æ™ºèƒ½ä½“çš„ç»Ÿè®¡ä¿¡æ¯
        """
        super().update_stats(reward=reward, action=action, loss=loss)
        
        if not hasattr(self, 'ppo_stats'):
            self.ppo_stats = {
                "policy_loss": [],
                "value_loss": [],
                "entropy": []
            }
        
        if "policy_loss" in kwargs:
            self.ppo_stats["policy_loss"].append(kwargs["policy_loss"])
        if "value_loss" in kwargs:
            self.ppo_stats["value_loss"].append(kwargs["value_loss"])
        if "entropy" in kwargs:
            self.ppo_stats["entropy"].append(kwargs["entropy"])
    
    def learn(self) -> Dict[str, float]:
        if len(self.experiences) < self.mini_batch_size:
            return {"loss": 0.0, "policy_loss": 0.0, "value_loss": 0.0, "entropy": 0.0}
        
        self._compute_gae()
        
        states = [exp.state for exp in self.experiences]
        actions = [exp.action for exp in self.experiences]
        old_log_probs = [exp.log_prob for exp in self.experiences]
        advantages = [exp.advantage for exp in self.experiences]
        returns = [exp.return_ for exp in self.experiences]
        
        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)
        old_log_probs_tensor = torch.tensor(old_log_probs, dtype=torch.float32, device=self.device)
        advantages_tensor = torch.tensor(advantages, dtype=torch.float32, device=self.device)
        returns_tensor = torch.tensor(returns, dtype=torch.float32, device=self.device)
        
        total_policy_loss = 0.0
        total_value_loss = 0.0
        total_entropy = 0.0
        batch_size = len(self.experiences)
        
        for epoch in range(self.ppo_epochs):
            indices = torch.randperm(batch_size, device=self.device)
            
            for start_idx in range(0, batch_size, self.mini_batch_size):
                end_idx = min(start_idx + self.mini_batch_size, batch_size)
                batch_indices = indices[start_idx:end_idx]
                
                batch_actions = actions_tensor[batch_indices]
                batch_old_log_probs = old_log_probs_tensor[batch_indices]
                batch_advantages = advantages_tensor[batch_indices]
                batch_returns = returns_tensor[batch_indices]
                
                batch_states = [states[i] for i in batch_indices.tolist()]
                if all(isinstance(s, Data) for s in batch_states):
                    from torch_geometric.data import Batch
                    states_batch = Batch.from_data_list(batch_states).to(self.device)
                    batch_state_embeddings = self.gnn_encoder(states_batch)
                else:
                    states_tensor = torch.stack([torch.tensor(s, dtype=torch.float32) if not isinstance(s, torch.Tensor) else s 
                                            for s in batch_states]).to(self.device)
                    batch_state_embeddings = states_tensor
                
                _, new_log_probs, entropy, values = self.policy_network.get_action_and_value(
                    batch_state_embeddings, batch_actions
                )
                
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                value_loss = F.mse_loss(values, batch_returns)
                
                total_loss = (policy_loss + 
                            self.value_coef * value_loss - 
                            self.entropy_coef * entropy.mean())
                
                self.optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    list(self.gnn_encoder.parameters()) + list(self.policy_network.parameters()),
                    max_norm=self.max_grad_norm
                )
                
                self.optimizer.step()
                
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_entropy += entropy.mean().item()
        
        self.lr_scheduler.step()
        self.training_step += 1
        
        avg_reward = np.mean([exp.reward for exp in self.experiences])
        first_action = actions[0] if actions else 0
        self.experiences.clear()
        
        self.update_stats(
            reward=avg_reward,
            action=first_action,
            loss=total_policy_loss,
            policy_loss=total_policy_loss,
            value_loss=total_value_loss,
            entropy=total_entropy
        )
        
        return {
            "loss": total_policy_loss / max(self.ppo_epochs, 1),
            "policy_loss": total_policy_loss / max(self.ppo_epochs, 1),
            "value_loss": total_value_loss / max(self.ppo_epochs, 1),
            "entropy": total_entropy / max(self.ppo_epochs, 1),
            "lr": self.lr_scheduler.get_last_lr()[0]
        }
    
    def should_update(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡ŒPPOæ›´æ–°"""
        return len(self.experiences) >= self.rollout_length
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'gnn_encoder': self.gnn_encoder.state_dict(),
            'policy_network': self.policy_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'training_step': self.training_step,
        }, filepath)
        print(f"ğŸ’¾ PPOæ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.gnn_encoder.load_state_dict(checkpoint['gnn_encoder'])
        self.policy_network.load_state_dict(checkpoint['policy_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.training_step = checkpoint['training_step']
        print(f"ğŸ“‚ PPOæ¨¡å‹å·²åŠ è½½: {filepath}")

def test_ppo_agent():
    print("ğŸ§ª æµ‹è¯•PPOæ™ºèƒ½ä½“ï¼ˆå¢å¼ºç‰ˆï¼‰...")
    
    from config_loader import get_scenario_config
    config = get_scenario_config('normal_operation')
    
    agent = MultiPPOAgent(
        agent_id="test_ppo_enhanced",
        state_dim=config['dimensions']['node_feature_dim'],
        action_dim=config['topology']['node_counts']['total'],
        edge_dim=config['dimensions']['edge_feature_dim_full'],
        config=config,
        use_enhanced_gnn=True
    )
    
    # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
    test_state = Data(
        x=torch.randn(config['topology']['node_counts']['total'], config['dimensions']['node_feature_dim']),
        edge_index=torch.randint(0, config['topology']['node_counts']['total'], (2, 100)),
        edge_attr=torch.randn(100, config['dimensions']['edge_feature_dim_full']),
        vnf_context=torch.randn(config['dimensions']['vnf_context_dim'])
    )
    valid_actions = list(range(config['topology']['node_counts']['total']))
    action = agent.select_action(test_state, valid_actions=valid_actions)
    print(f"âœ… åŠ¨ä½œé€‰æ‹©æµ‹è¯•: {action}")
    
    # æµ‹è¯•ç»éªŒå­˜å‚¨å’Œå­¦ä¹ 
    for i in range(40):
        state = Data(
            x=torch.randn(config['topology']['node_counts']['total'], config['dimensions']['node_feature_dim']),
            edge_index=torch.randint(0, config['topology']['node_counts']['total'], (2, 100)),
            edge_attr=torch.randn(100, config['dimensions']['edge_feature_dim_full']),
            vnf_context=torch.randn(config['dimensions']['vnf_context_dim'])
        )
        action = i % config['topology']['node_counts']['total']
        reward = np.random.random()
        next_state = Data(
            x=torch.randn(config['topology']['node_counts']['total'], config['dimensions']['node_feature_dim']),
            edge_index=torch.randint(0, config['topology']['node_counts']['total'], (2, 100)),
            edge_attr=torch.randn(100, config['dimensions']['edge_feature_dim_full']),
            vnf_context=torch.randn(config['dimensions']['vnf_context_dim'])
        )
        done = (i % 20 == 0)
        
        agent.store_transition(state, action, reward, next_state, done)
        
        if agent.should_update():
            learning_info = agent.learn()
            print(f"âœ… PPOå­¦ä¹ æ›´æ–°: Policy Loss={learning_info['policy_loss']:.4f}, "
                  f"Value Loss={learning_info['value_loss']:.4f}, "
                  f"Entropy={learning_info['entropy']:.4f}")
            break
    
    print("âœ… å¢å¼ºPPOæ™ºèƒ½ä½“æµ‹è¯•å®Œæˆ!")