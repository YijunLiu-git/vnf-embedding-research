# agents/multi_ppo_agent.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Union, List, Dict, Any, Tuple
from torch_geometric.data import Data
from collections import namedtuple

from agents.base_agent import BaseAgent

# PPOç»éªŒæ•°æ®ç»“æ„
PPOExperience = namedtuple('PPOExperience', [
    'state', 'action', 'reward', 'next_state', 'done', 
    'log_prob', 'value', 'advantage', 'return_'
])

class PPONetwork(nn.Module):
    """
    PPOç½‘ç»œ - Actor-Criticæ¶æ„
    
    åŒ…å«ï¼š
    1. Actorç½‘ç»œï¼šè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
    2. Criticç½‘ç»œï¼šè¾“å‡ºçŠ¶æ€ä»·å€¼å‡½æ•°
    """
    
    def __init__(self, input_dim: int, action_dim: int, hidden_dim: int = 512):
        super(PPONetwork, self).__init__()
        
        # å…±äº«å±‚
        self.shared_layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Actorç½‘ç»œ - è¾“å‡ºåŠ¨ä½œæ¦‚ç‡
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        # Criticç½‘ç»œ - è¾“å‡ºçŠ¶æ€ä»·å€¼
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Xavieråˆå§‹åŒ–"""
        for module in [self.shared_layers, self.actor, self.critic]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.constant_(layer.bias, 0.01)
    
    def forward(self, state_embedding: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            state_embedding: GNNç¼–ç åçš„çŠ¶æ€ [batch_size, input_dim]
            
        Returns:
            logits: åŠ¨ä½œlogits [batch_size, action_dim]
            values: çŠ¶æ€ä»·å€¼ [batch_size, 1]
        """
        shared_features = self.shared_layers(state_embedding)
        
        # Actorè¾“å‡º
        logits = self.actor(shared_features)
        
        # Criticè¾“å‡º
        values = self.critic(shared_features)
        
        return logits, values
    
    def get_action_and_value(self, state_embedding: torch.Tensor, action: torch.Tensor = None):
        """
        è·å–åŠ¨ä½œæ¦‚ç‡å’Œä»·å€¼ï¼ˆç”¨äºè®­ç»ƒï¼‰
        
        Args:
            state_embedding: çŠ¶æ€ç¼–ç 
            action: åŠ¨ä½œï¼ˆå¯é€‰ï¼Œç”¨äºè®¡ç®—ç‰¹å®šåŠ¨ä½œçš„logæ¦‚ç‡ï¼‰
            
        Returns:
            action: é‡‡æ ·çš„åŠ¨ä½œ
            log_prob: åŠ¨ä½œçš„logæ¦‚ç‡
            entropy: ç­–ç•¥ç†µ
            value: çŠ¶æ€ä»·å€¼
        """
        logits, values = self.forward(state_embedding)
        
        # åˆ›å»ºåŠ¨ä½œåˆ†å¸ƒ
        probs = F.softmax(logits, dim=-1)
        log_probs = F.log_softmax(logits, dim=-1)
        
        dist = torch.distributions.Categorical(probs)
        
        if action is None:
            # é‡‡æ ·åŠ¨ä½œ
            action = dist.sample()
        
        # è®¡ç®—logæ¦‚ç‡å’Œç†µ
        action_log_prob = dist.log_prob(action)
        entropy = dist.entropy()
        
        return action, action_log_prob, entropy, values.squeeze(-1)


class MultiPPOAgent(BaseAgent):
    """
    å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ™ºèƒ½ä½“
    
    ç‰¹æ€§ï¼š
    1. Actor-Criticæ¶æ„
    2. è£å‰ªç­–ç•¥æŸå¤±é˜²æ­¢è¿‡å¤§æ›´æ–°
    3. å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰
    4. ä½¿ç”¨GNNç¼–ç å™¨å¤„ç†å›¾çŠ¶æ€
    """
    
    def __init__(self, agent_id: str, state_dim: int, action_dim: int, edge_dim: int, config: Dict[str, Any]):
        super().__init__(agent_id, state_dim, action_dim, edge_dim, config)
        
        # PPOç‰¹å®šé…ç½®
        self.clip_epsilon = config.get("train", {}).get("eps_clip", 0.2)
        self.entropy_coef = config.get("train", {}).get("entropy_coef", 0.01)
        self.value_coef = config.get("train", {}).get("value_coef", 0.5)
        self.max_grad_norm = config.get("train", {}).get("max_grad_norm", 0.5)
        
        # GAEå‚æ•°
        self.gae_lambda = config.get("train", {}).get("gae_lambda", 0.95)
        
        # æ›´æ–°å‚æ•°
        self.ppo_epochs = config.get("train", {}).get("ppo_epochs", 4)
        self.mini_batch_size = config.get("train", {}).get("mini_batch_size", 64)
        
        # ç½‘ç»œæ¶æ„
        network_input_dim = self.output_dim  # GNNEncoderçš„è¾“å‡ºç»´åº¦
        
        # PPOç½‘ç»œï¼ˆActor-Criticï¼‰
        self.policy_network = PPONetwork(
            input_dim=network_input_dim,
            action_dim=action_dim,
            hidden_dim=config.get("network", {}).get("hidden_dim", 512)
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            list(self.gnn_encoder.parameters()) + list(self.policy_network.parameters()),
            lr=self.learning_rate,
            eps=1e-5
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(
            self.optimizer, step_size=1000, gamma=0.95
        )
        
        # ç»éªŒå­˜å‚¨ï¼ˆPPOä½¿ç”¨on-policyå­¦ä¹ ï¼‰
        self.experiences = []
        self.rollout_length = config.get("train", {}).get("rollout_length", 128)
        
        # PPOç‰¹å®šç»Ÿè®¡
        self.ppo_stats = {
            "policy_loss": [],
            "value_loss": [],
            "entropy": []
        }
        
        print(f"ğŸš€ PPO Agent {agent_id} åˆå§‹åŒ–å®Œæˆ")
        print(f"   - çŠ¶æ€ç»´åº¦: {state_dim} -> GNNç¼–ç  -> {network_input_dim}")
        print(f"   - åŠ¨ä½œç»´åº¦: {action_dim}")
        print(f"   - è£å‰ªå‚æ•°: {self.clip_epsilon}")
        print(f"   - è®¾å¤‡: {self.device}")
    
    def select_action(self, state: Union[Data, Dict], valid_actions: List[int] = None, **kwargs) -> int:
        """
        é€‰æ‹©åŠ¨ä½œ - ç­–ç•¥é‡‡æ ·
        
        Args:
            state: å½“å‰çŠ¶æ€ï¼ˆå›¾æ•°æ®æˆ–ç¼–ç åçŠ¶æ€ï¼‰
            valid_actions: æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
        """
        
        # å¤„ç†çŠ¶æ€
        if isinstance(state, Data):
            state_embedding = self.process_state(state)
        else:
            state_embedding = state.to(self.device) if isinstance(state, torch.Tensor) else torch.tensor(state, device=self.device)
            if state_embedding.dim() == 1:
                state_embedding = state_embedding.unsqueeze(0)
        
        # è·å–æœ‰æ•ˆåŠ¨ä½œ
        if valid_actions is None:
            valid_actions = self.get_valid_actions(state, **kwargs)
        
        self.policy_network.eval()
        with torch.no_grad():
            logits, values = self.policy_network(state_embedding)
            
            # å±è”½æ— æ•ˆåŠ¨ä½œ
            masked_logits = self._mask_invalid_logits(logits, valid_actions)
            
            # åˆ›å»ºåŠ¨ä½œåˆ†å¸ƒå¹¶é‡‡æ ·
            probs = F.softmax(masked_logits, dim=-1)
            dist = torch.distributions.Categorical(probs)
            action = dist.sample()
            
            # å­˜å‚¨ç”¨äºè®­ç»ƒçš„ä¿¡æ¯
            if self.is_training:
                log_prob = dist.log_prob(action)
                self._store_action_info(state, action.item(), log_prob.item(), values.item())
        
        if self.is_training:
            self.policy_network.train()
        
        return action.item()
    
    def _mask_invalid_logits(self, logits: torch.Tensor, valid_actions: List[int]) -> torch.Tensor:
        """å±è”½æ— æ•ˆåŠ¨ä½œçš„logits"""
        masked_logits = logits.clone()
        
        # å°†æ— æ•ˆåŠ¨ä½œçš„logitsè®¾ä¸ºå¾ˆå°çš„å€¼
        invalid_actions = [a for a in range(self.action_dim) if a not in valid_actions]
        if invalid_actions:
            masked_logits[:, invalid_actions] = -float('inf')
        
        return masked_logits
    
    def _store_action_info(self, state, action: int, log_prob: float, value: float):
        """å­˜å‚¨åŠ¨ä½œé€‰æ‹©æ—¶çš„ä¿¡æ¯"""
        # è¿™äº›ä¿¡æ¯å°†åœ¨store_transitionä¸­ä½¿ç”¨
        self._last_action_info = {
            'state': state,
            'action': action,
            'log_prob': log_prob,
            'value': value
        }
    
    def get_valid_actions(self, state: Union[Data, Dict], **kwargs) -> List[int]:
        """è·å–VNFåµŒå…¥åœºæ™¯ä¸‹çš„æœ‰æ•ˆåŠ¨ä½œ"""
        # åŸºç¡€å®ç°
        available_nodes = kwargs.get('available_nodes', list(range(self.action_dim)))
        resource_constraints = kwargs.get('resource_constraints', {})
        
        valid_actions = []
        for node in available_nodes:
            if self._check_node_feasibility(node, resource_constraints):
                valid_actions.append(node)
        
        if not valid_actions:
            valid_actions = [0]
        
        return valid_actions
    
    def _check_node_feasibility(self, node_id: int, constraints: Dict) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ»¡è¶³VNFåµŒå…¥çº¦æŸ"""
        return True
    
    def store_transition(self, state, action, reward, next_state, done, **kwargs):
        """
        å­˜å‚¨ç»éªŒï¼ˆPPOä½¿ç”¨on-policyå­¦ä¹ ï¼‰
        
        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
            **kwargs: é¢å¤–ä¿¡æ¯
        """
        
        # è·å–åŠ¨ä½œé€‰æ‹©æ—¶å­˜å‚¨çš„ä¿¡æ¯
        if hasattr(self, '_last_action_info'):
            log_prob = self._last_action_info['log_prob']
            value = self._last_action_info['value']
        else:
            # å¦‚æœæ²¡æœ‰é¢„å­˜ä¿¡æ¯ï¼Œé‡æ–°è®¡ç®—
            with torch.no_grad():
                if isinstance(state, Data):
                    state_emb = self.process_state(state)
                else:
                    state_emb = torch.tensor(state, device=self.device).unsqueeze(0)
                
                _, log_prob, _, value = self.policy_network.get_action_and_value(
                    state_emb, torch.tensor([action], device=self.device)
                )
                log_prob = log_prob.item()
                value = value.item()
        
        # å­˜å‚¨ç»éªŒ
        experience = PPOExperience(
            state=state,
            action=action,
            reward=float(reward),
            next_state=next_state,
            done=bool(done),
            log_prob=log_prob,
            value=value,
            advantage=0.0,  # ç¨åè®¡ç®—
            return_=0.0     # ç¨åè®¡ç®—
        )
        
        self.experiences.append(experience)
    
    def _compute_gae(self):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰"""
        if len(self.experiences) == 0:
            return
        
        # è®¡ç®—returnså’Œadvantages
        returns = []
        advantages = []
        
        # è·å–æœ€åä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼ˆç”¨äºbootstrapï¼‰
        if not self.experiences[-1].done:
            with torch.no_grad():
                if isinstance(self.experiences[-1].next_state, Data):
                    last_state_emb = self.process_state(self.experiences[-1].next_state)
                else:
                    last_state_emb = torch.tensor(self.experiences[-1].next_state, device=self.device).unsqueeze(0)
                
                _, last_value = self.policy_network(last_state_emb)
                last_value = last_value.item()
        else:
            last_value = 0.0
        
        # åå‘è®¡ç®—GAE
        gae = 0.0
        for i in reversed(range(len(self.experiences))):
            exp = self.experiences[i]
            
            if i == len(self.experiences) - 1:
                next_value = last_value
            else:
                next_value = self.experiences[i + 1].value
            
            # TDè¯¯å·®
            delta = exp.reward + self.gamma * next_value * (1 - exp.done) - exp.value
            
            # GAE
            gae = delta + self.gamma * self.gae_lambda * (1 - exp.done) * gae
            
            # Return = Advantage + Value
            return_ = gae + exp.value
            
            returns.insert(0, return_)
            advantages.insert(0, gae)
        
        # æ›´æ–°ç»éªŒ
        for i, exp in enumerate(self.experiences):
            self.experiences[i] = exp._replace(
                advantage=advantages[i],
                return_=returns[i]
            )
        
        # å½’ä¸€åŒ–advantages
        advantages_tensor = torch.tensor([exp.advantage for exp in self.experiences], device=self.device)
        if len(advantages_tensor) > 1:
            advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)
            for i, adv in enumerate(advantages_tensor):
                self.experiences[i] = self.experiences[i]._replace(advantage=adv.item())
    
    def update_stats(self, reward: float, action: int, loss: float, **kwargs):
        """
        æ›´æ–° PPO æ™ºèƒ½ä½“çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå…¼å®¹åŸºç±»å¹¶å­˜å‚¨ PPO ç‰¹å®šç»Ÿè®¡
        
        Args:
            reward: å¥–åŠ±å€¼
            action: åŠ¨ä½œ
            loss: æ€»æŸå¤±
            **kwargs: PPO ç‰¹å®šå‚æ•°ï¼ˆpolicy_loss, value_loss, entropyï¼‰
        """
        # è°ƒç”¨åŸºç±»çš„ update_stats æ–¹æ³•
        super().update_stats(reward=reward, action=action, loss=loss)
        
        # å­˜å‚¨ PPO ç‰¹å®šç»Ÿè®¡
        if not hasattr(self, 'ppo_stats'):
            self.ppo_stats = {
                "policy_loss": [],
                "value_loss": [],
                "entropy": []
            }
        
        if "policy_loss" in kwargs:
            self.ppo_stats["policy_loss"].append(kwargs["policy_loss"])
        if "value_loss" in kwargs:
            self.ppo_stats["value_loss"].append(kwargs["value_loss"])
        if "entropy" in kwargs:
            self.ppo_stats["entropy"].append(kwargs["entropy"])
    
    def learn(self) -> Dict[str, float]:
        if len(self.experiences) < self.mini_batch_size:
            return {"loss": 0.0, "policy_loss": 0.0, "value_loss": 0.0, "entropy": 0.0}
        
        # è®¡ç®—GAE
        self._compute_gae()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        states = [exp.state for exp in self.experiences]
        actions = [exp.action for exp in self.experiences]
        old_log_probs = [exp.log_prob for exp in self.experiences]
        advantages = [exp.advantage for exp in self.experiences]
        returns = [exp.return_ for exp in self.experiences]
        
        # è½¬æ¢ä¸ºå¼ é‡
        actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)
        old_log_probs_tensor = torch.tensor(old_log_probs, dtype=torch.float32, device=self.device)
        advantages_tensor = torch.tensor(advantages, dtype=torch.float32, device=self.device)
        returns_tensor = torch.tensor(returns, dtype=torch.float32, device=self.device)
        
        # å¤šè½®PPOæ›´æ–°
        total_policy_loss = 0.0
        total_value_loss = 0.0
        total_entropy = 0.0
        batch_size = len(self.experiences)
        
        for epoch in range(self.ppo_epochs):
            indices = torch.randperm(batch_size, device=self.device)
            
            for start_idx in range(0, batch_size, self.mini_batch_size):
                end_idx = min(start_idx + self.mini_batch_size, batch_size)
                batch_indices = indices[start_idx:end_idx]
                
                # è·å–mini-batchæ•°æ®
                batch_actions = actions_tensor[batch_indices]
                batch_old_log_probs = old_log_probs_tensor[batch_indices]
                batch_advantages = advantages_tensor[batch_indices]
                batch_returns = returns_tensor[batch_indices]
                
                # å¤„ç†çŠ¶æ€
                batch_states = [states[i] for i in batch_indices.tolist()]
                if all(isinstance(s, Data) for s in batch_states):
                    from torch_geometric.data import Batch
                    states_batch = Batch.from_data_list(batch_states).to(self.device)
                    batch_state_embeddings = self.gnn_encoder(states_batch)
                else:
                    states_tensor = torch.stack([torch.tensor(s, dtype=torch.float32) if not isinstance(s, torch.Tensor) else s 
                                            for s in batch_states]).to(self.device)
                    batch_state_embeddings = states_tensor  # ç›´æ¥ä½¿ç”¨çŠ¶æ€ï¼Œæ— éœ€ GNN
        
                # å‰å‘ä¼ æ’­
                _, new_log_probs, entropy, values = self.policy_network.get_action_and_value(
                    batch_state_embeddings, batch_actions
                )
                
                # è®¡ç®—æ¯”ç‡
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                
                # è®¡ç®—ç­–ç•¥æŸå¤±
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # è®¡ç®—ä»·å€¼æŸå¤±
                value_loss = F.mse_loss(values, batch_returns)
                
                # è®¡ç®—æ€»æŸå¤±
                total_loss = (policy_loss + 
                            self.value_coef * value_loss - 
                            self.entropy_coef * entropy.mean())
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    list(self.gnn_encoder.parameters()) + list(self.policy_network.parameters()),
                    max_norm=self.max_grad_norm
                )
                
                self.optimizer.step()
                
                # ç´¯ç§¯æŸå¤±
                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_entropy += entropy.mean().item()
        
        # æ›´æ–°å­¦ä¹ ç‡
        self.lr_scheduler.step()
        self.training_step += 1
        
        # æ¸…ç©ºç»éªŒç¼“å†²åŒº
        avg_reward = np.mean([exp.reward for exp in self.experiences])
        first_action = actions[0] if actions else 0
        self.experiences.clear()
        
        # æ›´æ–°ç»Ÿè®¡
        self.update_stats(
            reward=avg_reward,
            action=first_action,
            loss=total_policy_loss,
            policy_loss=total_policy_loss,
            value_loss=total_value_loss,
            entropy=total_entropy
        )
        
        return {
            "loss": total_policy_loss / max(self.ppo_epochs, 1),
            "policy_loss": total_policy_loss / max(self.ppo_epochs, 1),
            "value_loss": total_value_loss / max(self.ppo_epochs, 1),
            "entropy": total_entropy / max(self.ppo_epochs, 1),
            "lr": self.lr_scheduler.get_last_lr()[0]
        }
    
    def should_update(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡ŒPPOæ›´æ–°"""
        return len(self.experiences) >= self.rollout_length
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'gnn_encoder': self.gnn_encoder.state_dict(),
            'policy_network': self.policy_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'training_step': self.training_step,
        }, filepath)
        print(f"ğŸ’¾ PPOæ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.gnn_encoder.load_state_dict(checkpoint['gnn_encoder'])
        self.policy_network.load_state_dict(checkpoint['policy_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.training_step = checkpoint['training_step']
        
        print(f"ğŸ“‚ PPOæ¨¡å‹å·²åŠ è½½: {filepath}")


# æµ‹è¯•å‡½æ•°
def test_ppo_agent():
    """æµ‹è¯•PPOæ™ºèƒ½ä½“"""
    print("ğŸ§ª æµ‹è¯•PPOæ™ºèƒ½ä½“...")
    
    config = {
        "gnn": {"hidden_dim": 64, "output_dim": 128, "edge_dim": 4},
        "train": {
            "lr": 0.001, "gamma": 0.99, "batch_size": 16,
            "eps_clip": 0.2, "entropy_coef": 0.01, "value_coef": 0.5,
            "ppo_epochs": 4, "mini_batch_size": 8, "rollout_length": 32
        },
        "network": {"hidden_dim": 128}
    }
    
    agent = MultiPPOAgent("test_ppo", state_dim=128, action_dim=42, edge_dim=4, config=config)
    
    # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
    test_state = Data(
        x=torch.randn(42, 8),
        edge_index=torch.randint(0, 42, (2, 100)),
        edge_attr=torch.randn(100, 4)
    )
    valid_actions = list(range(42))
    action = agent.select_action(test_state, valid_actions=valid_actions)
    print(f"âœ… åŠ¨ä½œé€‰æ‹©æµ‹è¯•: {action}")
    
    # æµ‹è¯•ç»éªŒå­˜å‚¨å’Œå­¦ä¹ 
    for i in range(40):
        state = Data(
            x=torch.randn(42, 8),
            edge_index=torch.randint(0, 42, (2, 100)),
            edge_attr=torch.randn(100, 4)
        )
        action = i % 42
        reward = np.random.random()
        next_state = Data(
            x=torch.randn(42, 8),
            edge_index=torch.randint(0, 42, (2, 100)),
            edge_attr=torch.randn(100, 4)
        )
        done = (i % 20 == 0)
        
        agent.store_transition(state, action, reward, next_state, done)
        
        if agent.should_update():
            learning_info = agent.learn()
            print(f"âœ… PPOå­¦ä¹ æ›´æ–°: Policy Loss={learning_info['policy_loss']:.4f}, "
                  f"Value Loss={learning_info['value_loss']:.4f}, "
                  f"Entropy={learning_info['entropy']:.4f}")
            break
    
    print("âœ… PPOæ™ºèƒ½ä½“æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_ppo_agent()