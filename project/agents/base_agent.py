# agents/base_agent.py - å®Œæ•´ä¿®å¤ç‰ˆï¼šé…ç½®é©±åŠ¨ï¼Œç»´åº¦ç»Ÿä¸€

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from abc import ABC, abstractmethod
from typing import Union, List, Dict, Any, Optional
from torch_geometric.data import Data, Batch
from agents.enhanced_base_agent import EnhancedBaseAgent

# ğŸ”§ å…³é”®ä¿®å¤ï¼šå¯¼å…¥é…ç½®åŠ è½½å™¨
try:
    from config_loader import get_config_loader
    CONFIG_AVAILABLE = True
except ImportError:
    print("âš ï¸ é…ç½®åŠ è½½å™¨æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤ç»´åº¦é…ç½®")
    CONFIG_AVAILABLE = False

# æ ‡å‡†GNNç¼–ç å™¨å¯¼å…¥
from models.gnn_encoder import GNNEncoder

class BaseAgent(ABC):
    """
    å¤šæ™ºèƒ½ä½“VNFåµŒå…¥ç³»ç»Ÿçš„åŸºç¡€æ™ºèƒ½ä½“ç±» - å®Œæ•´ä¿®å¤ç‰ˆ
    
    ğŸ”§ ä¸»è¦ä¿®å¤ï¼š
    1. å®Œå…¨åŸºäºé…ç½®æ–‡ä»¶çš„ç»´åº¦ç®¡ç†
    2. æ™ºèƒ½ä½“ç±»å‹è‡ªåŠ¨è¯†åˆ«ï¼ˆEdge-aware/Baselineï¼‰
    3. è¿è¡Œæ—¶ç»´åº¦éªŒè¯
    4. å…¼å®¹æ€§ä¿è¯å’Œé”™è¯¯å¤„ç†
    """
    
    def __init__(self, 
                 agent_id: str,
                 state_dim: int, 
                 action_dim: int, 
                 edge_dim: int,
                 config: Dict[str, Any]):
        
        self.agent_id = agent_id
        self.action_dim = action_dim
        self.config = config
        
        # ğŸ”§ å…³é”®ä¿®å¤1ï¼šä»é…ç½®æ–‡ä»¶è·å–æ ‡å‡†ç»´åº¦
        self._load_dimension_config()
        
        # ğŸ”§ å…³é”®ä¿®å¤2ï¼šæ™ºèƒ½ä½“ç±»å‹è‡ªåŠ¨è¯†åˆ«
        self._detect_agent_mode()
        
        # ğŸ”§ å…³é”®ä¿®å¤3ï¼šç»´åº¦æ ‡å‡†åŒ–
        self._standardize_dimensions()
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ¤– Agent {agent_id} ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ğŸ”§ å…³é”®ä¿®å¤4ï¼šé…ç½®é©±åŠ¨çš„GNNé…ç½®è·å–
        self._load_gnn_config()
        
        # è®­ç»ƒé…ç½®
        self._load_training_config()
        
        # ğŸ”§ å…³é”®ä¿®å¤5ï¼šåˆ›å»ºé…ç½®åŒ¹é…çš„GNNç¼–ç å™¨
        self._create_gnn_encoder()
        
        # ç­–ç•¥ç½‘ç»œï¼ˆå­ç±»å®ç°å…·ä½“ç»“æ„ï¼‰
        self.policy_network = None
        self.target_network = None  # DQNç³»åˆ—ä½¿ç”¨
        self.optimizer = None
        
        # è®­ç»ƒçŠ¶æ€
        self.training_step = 0
        self.episode_count = 0
        self.is_training = True
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_reward": 0.0,
            "episodes": 0,
            "steps": 0,
            "losses": [],
            "q_values": [],
            "actions_taken": {}
        }
        
        # å¤šæ™ºèƒ½ä½“åè°ƒï¼ˆé¢„ç•™ï¼‰
        self.other_agents = {}
        self.communication_enabled = False
        
        print(f"âœ… BaseAgent {agent_id} åˆå§‹åŒ–å®Œæˆ")
        self._print_dimension_summary()
        
    def _load_dimension_config(self):
        """åŠ è½½ç»´åº¦é…ç½®"""
        if CONFIG_AVAILABLE:
            try:
                config_loader = get_config_loader()
                self.dimensions = config_loader.config.get('dimensions', {})
                print(f"âœ… ä»é…ç½®æ–‡ä»¶åŠ è½½ç»´åº¦é…ç½®")
            except Exception as e:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                self.dimensions = self._get_default_dimensions()
        else:
            self.dimensions = self._get_default_dimensions()
    
    def _get_default_dimensions(self):
        """è·å–é»˜è®¤ç»´åº¦é…ç½®ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰"""
        return {
            'node_feature_dim': 8,
            'edge_feature_dim_full': 4,
            'edge_feature_dim_baseline': 2,
            'vnf_context_dim': 6,
            'gnn_output_dim': 256
        }
    
    def _detect_agent_mode(self):
        """è‡ªåŠ¨æ£€æµ‹æ™ºèƒ½ä½“æ¨¡å¼"""
        agent_id_lower = self.agent_id.lower()
        
        # ğŸ”§ æ™ºèƒ½ä½“ç±»å‹æ£€æµ‹é€»è¾‘
        if any(keyword in agent_id_lower for keyword in ['edge_aware', 'edge-aware', 'enhanced']):
            self.agent_mode = 'edge_aware'
            self.is_edge_aware = True
        elif any(keyword in agent_id_lower for keyword in ['baseline', 'base', 'standard']):
            self.agent_mode = 'baseline'
            self.is_edge_aware = False
        else:
            # é»˜è®¤æ ¹æ®é…ç½®åˆ¤æ–­
            self.agent_mode = 'edge_aware' if self.config.get('edge_aware_mode', True) else 'baseline'
            self.is_edge_aware = self.agent_mode == 'edge_aware'
        
        print(f"ğŸ”§ æ™ºèƒ½ä½“æ¨¡å¼æ£€æµ‹: {self.agent_id} -> {self.agent_mode}")
    
    def _standardize_dimensions(self):
        """æ ‡å‡†åŒ–ç»´åº¦é…ç½®"""
        # ğŸ”§ å¼ºåˆ¶ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç»´åº¦ï¼Œå¿½ç•¥ä¼ å…¥å‚æ•°
        self.state_dim = self.dimensions.get('node_feature_dim', 8)
        self.output_dim = self.dimensions.get('gnn_output_dim', 256)
        
        # æ ¹æ®æ™ºèƒ½ä½“æ¨¡å¼ç¡®å®šè¾¹ç‰¹å¾ç»´åº¦
        if self.is_edge_aware:
            self.edge_dim = self.dimensions.get('edge_feature_dim_full', 4)
        else:
            self.edge_dim = self.dimensions.get('edge_feature_dim_baseline', 2)
        
        self.vnf_context_dim = self.dimensions.get('vnf_context_dim', 6)
        
        print(f"ğŸ”§ ç»´åº¦æ ‡å‡†åŒ–å®Œæˆ:")
        print(f"   èŠ‚ç‚¹ç‰¹å¾: {self.state_dim}ç»´")
        print(f"   è¾¹ç‰¹å¾: {self.edge_dim}ç»´ ({self.agent_mode})")
        print(f"   VNFä¸Šä¸‹æ–‡: {self.vnf_context_dim}ç»´")
        print(f"   GNNè¾“å‡º: {self.output_dim}ç»´")
    
    def _load_gnn_config(self):
        """åŠ è½½GNNé…ç½®"""
        gnn_config = self.config.get("gnn", {}).get(self.agent_mode, {})
        
        self.hidden_dim = gnn_config.get("hidden_dim", 128 if self.is_edge_aware else 64)
        self.num_layers = gnn_config.get("layers", 6 if self.is_edge_aware else 4)
        self.dropout = gnn_config.get("dropout", 0.1)
        self.heads = gnn_config.get("heads", 4)
        
        print(f"ğŸ”§ GNNé…ç½®åŠ è½½: hidden_dim={self.hidden_dim}, layers={self.num_layers}")
    
    def _load_training_config(self):
        """åŠ è½½è®­ç»ƒé…ç½®"""
        train_config = self.config.get("train", {})
        
        self.learning_rate = train_config.get("lr", 0.0003)
        self.gamma = train_config.get("gamma", 0.99)
        self.batch_size = train_config.get("batch_size", 32)
        
        # æ¢ç´¢é…ç½®
        self.epsilon = train_config.get("epsilon_start", 1.0)
        self.epsilon_decay = train_config.get("epsilon_decay", 0.998)
        self.epsilon_min = train_config.get("epsilon_min", 0.05)
    
    def _create_gnn_encoder(self):
        """åˆ›å»ºé…ç½®åŒ¹é…çš„GNNç¼–ç å™¨"""
        print(f"ğŸ”§ åˆ›å»ºGNNç¼–ç å™¨:")
        print(f"   æ¨¡å¼: {self.agent_mode}")
        print(f"   èŠ‚ç‚¹ç»´åº¦: {self.state_dim}")
        print(f"   è¾¹ç»´åº¦: {self.edge_dim}")
        print(f"   éšè—ç»´åº¦: {self.hidden_dim}")
        print(f"   è¾“å‡ºç»´åº¦: {self.output_dim}")
        print(f"   å±‚æ•°: {self.num_layers}")
        
        try:
            self.gnn_encoder = GNNEncoder(
                node_dim=self.state_dim,           # é…ç½®ä¸­çš„èŠ‚ç‚¹ç»´åº¦
                edge_dim=self.edge_dim,            # æ ¹æ®æ¨¡å¼ç¡®å®šçš„è¾¹ç»´åº¦
                hidden_dim=self.hidden_dim,        # é…ç½®ä¸­çš„éšè—ç»´åº¦
                output_dim=self.output_dim,        # é…ç½®ä¸­çš„è¾“å‡ºç»´åº¦
                num_layers=self.num_layers         # é…ç½®ä¸­çš„å±‚æ•°
            ).to(self.device)
            
            print(f"âœ… GNNç¼–ç å™¨åˆ›å»ºæˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ GNNç¼–ç å™¨åˆ›å»ºå¤±è´¥: {e}")
            # å…œåº•æ–¹æ¡ˆï¼šä½¿ç”¨ç®€åŒ–å‚æ•°
            self.gnn_encoder = GNNEncoder(
                node_dim=8,
                edge_dim=4 if self.is_edge_aware else 2,
                hidden_dim=128,
                output_dim=256,
                num_layers=4
            ).to(self.device)
            print(f"âš ï¸ ä½¿ç”¨å…œåº•GNNç¼–ç å™¨")
    
    def _print_dimension_summary(self):
        """æ‰“å°ç»´åº¦é…ç½®æ‘˜è¦"""
        print(f"\nğŸ“Š {self.agent_id} ç»´åº¦é…ç½®æ‘˜è¦:")
        print(f"{'='*50}")
        print(f"æ™ºèƒ½ä½“æ¨¡å¼: {self.agent_mode}")
        print(f"èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {self.state_dim}")
        print(f"è¾¹ç‰¹å¾ç»´åº¦: {self.edge_dim}")
        print(f"VNFä¸Šä¸‹æ–‡ç»´åº¦: {self.vnf_context_dim}")
        print(f"GNNéšè—ç»´åº¦: {self.hidden_dim}")
        print(f"GNNè¾“å‡ºç»´åº¦: {self.output_dim}")
        print(f"GNNå±‚æ•°: {self.num_layers}")
        print(f"{'='*50}\n")
    
    def process_state(self, state: Union[Data, Dict, np.ndarray]) -> torch.Tensor:
        """
        å¤„ç†çŠ¶æ€è¾“å…¥ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºå›¾ç¥ç»ç½‘ç»œå¯å¤„ç†çš„æ ¼å¼ - ä¿®å¤ç‰ˆ
        
        ğŸ”§ ä¸»è¦æ”¹è¿›ï¼š
        1. ä¸¥æ ¼çš„ç»´åº¦éªŒè¯
        2. è‡ªåŠ¨ç»´åº¦ä¿®å¤
        3. æ¨¡å¼æ„ŸçŸ¥å¤„ç†
        4. è¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š
        """
        self.gnn_encoder.eval()
        
        with torch.no_grad():
            if isinstance(state, Data):
                # ğŸ”§ PyGæ•°æ®å¯¹è±¡å¤„ç†
                processed_state = self._process_pyg_data(state)
                
            elif isinstance(state, dict) and 'graph_data' in state:
                # ğŸ”§ å­—å…¸æ ¼å¼å¤„ç†
                graph_data = state['graph_data']
                processed_state = self._process_pyg_data(graph_data)
                
            elif isinstance(state, (np.ndarray, torch.Tensor)):
                # ğŸ”§ å¼ é‡æ ¼å¼å¤„ç†
                if isinstance(state, np.ndarray):
                    state = torch.tensor(state, dtype=torch.float32)
                
                # ç¡®ä¿æ˜¯äºŒç»´å¼ é‡
                if state.dim() == 1:
                    state = state.unsqueeze(0)
                
                # ç»´åº¦æ£€æŸ¥å’Œä¿®å¤
                if state.size(-1) != self.output_dim:
                    if state.size(-1) < self.output_dim:
                        # è¡¥å……åˆ°ç›®æ ‡ç»´åº¦
                        padding = torch.zeros(*state.shape[:-1], self.output_dim - state.size(-1))
                        state = torch.cat([state, padding], dim=-1)
                    else:
                        # æˆªå–åˆ°ç›®æ ‡ç»´åº¦
                        state = state[..., :self.output_dim]
                
                processed_state = state.to(self.device)
                
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„çŠ¶æ€æ ¼å¼: {type(state)}")
        
        if self.is_training:
            self.gnn_encoder.train()
        
        return processed_state
    
    def _process_pyg_data(self, data: Data) -> torch.Tensor:
        """å¤„ç†PyTorch Geometricæ•°æ®å¯¹è±¡"""
        # ğŸ”§ ç»´åº¦éªŒè¯å’Œä¿®å¤
        data = self._validate_and_fix_pyg_dimensions(data)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        data = data.to(self.device)
        
        # GNNç¼–ç 
        try:
            encoded_state = self.gnn_encoder(data)
            
            # è¾“å‡ºç»´åº¦éªŒè¯
            if encoded_state.size(-1) != self.output_dim:
                print(f"âš ï¸ GNNè¾“å‡ºç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.output_dim}, å®é™…{encoded_state.size(-1)}")
                # è‡ªåŠ¨ä¿®å¤
                if encoded_state.size(-1) < self.output_dim:
                    padding = torch.zeros(*encoded_state.shape[:-1], 
                                        self.output_dim - encoded_state.size(-1), 
                                        device=encoded_state.device)
                    encoded_state = torch.cat([encoded_state, padding], dim=-1)
                else:
                    encoded_state = encoded_state[..., :self.output_dim]
            
            return encoded_state
            
        except Exception as e:
            print(f"âŒ GNNç¼–ç å¤±è´¥: {e}")
            # å…œåº•æ–¹æ¡ˆï¼šè¿”å›é»˜è®¤å¼ é‡
            return torch.zeros(1, self.output_dim, device=self.device)
    
    def _validate_and_fix_pyg_dimensions(self, data: Data) -> Data:
        """éªŒè¯å’Œä¿®å¤PyGæ•°æ®ç»´åº¦"""
        
        # ğŸ”§ èŠ‚ç‚¹ç‰¹å¾ç»´åº¦æ£€æŸ¥å’Œä¿®å¤
        if data.x is not None:
            current_node_dim = data.x.size(1)
            if current_node_dim != self.state_dim:
                print(f"ğŸ”§ ä¿®å¤èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {current_node_dim} -> {self.state_dim}")
                
                if current_node_dim < self.state_dim:
                    # è¡¥å……åˆ°ç›®æ ‡ç»´åº¦
                    padding = torch.zeros(data.x.size(0), self.state_dim - current_node_dim, 
                                        device=data.x.device, dtype=data.x.dtype)
                    data.x = torch.cat([data.x, padding], dim=1)
                else:
                    # æˆªå–åˆ°ç›®æ ‡ç»´åº¦
                    data.x = data.x[:, :self.state_dim]
        
        # ğŸ”§ è¾¹ç‰¹å¾ç»´åº¦æ£€æŸ¥å’Œä¿®å¤
        if hasattr(data, 'edge_attr') and data.edge_attr is not None:
            current_edge_dim = data.edge_attr.size(1)
            if current_edge_dim != self.edge_dim:
                print(f"ğŸ”§ ä¿®å¤è¾¹ç‰¹å¾ç»´åº¦: {current_edge_dim} -> {self.edge_dim}")
                
                if current_edge_dim < self.edge_dim:
                    # è¡¥å……åˆ°ç›®æ ‡ç»´åº¦
                    padding = torch.zeros(data.edge_attr.size(0), self.edge_dim - current_edge_dim,
                                        device=data.edge_attr.device, dtype=data.edge_attr.dtype)
                    
                    # ä¸ºEdge-awareæ¨¡å¼æ™ºèƒ½è¡¥å……ç‰¹å¾
                    if self.is_edge_aware and current_edge_dim == 2 and self.edge_dim == 4:
                        # è¡¥å……æŠ–åŠ¨å’Œä¸¢åŒ…ç‡çš„é»˜è®¤å€¼
                        padding[:, 0] = torch.rand(data.edge_attr.size(0), device=data.edge_attr.device) * 0.01  # æŠ–åŠ¨
                        padding[:, 1] = torch.rand(data.edge_attr.size(0), device=data.edge_attr.device) * 0.005  # ä¸¢åŒ…ç‡
                    
                    data.edge_attr = torch.cat([data.edge_attr, padding], dim=1)
                else:
                    # æˆªå–åˆ°ç›®æ ‡ç»´åº¦
                    data.edge_attr = data.edge_attr[:, :self.edge_dim]
        
        # ğŸ”§ VNFä¸Šä¸‹æ–‡ç»´åº¦æ£€æŸ¥
        if hasattr(data, 'vnf_context') and data.vnf_context is not None:
            if data.vnf_context.size(-1) != self.vnf_context_dim:
                print(f"ğŸ”§ VNFä¸Šä¸‹æ–‡ç»´åº¦è­¦å‘Š: æœŸæœ›{self.vnf_context_dim}, å®é™…{data.vnf_context.size(-1)}")
        
        return data
    
    def update_target_network(self, tau: float = None):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆç”¨äºDQNç³»åˆ—ç®—æ³•ï¼‰"""
        if self.target_network is None:
            return
            
        if tau is None:
            # ç¡¬æ›´æ–°
            self.target_network.load_state_dict(self.policy_network.state_dict())
        else:
            # è½¯æ›´æ–°
            for target_param, policy_param in zip(
                self.target_network.parameters(), 
                self.policy_network.parameters()
            ):
                target_param.data.copy_(
                    tau * policy_param.data + (1 - tau) * target_param.data
                )
    
    def decay_epsilon(self):
        """æ›´æ–°æ¢ç´¢ç‡"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
            self.epsilon = max(self.epsilon, self.epsilon_min)
    
    def get_valid_actions(self, state: Union[Data, Dict], **kwargs) -> List[int]:
        """
        è·å–å½“å‰çŠ¶æ€ä¸‹çš„æœ‰æ•ˆåŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            **kwargs: é¢å¤–å‚æ•°ï¼ˆå¦‚èµ„æºçº¦æŸç­‰ï¼‰
            
        Returns:
            valid_actions: æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨
        """
        # åŸºç¡€å®ç°ï¼šæ‰€æœ‰åŠ¨ä½œéƒ½æœ‰æ•ˆ
        available_nodes = kwargs.get('available_nodes', list(range(self.action_dim)))
        resource_constraints = kwargs.get('resource_constraints', {})
        
        valid_actions = []
        for node in available_nodes:
            if self._check_node_feasibility(node, resource_constraints):
                valid_actions.append(node)
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆåŠ¨ä½œ
        if not valid_actions:
            valid_actions = [0]
        
        return valid_actions
    
    def _check_node_feasibility(self, node_id: int, constraints: Dict) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹å¯è¡Œæ€§ï¼ˆå­ç±»å¯é‡å†™ï¼‰"""
        return True
    
    def mask_invalid_actions(self, q_values: torch.Tensor, valid_actions: List[int]) -> torch.Tensor:
        """
        å±è”½æ— æ•ˆåŠ¨ä½œçš„Qå€¼
        
        Args:
            q_values: åŸå§‹Qå€¼ [batch_size, action_dim]
            valid_actions: æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨
            
        Returns:
            masked_q_values: å±è”½åçš„Qå€¼
        """
        masked_q_values = q_values.clone()
        
        # è·å–æ— æ•ˆåŠ¨ä½œ
        invalid_actions = [a for a in range(self.action_dim) if a not in valid_actions]
        
        if invalid_actions:
            masked_q_values[:, invalid_actions] = -float('inf')
        
        return masked_q_values
    
    def update_stats(self, reward: float, action: int, loss: float = None, q_values: torch.Tensor = None):
        """æ›´æ–°æ™ºèƒ½ä½“ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["total_reward"] += reward
        self.stats["steps"] += 1
        
        if loss is not None:
            self.stats["losses"].append(loss)
        
        if q_values is not None:
            self.stats["q_values"].append(q_values.mean().item() if isinstance(q_values, torch.Tensor) else q_values)
        
        # è®°å½•åŠ¨ä½œåˆ†å¸ƒ
        if action not in self.stats["actions_taken"]:
            self.stats["actions_taken"][action] = 0
        self.stats["actions_taken"][action] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½ä½“ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if stats["episodes"] > 0:
            stats["avg_reward"] = stats["total_reward"] / stats["episodes"]
        else:
            stats["avg_reward"] = 0.0
            
        if stats["losses"]:
            stats["avg_loss"] = np.mean(stats["losses"][-100:])  # æœ€è¿‘100æ¬¡çš„å¹³å‡loss
            
        if stats["q_values"]:
            stats["avg_q_value"] = np.mean(stats["q_values"][-100:])  # æœ€è¿‘100æ¬¡çš„å¹³å‡Qå€¼
            
        # æ·»åŠ å½“å‰çŠ¶æ€
        stats["epsilon"] = self.epsilon
        stats["training_step"] = self.training_step
        stats["agent_mode"] = self.agent_mode
        
        return stats
    
    def reset_episode_stats(self):
        """é‡ç½®episodeç»Ÿè®¡"""
        self.stats["total_reward"] = 0.0
        self.stats["episodes"] += 1
    
    def save_checkpoint(self, filepath: str):
        """ä¿å­˜æ™ºèƒ½ä½“æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'agent_id': self.agent_id,
            'agent_mode': self.agent_mode,
            'dimensions': self.dimensions,
            'training_step': self.training_step,
            'episode_count': self.episode_count,
            'epsilon': self.epsilon,
            'stats': self.stats,
            'gnn_encoder_state': self.gnn_encoder.state_dict() if self.gnn_encoder else None,
        }
        
        if self.policy_network is not None:
            checkpoint['policy_network_state'] = self.policy_network.state_dict()
            
        if self.target_network is not None:
            checkpoint['target_network_state'] = self.target_network.state_dict()
            
        if self.optimizer is not None:
            checkpoint['optimizer_state'] = self.optimizer.state_dict()
        
        torch.save(checkpoint, filepath)
        print(f"ğŸ’¾ Agent {self.agent_id} æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """åŠ è½½æ™ºèƒ½ä½“æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # æ¢å¤åŸºæœ¬çŠ¶æ€
        self.training_step = checkpoint.get('training_step', 0)
        self.episode_count = checkpoint.get('episode_count', 0)
        self.epsilon = checkpoint.get('epsilon', 1.0)
        self.stats = checkpoint.get('stats', {})
        
        # æ¢å¤æ¨¡å‹çŠ¶æ€
        if 'gnn_encoder_state' in checkpoint and self.gnn_encoder is not None:
            try:
                self.gnn_encoder.load_state_dict(checkpoint['gnn_encoder_state'])
                print(f"âœ… GNNç¼–ç å™¨çŠ¶æ€å·²æ¢å¤")
            except Exception as e:
                print(f"âš ï¸ GNNç¼–ç å™¨çŠ¶æ€æ¢å¤å¤±è´¥: {e}")
            
        if 'policy_network_state' in checkpoint and self.policy_network is not None:
            try:
                self.policy_network.load_state_dict(checkpoint['policy_network_state'])
                print(f"âœ… ç­–ç•¥ç½‘ç»œçŠ¶æ€å·²æ¢å¤")
            except Exception as e:
                print(f"âš ï¸ ç­–ç•¥ç½‘ç»œçŠ¶æ€æ¢å¤å¤±è´¥: {e}")
            
        if 'target_network_state' in checkpoint and self.target_network is not None:
            try:
                self.target_network.load_state_dict(checkpoint['target_network_state'])
                print(f"âœ… ç›®æ ‡ç½‘ç»œçŠ¶æ€å·²æ¢å¤")
            except Exception as e:
                print(f"âš ï¸ ç›®æ ‡ç½‘ç»œçŠ¶æ€æ¢å¤å¤±è´¥: {e}")
            
        if 'optimizer_state' in checkpoint and self.optimizer is not None:
            try:
                self.optimizer.load_state_dict(checkpoint['optimizer_state'])
                print(f"âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤")
            except Exception as e:
                print(f"âš ï¸ ä¼˜åŒ–å™¨çŠ¶æ€æ¢å¤å¤±è´¥: {e}")
        
        print(f"ğŸ“‚ Agent {self.agent_id} æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}")
    
    def set_training_mode(self, training: bool = True):
        """è®¾ç½®è®­ç»ƒ/è¯„ä¼°æ¨¡å¼"""
        self.is_training = training
        
        if self.gnn_encoder is not None:
            if training:
                self.gnn_encoder.train()
            else:
                self.gnn_encoder.eval()
                
        if self.policy_network is not None:
            if training:
                self.policy_network.train()
            else:
                self.policy_network.eval()
    
    def register_other_agents(self, agents: Dict[str, 'BaseAgent']):
        """æ³¨å†Œå…¶ä»–æ™ºèƒ½ä½“ï¼ˆç”¨äºå¤šæ™ºèƒ½ä½“åè°ƒï¼‰"""
        self.other_agents = agents
        self.communication_enabled = len(agents) > 0
        print(f"ğŸ¤ Agent {self.agent_id} å·²æ³¨å†Œ {len(agents)} ä¸ªå…¶ä»–æ™ºèƒ½ä½“")
    
    def get_dimension_info(self) -> Dict[str, Any]:
        """è·å–ç»´åº¦ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        return {
            'agent_id': self.agent_id,
            'agent_mode': self.agent_mode,
            'is_edge_aware': self.is_edge_aware,
            'state_dim': self.state_dim,
            'edge_dim': self.edge_dim,
            'vnf_context_dim': self.vnf_context_dim,
            'output_dim': self.output_dim,
            'hidden_dim': self.hidden_dim,
            'num_layers': self.num_layers
        }
    
    # ğŸ”§ æŠ½è±¡æ–¹æ³•ï¼šå­ç±»å¿…é¡»å®ç°
    @abstractmethod
    def select_action(self, state: Union[Data, Dict], **kwargs) -> Union[int, List[int]]:
        """é€‰æ‹©åŠ¨ä½œ"""
        pass
    
    @abstractmethod
    def store_transition(self, state, action, reward, next_state, done, **kwargs):
        """å­˜å‚¨ç»éªŒ"""
        pass
    
    @abstractmethod
    def learn(self) -> Dict[str, float]:
        """å­¦ä¹ æ›´æ–°"""
        pass


def create_agent(agent_type: str, agent_id: str, state_dim: int, action_dim: int, 
                 edge_dim: int, config: Dict[str, Any], use_enhanced_gnn: bool = False) -> EnhancedBaseAgent:
    print(f"ğŸ­ åˆ›å»ºæ™ºèƒ½ä½“: {agent_type} -> {agent_id} (å¢å¼ºæ¨¡å¼: {use_enhanced_gnn})")
    
    try:
        if agent_type.lower() == 'ddqn':
            from agents.multi_ddqn_agent import MultiDDQNAgent
            return MultiDDQNAgent(agent_id, state_dim, action_dim, edge_dim, config, use_enhanced_gnn)
        elif agent_type.lower() == 'dqn':
            from agents.multi_dqn_agent import MultiDQNAgent
            return MultiDQNAgent(agent_id, state_dim, action_dim, edge_dim, config, use_enhanced_gnn)
        elif agent_type.lower() == 'ppo':
            from agents.multi_ppo_agent import MultiPPOAgent
            return MultiPPOAgent(agent_id, state_dim, action_dim, edge_dim, config, use_enhanced_gnn)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ™ºèƒ½ä½“ç±»å‹: {agent_type}")
    except Exception as e:
        print(f"âŒ æ™ºèƒ½ä½“åˆ›å»ºå¤±è´¥: {e}")
        raise


def test_base_agent_config_driven():
    """æµ‹è¯•é…ç½®é©±åŠ¨çš„BaseAgent"""
    print("ğŸ§ª æµ‹è¯•é…ç½®é©±åŠ¨çš„BaseAgent...")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿé…ç½®
    config = {
        "edge_aware_mode": True,
        "gnn": {
            "edge_aware": {"hidden_dim": 128, "output_dim": 256, "layers": 6},
            "baseline": {"hidden_dim": 64, "output_dim": 256, "layers": 4}
        },
        "train": {"lr": 0.0003, "gamma": 0.99, "batch_size": 32}
    }
    
    # æµ‹è¯•æ™ºèƒ½ä½“ç±» 
    class TestAgent(BaseAgent):
        def select_action(self, state, **kwargs):
            return np.random.randint(0, self.action_dim)
        
        def store_transition(self, state, action, reward, next_state, done, **kwargs):
            pass
        
        def learn(self):
            return {"loss": 0.1}
    
    print("1. æµ‹è¯•Edge-awareæ™ºèƒ½ä½“åˆ›å»º:")
    try:
        agent_edge = TestAgent("ddqn_edge_aware_test", state_dim=8, action_dim=42, edge_dim=4, config=config)
        print(f"âœ… Edge-awareæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å¼: {agent_edge.agent_mode}")
        print(f"   èŠ‚ç‚¹ç»´åº¦: {agent_edge.state_dim}")
        print(f"   è¾¹ç»´åº¦: {agent_edge.edge_dim}")
        print(f"   è¾“å‡ºç»´åº¦: {agent_edge.output_dim}")
    except Exception as e:
        print(f"âŒ Edge-awareæ™ºèƒ½ä½“åˆ›å»ºå¤±è´¥: {e}")
    
    print("\n2. æµ‹è¯•Baselineæ™ºèƒ½ä½“åˆ›å»º:")
    try:
        agent_baseline = TestAgent("dqn_baseline_test", state_dim=8, action_dim=42, edge_dim=2, config=config)
        print(f"âœ… Baselineæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å¼: {agent_baseline.agent_mode}")
        print(f"   èŠ‚ç‚¹ç»´åº¦: {agent_baseline.state_dim}")
        print(f"   è¾¹ç»´åº¦: {agent_baseline.edge_dim}")
        print(f"   è¾“å‡ºç»´åº¦: {agent_baseline.output_dim}")
    except Exception as e:
        print(f"âŒ Baselineæ™ºèƒ½ä½“åˆ›å»ºå¤±è´¥: {e}")
    
    print("\n3. æµ‹è¯•çŠ¶æ€å¤„ç†:")
    try:
        # åˆ›å»ºæµ‹è¯•çŠ¶æ€
        test_state = Data(
            x=torch.randn(42, 8),  # æ­£ç¡®çš„8ç»´èŠ‚ç‚¹ç‰¹å¾
            edge_index=torch.randint(0, 42, (2, 100)),
            edge_attr=torch.randn(100, 4)  # Edge-aware 4ç»´è¾¹ç‰¹å¾
        )
        
        processed_state = agent_edge.process_state(test_state)
        print(f"âœ… Edge-awareçŠ¶æ€å¤„ç†æˆåŠŸ: {processed_state.shape}")
        
        # æµ‹è¯•ç»´åº¦ä¸åŒ¹é…çš„è‡ªåŠ¨ä¿®å¤
        test_state_wrong = Data(
            x=torch.randn(42, 6),  # é”™è¯¯çš„6ç»´èŠ‚ç‚¹ç‰¹å¾
            edge_index=torch.randint(0, 42, (2, 100)),
            edge_attr=torch.randn(100, 2)  # 2ç»´è¾¹ç‰¹å¾
        )
        
        processed_state_fixed = agent_edge.process_state(test_state_wrong)
        print(f"âœ… ç»´åº¦è‡ªåŠ¨ä¿®å¤æµ‹è¯•æˆåŠŸ: {processed_state_fixed.shape}")
        
    except Exception as e:
        print(f"âŒ çŠ¶æ€å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n4. æµ‹è¯•ç»Ÿè®¡åŠŸèƒ½:")
    try:
        agent_edge.update_stats(reward=10.5, action=5, loss=0.2, q_values=torch.tensor([1.5, 2.0, 1.8]))
        stats = agent_edge.get_stats()
        print(f"âœ… ç»Ÿè®¡åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
        print(f"   æ€»å¥–åŠ±: {stats['total_reward']}")
        print(f"   æ­¥æ•°: {stats['steps']}")
        print(f"   æ¨¡å¼: {stats['agent_mode']}")
    except Exception as e:
        print(f"âŒ ç»Ÿè®¡åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n5. æµ‹è¯•ç»´åº¦ä¿¡æ¯:")
    try:
        dim_info = agent_edge.get_dimension_info()
        print(f"âœ… ç»´åº¦ä¿¡æ¯è·å–æˆåŠŸ:")
        for key, value in dim_info.items():
            print(f"   {key}: {value}")
    except Exception as e:
        print(f"âŒ ç»´åº¦ä¿¡æ¯è·å–å¤±è´¥: {e}")
    
    print(f"\nğŸ‰ é…ç½®é©±åŠ¨BaseAgentæµ‹è¯•å®Œæˆ!")


# ğŸ”§ å…¼å®¹æ€§åŒ…è£…å‡½æ•°
def create_legacy_agent(*args, **kwargs):
    """ä¸ºäº†å‘åå…¼å®¹çš„åŒ…è£…å‡½æ•°"""
    print("âš ï¸ ä½¿ç”¨äº†æ—§ç‰ˆæ™ºèƒ½ä½“åˆ›å»ºæ¥å£ï¼Œå»ºè®®ä½¿ç”¨æ–°çš„é…ç½®é©±åŠ¨æ¥å£")
    return create_agent(*args, **kwargs)


if __name__ == "__main__":
    test_base_agent_config_driven()